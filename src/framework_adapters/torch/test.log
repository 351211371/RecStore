xmh:  {'test_cache': 'KnownShardedCachedEmbedding', 'cache_ratio': 0.1}
========== Running Test with routine <bound method TestShardedCache.routine_cache_helper of <__main__.TestShardedCache object at 0x7f3c61073eb0>> {'test_cache': 'KnownShardedCachedEmbedding', 'cache_ratio': 0.1}==========
INFO [distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 1
INFO [distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 0
INFO [distributed_c10d.py:476] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.
INFO [distributed_c10d.py:476] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.
/home/xieminhui/RecStore/src/framework_adapters/torch/DistTensor.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return th.tensor(result.reshape((len(idx), self.shape[1])))
weight.shape torch.Size([1000, 3])
/home/xieminhui/RecStore/src/framework_adapters/torch/DistTensor.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return th.tensor(result.reshape((len(idx), self.shape[1])))
weight.shape torch.Size([1000, 3])
cache_range is [(0, 50), (50, 100)]
  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]DEBUG [test_emb.py:176] emb._trace []
DEBUG [test_emb.py:176] emb._trace []
DEBUG [test_emb.py:177] emb._hand_grad []
DEBUG [test_emb.py:177] emb._hand_grad []
DEBUG [test_emb.py:188] 1:step0, input_keys tensor([263, 313, 491, 341, 759, 432, 248, 249, 516, 943], device='cuda:1')
DEBUG [test_emb.py:188] 0:step0, input_keys tensor([963, 379, 427, 503, 497, 683, 101, 866, 756, 399], device='cuda:0')
DEBUG [test_emb.py:192] 1:std_embed_value tensor([[263., 263., 263.],
        [313., 313., 313.],
        [491., 491., 491.],
        [341., 341., 341.],
        [759., 759., 759.],
        [432., 432., 432.],
        [248., 248., 248.],
        [249., 249., 249.],
        [516., 516., 516.],
        [943., 943., 943.]], device='cuda:1', grad_fn=<EmbeddingBackward0>)
DEBUG [test_emb.py:194] emb._trace []
DEBUG [test_emb.py:195] emb._hand_grad []
DEBUG [test_emb.py:192] 0:std_embed_value tensor([[963., 963., 963.],
        [379., 379., 379.],
        [427., 427., 427.],
        [503., 503., 503.],
        [497., 497., 497.],
        [683., 683., 683.],
        [101., 101., 101.],
        [866., 866., 866.],
        [756., 756., 756.],
        [399., 399., 399.]], device='cuda:0', grad_fn=<EmbeddingBackward0>)
DEBUG [test_emb.py:194] emb._trace []
DEBUG [test_emb.py:195] emb._hand_grad []
DEBUG [sharded_cache.py:235] 0: a2a cache_query_values
DEBUG [sharded_cache.py:235] 1: a2a cache_query_values
DEBUG [sharded_cache.py:238] 0: a2a cache_query_values done
DEBUG [sharded_cache.py:238] 1: a2a cache_query_values done
DEBUG [utils.py:209] rank0: before sum sparse tensors
INFO [utils.py:213] rank0: after sum sparse tensors
DEBUG [sharded_cache.py:286] 0 missing_keys = tensor([963, 379, 427, 503, 497, 683, 101, 866, 756, 399], device='cuda:0')
DEBUG [sharded_cache.py:286] 1 missing_keys = tensor([263, 313, 491, 341, 759, 432, 248, 249, 516, 943], device='cuda:1')
DEBUG [sharded_cache.py:287] 0 missing_grads = tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], device='cuda:0')
DEBUG [sharded_cache.py:287] 1 missing_grads = tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], device='cuda:1')
DEBUG [sharded_cache.py:292] reduced_missing_grads = tensor(indices=tensor([[101, 248, 249, 263, 313, 341, 379, 399, 427, 432, 491,
                        497, 503, 516, 683, 756, 759, 866, 943, 963]]),
       values=tensor([[1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.]]),
       size=(1000, 3), nnz=20, layout=torch.sparse_coo)
DEBUG [DistEmb.py:116] record_grad tensor([101, 248, 249, 263, 313, 341, 379, 399, 427, 432, 491, 497, 503, 516,
        683, 756, 759, 866, 943, 963]) tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]])
DEBUG [test_emb.py:200] 1:embed_value tensor([[263., 263., 263.],
        [313., 313., 313.],
        [491., 491., 491.],
        [341., 341., 341.],
        [759., 759., 759.],
        [432., 432., 432.],
        [248., 248., 248.],
        [249., 249., 249.],
        [516., 516., 516.],
        [943., 943., 943.]], device='cuda:1',
       grad_fn=<KnownShardedCachedEmbeddingFnBackward>)
DEBUG [test_emb.py:200] 0:embed_value tensor([[963., 963., 963.],
        [379., 379., 379.],
        [427., 427., 427.],
        [503., 503., 503.],
        [497., 497., 497.],
        [683., 683., 683.],
        [101., 101., 101.],
        [866., 866., 866.],
        [756., 756., 756.],
        [399., 399., 399.]], device='cuda:0',
       grad_fn=<KnownShardedCachedEmbeddingFnBackward>)
INFO [distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
INFO [distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
DEBUG [sharded_cache.py:235] 1: a2a cache_query_values
DEBUG [sharded_cache.py:235] 0: a2a cache_query_values
DEBUG [sharded_cache.py:238] 1: a2a cache_query_values done
DEBUG [sharded_cache.py:238] 0: a2a cache_query_values done
  5%|▌         | 1/20 [00:00<00:04,  4.20it/s]DEBUG [test_emb.py:176] emb._trace []
DEBUG [test_emb.py:177] emb._hand_grad []
  5%|▌         | 1/20 [00:00<00:04,  4.20it/s]DEBUG [test_emb.py:176] emb._trace []
DEBUG [test_emb.py:177] emb._hand_grad []
DEBUG [test_emb.py:188] 1:step1, input_keys tensor([ 13, 340, 302, 721, 242, 716, 750, 493, 346, 204], device='cuda:1')
DEBUG [test_emb.py:188] 0:step1, input_keys tensor([878, 376,  56, 868, 794,  33, 126, 119, 391, 254], device='cuda:0')
DEBUG [test_emb.py:192] 1:std_embed_value tensor([[ 13.,  13.,  13.],
        [340., 340., 340.],
        [302., 302., 302.],
        [721., 721., 721.],
        [242., 242., 242.],
        [716., 716., 716.],
        [750., 750., 750.],
        [493., 493., 493.],
        [346., 346., 346.],
        [204., 204., 204.]], device='cuda:1', grad_fn=<EmbeddingBackward0>)
DEBUG [test_emb.py:194] emb._trace []
DEBUG [test_emb.py:195] emb._hand_grad []
DEBUG [test_emb.py:192] 0:std_embed_value tensor([[878., 878., 878.],
        [376., 376., 376.],
        [ 56.,  56.,  56.],
        [868., 868., 868.],
        [794., 794., 794.],
        [ 33.,  33.,  33.],
        [126., 126., 126.],
        [119., 119., 119.],
        [391., 391., 391.],
        [254., 254., 254.]], device='cuda:0', grad_fn=<EmbeddingBackward0>)
DEBUG [test_emb.py:194] emb._trace []
DEBUG [test_emb.py:195] emb._hand_grad []
DEBUG [sharded_cache.py:235] 1: a2a cache_query_values
DEBUG [sharded_cache.py:235] 0: a2a cache_query_values
DEBUG [sharded_cache.py:238] 1: a2a cache_query_values done
DEBUG [sharded_cache.py:238] 0: a2a cache_query_values done
DEBUG [utils.py:209] rank0: before sum sparse tensors
INFO [utils.py:213] rank0: after sum sparse tensors
DEBUG [sharded_cache.py:286] 1 missing_keys = tensor([340, 302, 721, 242, 716, 750, 493, 346, 204], device='cuda:1')
DEBUG [sharded_cache.py:286] 0 missing_keys = tensor([878, 376, 868, 794, 126, 119, 391, 254], device='cuda:0')
DEBUG [sharded_cache.py:287] 0 missing_grads = tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], device='cuda:0')
DEBUG [sharded_cache.py:287] 1 missing_grads = tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], device='cuda:1')
DEBUG [sharded_cache.py:292] reduced_missing_grads = tensor(indices=tensor([[119, 126, 204, 242, 254, 302, 340, 346, 376, 391, 493,
                        716, 721, 750, 794, 868, 878]]),
       values=tensor([[1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.]]),
       size=(1000, 3), nnz=17, layout=torch.sparse_coo)
DEBUG [DistEmb.py:116] record_grad tensor([119, 126, 204, 242, 254, 302, 340, 346, 376, 391, 493, 716, 721, 750,
        794, 868, 878]) tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]])
DEBUG [test_emb.py:200] 1:embed_value tensor([[ 13.,  13.,  13.],
        [340., 340., 340.],
        [302., 302., 302.],
        [721., 721., 721.],
        [242., 242., 242.],
        [716., 716., 716.],
        [750., 750., 750.],
        [493., 493., 493.],
        [346., 346., 346.],
        [204., 204., 204.]], device='cuda:1',
       grad_fn=<KnownShardedCachedEmbeddingFnBackward>)
DEBUG [test_emb.py:200] 0:embed_value tensor([[878., 878., 878.],
        [376., 376., 376.],
        [ 56.,  56.,  56.],
        [868., 868., 868.],
        [794., 794., 794.],
        [ 33.,  33.,  33.],
        [126., 126., 126.],
        [119., 119., 119.],
        [391., 391., 391.],
        [254., 254., 254.]], device='cuda:0',
       grad_fn=<KnownShardedCachedEmbeddingFnBackward>)
DEBUG [sharded_cache.py:235] 1: a2a cache_query_values
DEBUG [sharded_cache.py:235] 0: a2a cache_query_values
DEBUG [sharded_cache.py:238] 1: a2a cache_query_values done
DEBUG [sharded_cache.py:238] 0: a2a cache_query_values done
DEBUG [test_emb.py:176] emb._trace []
DEBUG [test_emb.py:177] emb._hand_grad []
DEBUG [test_emb.py:176] emb._trace []
DEBUG [test_emb.py:177] emb._hand_grad []
DEBUG [test_emb.py:188] 1:step2, input_keys tensor([829, 254,  18, 996, 487, 703, 537, 273, 303, 166], device='cuda:1')
DEBUG [test_emb.py:188] 0:step2, input_keys tensor([549, 480, 481,  12, 363, 360, 995,  85, 822, 999], device='cuda:0')
DEBUG [test_emb.py:192] 1:std_embed_value tensor([[829.0000, 829.0000, 829.0000],
        [253.5000, 253.5000, 253.5000],
        [ 18.0000,  18.0000,  18.0000],
        [996.0000, 996.0000, 996.0000],
        [487.0000, 487.0000, 487.0000],
        [703.0000, 703.0000, 703.0000],
        [537.0000, 537.0000, 537.0000],
        [273.0000, 273.0000, 273.0000],
        [303.0000, 303.0000, 303.0000],
        [166.0000, 166.0000, 166.0000]], device='cuda:1',
       grad_fn=<EmbeddingBackward0>)
DEBUG [test_emb.py:194] emb._trace []
DEBUG [test_emb.py:195] emb._hand_grad []
DEBUG [test_emb.py:192] 0:std_embed_value tensor([[549., 549., 549.],
        [480., 480., 480.],
        [481., 481., 481.],
        [ 12.,  12.,  12.],
        [363., 363., 363.],
        [360., 360., 360.],
        [995., 995., 995.],
        [ 85.,  85.,  85.],
        [822., 822., 822.],
        [999., 999., 999.]], device='cuda:0', grad_fn=<EmbeddingBackward0>)
DEBUG [test_emb.py:194] emb._trace []
DEBUG [test_emb.py:195] emb._hand_grad []
DEBUG [sharded_cache.py:235] 1: a2a cache_query_values
DEBUG [sharded_cache.py:235] 0: a2a cache_query_values
DEBUG [sharded_cache.py:238] 1: a2a cache_query_values done
DEBUG [sharded_cache.py:238] 0: a2a cache_query_values done
DEBUG [utils.py:209] rank0: before sum sparse tensors
INFO [utils.py:213] rank0: after sum sparse tensors
DEBUG [sharded_cache.py:286] 1 missing_keys = tensor([829, 254, 996, 487, 703, 537, 273, 303, 166], device='cuda:1')
DEBUG [sharded_cache.py:286] 0 missing_keys = tensor([549, 480, 481, 363, 360, 995, 822, 999], device='cuda:0')
DEBUG [sharded_cache.py:287] 1 missing_grads = tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], device='cuda:1')
DEBUG [sharded_cache.py:287] 0 missing_grads = tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], device='cuda:0')
DEBUG [sharded_cache.py:292] reduced_missing_grads = tensor(indices=tensor([[166, 254, 273, 303, 360, 363, 480, 481, 487, 537, 549,
                        703, 822, 829, 995, 996, 999]]),
       values=tensor([[1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.]]),
       size=(1000, 3), nnz=17, layout=torch.sparse_coo)
DEBUG [DistEmb.py:116] record_grad tensor([166, 254, 273, 303, 360, 363, 480, 481, 487, 537, 549, 703, 822, 829,
        995, 996, 999]) tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]])
DEBUG [test_emb.py:200] 1:embed_value tensor([[829.0000, 829.0000, 829.0000],
        [253.5000, 253.5000, 253.5000],
        [ 18.0000,  18.0000,  18.0000],
        [996.0000, 996.0000, 996.0000],
        [487.0000, 487.0000, 487.0000],
        [703.0000, 703.0000, 703.0000],
        [537.0000, 537.0000, 537.0000],
        [273.0000, 273.0000, 273.0000],
        [303.0000, 303.0000, 303.0000],
        [166.0000, 166.0000, 166.0000]], device='cuda:1',
       grad_fn=<KnownShardedCachedEmbeddingFnBackward>)
DEBUG [test_emb.py:200] 0:embed_value tensor([[549., 549., 549.],
        [480., 480., 480.],
        [481., 481., 481.],
        [ 12.,  12.,  12.],
        [363., 363., 363.],
        [360., 360., 360.],
        [995., 995., 995.],
        [ 85.,  85.,  85.],
        [822., 822., 822.],
        [999., 999., 999.]], device='cuda:0',
       grad_fn=<KnownShardedCachedEmbeddingFnBackward>)
DEBUG [sharded_cache.py:235] 1: a2a cache_query_values
DEBUG [sharded_cache.py:235] 0: a2a cache_query_values
DEBUG [sharded_cache.py:238] 1: a2a cache_query_values done
DEBUG [sharded_cache.py:238] 0: a2a cache_query_values done
 15%|█▌        | 3/20 [00:00<00:01,  9.13it/s]DEBUG [test_emb.py:176] emb._trace []
DEBUG [test_emb.py:177] emb._hand_grad []
 15%|█▌        | 3/20 [00:00<00:01,  9.12it/s]DEBUG [test_emb.py:176] emb._trace []
DEBUG [test_emb.py:177] emb._hand_grad []
DEBUG [test_emb.py:188] 1:step3, input_keys tensor([392, 310, 725, 929, 785, 223, 623, 445, 998, 249], device='cuda:1')
DEBUG [test_emb.py:188] 0:step3, input_keys tensor([711, 288, 778,  43, 896, 489, 771, 757, 283, 395], device='cuda:0')
DEBUG [test_emb.py:192] 1:std_embed_value tensor([[392.0000, 392.0000, 392.0000],
        [310.0000, 310.0000, 310.0000],
        [725.0000, 725.0000, 725.0000],
        [929.0000, 929.0000, 929.0000],
        [785.0000, 785.0000, 785.0000],
        [223.0000, 223.0000, 223.0000],
        [623.0000, 623.0000, 623.0000],
        [445.0000, 445.0000, 445.0000],
        [998.0000, 998.0000, 998.0000],
        [248.5000, 248.5000, 248.5000]], device='cuda:1',
       grad_fn=<EmbeddingBackward0>)
DEBUG [test_emb.py:194] emb._trace []
DEBUG [test_emb.py:195] emb._hand_grad []
DEBUG [test_emb.py:192] 0:std_embed_value tensor([[711., 711., 711.],
        [288., 288., 288.],
        [778., 778., 778.],
        [ 43.,  43.,  43.],
        [896., 896., 896.],
        [489., 489., 489.],
        [771., 771., 771.],
        [757., 757., 757.],
        [283., 283., 283.],
        [395., 395., 395.]], device='cuda:0', grad_fn=<EmbeddingBackward0>)
DEBUG [test_emb.py:194] emb._trace []
DEBUG [test_emb.py:195] emb._hand_grad []
DEBUG [sharded_cache.py:235] 1: a2a cache_query_values
DEBUG [sharded_cache.py:235] 0: a2a cache_query_values
DEBUG [sharded_cache.py:238] 1: a2a cache_query_values done
DEBUG [sharded_cache.py:238] 0: a2a cache_query_values done
DEBUG [utils.py:209] rank0: before sum sparse tensors
INFO [utils.py:213] rank0: after sum sparse tensors
DEBUG [sharded_cache.py:286] 1 missing_keys = tensor([392, 310, 725, 929, 785, 223, 623, 445, 998, 249], device='cuda:1')
DEBUG [sharded_cache.py:286] 0 missing_keys = tensor([711, 288, 778, 896, 489, 771, 757, 283, 395], device='cuda:0')
DEBUG [sharded_cache.py:287] 1 missing_grads = tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], device='cuda:1')
DEBUG [sharded_cache.py:287] 0 missing_grads = tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], device='cuda:0')
DEBUG [sharded_cache.py:292] reduced_missing_grads = tensor(indices=tensor([[223, 249, 283, 288, 310, 392, 395, 445, 489, 623, 711,
                        725, 757, 771, 778, 785, 896, 929, 998]]),
       values=tensor([[1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.]]),
       size=(1000, 3), nnz=19, layout=torch.sparse_coo)
DEBUG [DistEmb.py:116] record_grad tensor([223, 249, 283, 288, 310, 392, 395, 445, 489, 623, 711, 725, 757, 771,
        778, 785, 896, 929, 998]) tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]])
DEBUG [test_emb.py:200] 1:embed_value tensor([[392.0000, 392.0000, 392.0000],
        [310.0000, 310.0000, 310.0000],
        [725.0000, 725.0000, 725.0000],
        [929.0000, 929.0000, 929.0000],
        [785.0000, 785.0000, 785.0000],
        [223.0000, 223.0000, 223.0000],
        [623.0000, 623.0000, 623.0000],
        [445.0000, 445.0000, 445.0000],
        [998.0000, 998.0000, 998.0000],
        [248.5000, 248.5000, 248.5000]], device='cuda:1',
       grad_fn=<KnownShardedCachedEmbeddingFnBackward>)
DEBUG [test_emb.py:176] emb._trace []
DEBUG [test_emb.py:177] emb._hand_grad []
DEBUG [test_emb.py:188] 1:step4, input_keys tensor([ 88, 320,  21, 723, 111, 594, 337,   8, 446, 999], device='cuda:1')
DEBUG [test_emb.py:200] 0:embed_value tensor([[711., 711., 711.],
        [288., 288., 288.],
        [778., 778., 778.],
        [ 43.,  43.,  43.],
        [896., 896., 896.],
        [489., 489., 489.],
        [771., 771., 771.],
        [757., 757., 757.],
        [283., 283., 283.],
        [395., 395., 395.]], device='cuda:0',
       grad_fn=<KnownShardedCachedEmbeddingFnBackward>)
DEBUG [test_emb.py:176] emb._trace []
DEBUG [test_emb.py:177] emb._hand_grad []
DEBUG [test_emb.py:188] 0:step4, input_keys tensor([673, 441, 111, 280, 503, 706, 476, 327, 699, 326], device='cuda:0')
DEBUG [test_emb.py:192] 1:std_embed_value tensor([[ 88.0000,  88.0000,  88.0000],
        [320.0000, 320.0000, 320.0000],
        [ 21.0000,  21.0000,  21.0000],
        [723.0000, 723.0000, 723.0000],
        [111.0000, 111.0000, 111.0000],
        [594.0000, 594.0000, 594.0000],
        [337.0000, 337.0000, 337.0000],
        [  8.0000,   8.0000,   8.0000],
        [446.0000, 446.0000, 446.0000],
        [998.5000, 998.5000, 998.5000]], device='cuda:1',
       grad_fn=<EmbeddingBackward0>)
DEBUG [test_emb.py:194] emb._trace []
DEBUG [test_emb.py:195] emb._hand_grad []
DEBUG [test_emb.py:192] 0:std_embed_value tensor([[673.0000, 673.0000, 673.0000],
        [441.0000, 441.0000, 441.0000],
        [111.0000, 111.0000, 111.0000],
        [280.0000, 280.0000, 280.0000],
        [502.5000, 502.5000, 502.5000],
        [706.0000, 706.0000, 706.0000],
        [476.0000, 476.0000, 476.0000],
        [327.0000, 327.0000, 327.0000],
        [699.0000, 699.0000, 699.0000],
        [326.0000, 326.0000, 326.0000]], device='cuda:0',
       grad_fn=<EmbeddingBackward0>)
DEBUG [test_emb.py:194] emb._trace []
DEBUG [test_emb.py:195] emb._hand_grad []
DEBUG [sharded_cache.py:235] 1: a2a cache_query_values
DEBUG [sharded_cache.py:235] 0: a2a cache_query_values
DEBUG [sharded_cache.py:238] 0: a2a cache_query_values done
DEBUG [sharded_cache.py:238] 1: a2a cache_query_values done
DEBUG [utils.py:209] rank0: before sum sparse tensors
INFO [utils.py:213] rank0: after sum sparse tensors
DEBUG [sharded_cache.py:286] 1 missing_keys = tensor([320, 723, 111, 594, 337, 446, 999], device='cuda:1')
DEBUG [sharded_cache.py:286] 0 missing_keys = tensor([673, 441, 111, 280, 503, 706, 476, 327, 699, 326], device='cuda:0')
DEBUG [sharded_cache.py:287] 1 missing_grads = tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], device='cuda:1')
DEBUG [sharded_cache.py:287] 0 missing_grads = tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], device='cuda:0')
DEBUG [sharded_cache.py:292] reduced_missing_grads = tensor(indices=tensor([[111, 280, 320, 326, 327, 337, 441, 446, 476, 503, 594,
                        673, 699, 706, 723, 999]]),
       values=tensor([[2., 2., 2.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.]]),
       size=(1000, 3), nnz=16, layout=torch.sparse_coo)
DEBUG [DistEmb.py:116] record_grad tensor([111, 280, 320, 326, 327, 337, 441, 446, 476, 503, 594, 673, 699, 706,
        723, 999]) tensor([[2., 2., 2.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]])
DEBUG [test_emb.py:200] 1:embed_value tensor([[ 88.0000,  88.0000,  88.0000],
        [320.0000, 320.0000, 320.0000],
        [ 21.0000,  21.0000,  21.0000],
        [723.0000, 723.0000, 723.0000],
        [111.0000, 111.0000, 111.0000],
        [594.0000, 594.0000, 594.0000],
        [337.0000, 337.0000, 337.0000],
        [  8.0000,   8.0000,   8.0000],
        [446.0000, 446.0000, 446.0000],
        [998.5000, 998.5000, 998.5000]], device='cuda:1',
       grad_fn=<KnownShardedCachedEmbeddingFnBackward>)
DEBUG [test_emb.py:176] emb._trace []
DEBUG [test_emb.py:177] emb._hand_grad []
DEBUG [test_emb.py:200] 0:embed_value tensor([[673.0000, 673.0000, 673.0000],
        [441.0000, 441.0000, 441.0000],
        [111.0000, 111.0000, 111.0000],
        [280.0000, 280.0000, 280.0000],
        [502.5000, 502.5000, 502.5000],
        [706.0000, 706.0000, 706.0000],
        [476.0000, 476.0000, 476.0000],
        [327.0000, 327.0000, 327.0000],
        [699.0000, 699.0000, 699.0000],
        [326.0000, 326.0000, 326.0000]], device='cuda:0',
       grad_fn=<KnownShardedCachedEmbeddingFnBackward>)
DEBUG [test_emb.py:188] 1:step5, input_keys tensor([459, 358, 213, 473, 481, 627, 572, 848, 329, 323], device='cuda:1')
DEBUG [test_emb.py:176] emb._trace []
DEBUG [test_emb.py:177] emb._hand_grad []
DEBUG [test_emb.py:188] 0:step5, input_keys tensor([363, 474, 775, 700, 218, 632, 568,  12, 277, 645], device='cuda:0')
DEBUG [test_emb.py:192] 1:std_embed_value tensor([[459.0000, 459.0000, 459.0000],
        [358.0000, 358.0000, 358.0000],
        [213.0000, 213.0000, 213.0000],
        [473.0000, 473.0000, 473.0000],
        [480.5000, 480.5000, 480.5000],
        [627.0000, 627.0000, 627.0000],
        [572.0000, 572.0000, 572.0000],
        [848.0000, 848.0000, 848.0000],
        [329.0000, 329.0000, 329.0000],
        [323.0000, 323.0000, 323.0000]], device='cuda:1',
       grad_fn=<EmbeddingBackward0>)
DEBUG [test_emb.py:194] emb._trace []
DEBUG [test_emb.py:195] emb._hand_grad []
DEBUG [test_emb.py:192] 0:std_embed_value tensor([[362.5000, 362.5000, 362.5000],
        [474.0000, 474.0000, 474.0000],
        [775.0000, 775.0000, 775.0000],
        [700.0000, 700.0000, 700.0000],
        [218.0000, 218.0000, 218.0000],
        [632.0000, 632.0000, 632.0000],
        [568.0000, 568.0000, 568.0000],
        [ 11.5000,  11.5000,  11.5000],
        [277.0000, 277.0000, 277.0000],
        [645.0000, 645.0000, 645.0000]], device='cuda:0',
       grad_fn=<EmbeddingBackward0>)
DEBUG [test_emb.py:194] emb._trace []
DEBUG [test_emb.py:195] emb._hand_grad []
DEBUG [sharded_cache.py:235] 1: a2a cache_query_values
DEBUG [sharded_cache.py:235] 0: a2a cache_query_values
DEBUG [sharded_cache.py:238] 1: a2a cache_query_values done
DEBUG [sharded_cache.py:238] 0: a2a cache_query_values done
DEBUG [utils.py:209] rank0: before sum sparse tensors
INFO [utils.py:213] rank0: after sum sparse tensors
DEBUG [sharded_cache.py:286] 1 missing_keys = tensor([459, 358, 213, 473, 481, 627, 572, 848, 329, 323], device='cuda:1')
DEBUG [sharded_cache.py:286] 0 missing_keys = tensor([363, 474, 775, 700, 218, 632, 568, 277, 645], device='cuda:0')
DEBUG [sharded_cache.py:287] 1 missing_grads = tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], device='cuda:1')
DEBUG [sharded_cache.py:287] 0 missing_grads = tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], device='cuda:0')
DEBUG [sharded_cache.py:292] reduced_missing_grads = tensor(indices=tensor([[213, 218, 277, 323, 329, 358, 363, 459, 473, 474, 481,
                        568, 572, 627, 632, 645, 700, 775, 848]]),
       values=tensor([[1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.]]),
       size=(1000, 3), nnz=19, layout=torch.sparse_coo)
DEBUG [DistEmb.py:116] record_grad tensor([213, 218, 277, 323, 329, 358, 363, 459, 473, 474, 481, 568, 572, 627,
        632, 645, 700, 775, 848]) tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]])
DEBUG [test_emb.py:200] 1:embed_value tensor([[459.0000, 459.0000, 459.0000],
        [358.0000, 358.0000, 358.0000],
        [213.0000, 213.0000, 213.0000],
        [473.0000, 473.0000, 473.0000],
        [480.5000, 480.5000, 480.5000],
        [627.0000, 627.0000, 627.0000],
        [572.0000, 572.0000, 572.0000],
        [848.0000, 848.0000, 848.0000],
        [329.0000, 329.0000, 329.0000],
        [323.0000, 323.0000, 323.0000]], device='cuda:1',
       grad_fn=<KnownShardedCachedEmbeddingFnBackward>)
DEBUG [test_emb.py:200] 0:embed_value tensor([[362.5000, 362.5000, 362.5000],
        [474.0000, 474.0000, 474.0000],
        [775.0000, 775.0000, 775.0000],
        [700.0000, 700.0000, 700.0000],
        [218.0000, 218.0000, 218.0000],
        [632.0000, 632.0000, 632.0000],
        [568.0000, 568.0000, 568.0000],
        [ 11.5000,  11.5000,  11.5000],
        [277.0000, 277.0000, 277.0000],
        [645.0000, 645.0000, 645.0000]], device='cuda:0',
       grad_fn=<KnownShardedCachedEmbeddingFnBackward>)
 30%|███       | 6/20 [00:00<00:00, 14.16it/s]DEBUG [test_emb.py:176] emb._trace []
DEBUG [test_emb.py:177] emb._hand_grad []
 30%|███       | 6/20 [00:00<00:00, 14.13it/s]DEBUG [test_emb.py:176] emb._trace []
DEBUG [test_emb.py:188] 1:step6, input_keys tensor([565, 659, 289, 370,  31, 832,  12, 545, 775,  90], device='cuda:1')
DEBUG [test_emb.py:177] emb._hand_grad []
DEBUG [test_emb.py:188] 0:step6, input_keys tensor([119, 916, 881, 490, 682, 519, 944, 633, 169, 863], device='cuda:0')
DEBUG [test_emb.py:192] 0:std_embed_value tensor([[118.5000, 118.5000, 118.5000],
        [916.0000, 916.0000, 916.0000],
        [881.0000, 881.0000, 881.0000],
        [490.0000, 490.0000, 490.0000],
        [682.0000, 682.0000, 682.0000],
        [519.0000, 519.0000, 519.0000],
        [944.0000, 944.0000, 944.0000],
        [633.0000, 633.0000, 633.0000],
        [169.0000, 169.0000, 169.0000],
        [863.0000, 863.0000, 863.0000]], device='cuda:0',
       grad_fn=<EmbeddingBackward0>)
DEBUG [test_emb.py:194] emb._trace []
DEBUG [test_emb.py:195] emb._hand_grad []
DEBUG [test_emb.py:192] 1:std_embed_value tensor([[565.0000, 565.0000, 565.0000],
        [659.0000, 659.0000, 659.0000],
        [289.0000, 289.0000, 289.0000],
        [370.0000, 370.0000, 370.0000],
        [ 31.0000,  31.0000,  31.0000],
        [832.0000, 832.0000, 832.0000],
        [ 11.0000,  11.0000,  11.0000],
        [545.0000, 545.0000, 545.0000],
        [774.5000, 774.5000, 774.5000],
        [ 90.0000,  90.0000,  90.0000]], device='cuda:1',
       grad_fn=<EmbeddingBackward0>)
DEBUG [test_emb.py:194] emb._trace []
DEBUG [test_emb.py:195] emb._hand_grad []
DEBUG [sharded_cache.py:235] 1: a2a cache_query_values
DEBUG [sharded_cache.py:235] 0: a2a cache_query_values
DEBUG [sharded_cache.py:238] 0: a2a cache_query_values done
DEBUG [sharded_cache.py:238] 1: a2a cache_query_values done
DEBUG [utils.py:209] rank0: before sum sparse tensors
INFO [utils.py:213] rank0: after sum sparse tensors
DEBUG [sharded_cache.py:286] 1 missing_keys = tensor([565, 659, 289, 370, 832, 545, 775], device='cuda:1')
DEBUG [sharded_cache.py:286] 0 missing_keys = tensor([119, 916, 881, 490, 682, 519, 944, 633, 169, 863], device='cuda:0')
DEBUG [sharded_cache.py:287] 1 missing_grads = tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], device='cuda:1')
DEBUG [sharded_cache.py:287] 0 missing_grads = tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], device='cuda:0')
DEBUG [sharded_cache.py:292] reduced_missing_grads = tensor(indices=tensor([[119, 169, 289, 370, 490, 519, 545, 565, 633, 659, 682,
                        775, 832, 863, 881, 916, 944]]),
       values=tensor([[1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.]]),
       size=(1000, 3), nnz=17, layout=torch.sparse_coo)
DEBUG [DistEmb.py:116] record_grad tensor([119, 169, 289, 370, 490, 519, 545, 565, 633, 659, 682, 775, 832, 863,
        881, 916, 944]) tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]])
DEBUG [test_emb.py:200] 0:embed_value tensor([[118.5000, 118.5000, 118.5000],
        [916.0000, 916.0000, 916.0000],
        [881.0000, 881.0000, 881.0000],
        [490.0000, 490.0000, 490.0000],
        [682.0000, 682.0000, 682.0000],
        [519.0000, 519.0000, 519.0000],
        [944.0000, 944.0000, 944.0000],
        [633.0000, 633.0000, 633.0000],
        [169.0000, 169.0000, 169.0000],
        [863.0000, 863.0000, 863.0000]], device='cuda:0',
       grad_fn=<KnownShardedCachedEmbeddingFnBackward>)
DEBUG [test_emb.py:200] 1:embed_value tensor([[565.0000, 565.0000, 565.0000],
        [659.0000, 659.0000, 659.0000],
        [289.0000, 289.0000, 289.0000],
        [370.0000, 370.0000, 370.0000],
        [ 31.0000,  31.0000,  31.0000],
        [832.0000, 832.0000, 832.0000],
        [ 11.0000,  11.0000,  11.0000],
        [545.0000, 545.0000, 545.0000],
        [774.5000, 774.5000, 774.5000],
        [ 90.0000,  90.0000,  90.0000]], device='cuda:1',
       grad_fn=<KnownShardedCachedEmbeddingFnBackward>)
DEBUG [test_emb.py:176] emb._trace []
DEBUG [test_emb.py:177] emb._hand_grad []
DEBUG [test_emb.py:176] emb._trace []
DEBUG [test_emb.py:177] emb._hand_grad []
DEBUG [test_emb.py:188] 1:step7, input_keys tensor([919, 671, 346, 729,  40, 118, 977, 707, 143, 342], device='cuda:1')
DEBUG [test_emb.py:188] 0:step7, input_keys tensor([409, 233, 219,  78, 735,  83, 222, 858, 235, 416], device='cuda:0')
DEBUG [test_emb.py:192] 1:std_embed_value tensor([[919.0000, 919.0000, 919.0000],
        [671.0000, 671.0000, 671.0000],
        [345.5000, 345.5000, 345.5000],
        [729.0000, 729.0000, 729.0000],
        [ 40.0000,  40.0000,  40.0000],
        [118.0000, 118.0000, 118.0000],
        [977.0000, 977.0000, 977.0000],
        [707.0000, 707.0000, 707.0000],
        [143.0000, 143.0000, 143.0000],
        [342.0000, 342.0000, 342.0000]], device='cuda:1',
       grad_fn=<EmbeddingBackward0>)
DEBUG [test_emb.py:194] emb._trace []
DEBUG [test_emb.py:195] emb._hand_grad []
DEBUG [test_emb.py:192] 0:std_embed_value tensor([[409., 409., 409.],
        [233., 233., 233.],
        [219., 219., 219.],
        [ 78.,  78.,  78.],
        [735., 735., 735.],
        [ 83.,  83.,  83.],
        [222., 222., 222.],
        [858., 858., 858.],
        [235., 235., 235.],
        [416., 416., 416.]], device='cuda:0', grad_fn=<EmbeddingBackward0>)
DEBUG [test_emb.py:194] emb._trace []
DEBUG [test_emb.py:195] emb._hand_grad []
DEBUG [sharded_cache.py:235] 1: a2a cache_query_values
DEBUG [sharded_cache.py:235] 0: a2a cache_query_values
DEBUG [sharded_cache.py:238] 1: a2a cache_query_values done
DEBUG [sharded_cache.py:238] 0: a2a cache_query_values done
DEBUG [utils.py:209] rank0: before sum sparse tensors
INFO [utils.py:213] rank0: after sum sparse tensors
DEBUG [sharded_cache.py:286] 1 missing_keys = tensor([919, 671, 346, 729, 118, 977, 707, 143, 342], device='cuda:1')
DEBUG [sharded_cache.py:286] 0 missing_keys = tensor([409, 233, 219, 735, 222, 858, 235, 416], device='cuda:0')
DEBUG [sharded_cache.py:287] 1 missing_grads = tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], device='cuda:1')
DEBUG [sharded_cache.py:287] 0 missing_grads = tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], device='cuda:0')
DEBUG [sharded_cache.py:292] reduced_missing_grads = tensor(indices=tensor([[118, 143, 219, 222, 233, 235, 342, 346, 409, 416, 671,
                        707, 729, 735, 858, 919, 977]]),
       values=tensor([[1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.]]),
       size=(1000, 3), nnz=17, layout=torch.sparse_coo)
DEBUG [DistEmb.py:116] record_grad tensor([118, 143, 219, 222, 233, 235, 342, 346, 409, 416, 671, 707, 729, 735,
        858, 919, 977]) tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]])
DEBUG [test_emb.py:200] 1:embed_value tensor([[919.0000, 919.0000, 919.0000],
        [671.0000, 671.0000, 671.0000],
        [345.5000, 345.5000, 345.5000],
        [729.0000, 729.0000, 729.0000],
        [ 40.0000,  40.0000,  40.0000],
        [118.0000, 118.0000, 118.0000],
        [977.0000, 977.0000, 977.0000],
        [707.0000, 707.0000, 707.0000],
        [143.0000, 143.0000, 143.0000],
        [342.0000, 342.0000, 342.0000]], device='cuda:1',
       grad_fn=<KnownShardedCachedEmbeddingFnBackward>)
DEBUG [test_emb.py:176] emb._trace []
DEBUG [test_emb.py:177] emb._hand_grad []
DEBUG [test_emb.py:188] 1:step8, input_keys tensor([453,  73, 403, 446, 816,  16, 210,  50, 647, 175], device='cuda:1')
DEBUG [test_emb.py:200] 0:embed_value tensor([[409., 409., 409.],
        [233., 233., 233.],
        [219., 219., 219.],
        [ 78.,  78.,  78.],
        [735., 735., 735.],
        [ 83.,  83.,  83.],
        [222., 222., 222.],
        [858., 858., 858.],
        [235., 235., 235.],
        [416., 416., 416.]], device='cuda:0',
       grad_fn=<KnownShardedCachedEmbeddingFnBackward>)
DEBUG [test_emb.py:176] emb._trace []
DEBUG [test_emb.py:177] emb._hand_grad []
DEBUG [test_emb.py:188] 0:step8, input_keys tensor([505, 946, 156, 315, 284, 750, 108,  71, 847, 108], device='cuda:0')
DEBUG [test_emb.py:192] 1:std_embed_value tensor([[453.0000, 453.0000, 453.0000],
        [ 73.0000,  73.0000,  73.0000],
        [403.0000, 403.0000, 403.0000],
        [445.5000, 445.5000, 445.5000],
        [816.0000, 816.0000, 816.0000],
        [ 16.0000,  16.0000,  16.0000],
        [210.0000, 210.0000, 210.0000],
        [ 50.0000,  50.0000,  50.0000],
        [647.0000, 647.0000, 647.0000],
        [175.0000, 175.0000, 175.0000]], device='cuda:1',
       grad_fn=<EmbeddingBackward0>)
DEBUG [test_emb.py:194] emb._trace []
DEBUG [test_emb.py:195] emb._hand_grad []
DEBUG [test_emb.py:192] 0:std_embed_value tensor([[505.0000, 505.0000, 505.0000],
        [946.0000, 946.0000, 946.0000],
        [156.0000, 156.0000, 156.0000],
        [315.0000, 315.0000, 315.0000],
        [284.0000, 284.0000, 284.0000],
        [749.5000, 749.5000, 749.5000],
        [108.0000, 108.0000, 108.0000],
        [ 71.0000,  71.0000,  71.0000],
        [847.0000, 847.0000, 847.0000],
        [108.0000, 108.0000, 108.0000]], device='cuda:0',
       grad_fn=<EmbeddingBackward0>)
DEBUG [test_emb.py:194] emb._trace []
DEBUG [test_emb.py:195] emb._hand_grad []
DEBUG [sharded_cache.py:235] 1: a2a cache_query_values
DEBUG [sharded_cache.py:235] 0: a2a cache_query_values
DEBUG [sharded_cache.py:238] 1: a2a cache_query_values done
DEBUG [sharded_cache.py:238] 0: a2a cache_query_values done
DEBUG [utils.py:209] rank0: before sum sparse tensors
INFO [utils.py:213] rank0: after sum sparse tensors
DEBUG [sharded_cache.py:286] 1 missing_keys = tensor([453, 403, 446, 816, 210, 647, 175], device='cuda:1')
DEBUG [sharded_cache.py:286] 0 missing_keys = tensor([505, 946, 156, 315, 284, 750, 108, 847, 108], device='cuda:0')
DEBUG [sharded_cache.py:287] 1 missing_grads = tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], device='cuda:1')
DEBUG [sharded_cache.py:287] 0 missing_grads = tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], device='cuda:0')
DEBUG [sharded_cache.py:292] reduced_missing_grads = tensor(indices=tensor([[108, 156, 175, 210, 284, 315, 403, 446, 453, 505, 647,
                        750, 816, 847, 946]]),
       values=tensor([[2., 2., 2.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.]]),
       size=(1000, 3), nnz=15, layout=torch.sparse_coo)
DEBUG [DistEmb.py:116] record_grad tensor([108, 156, 175, 210, 284, 315, 403, 446, 453, 505, 647, 750, 816, 847,
        946]) tensor([[2., 2., 2.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]])
DEBUG [test_emb.py:200] 1:embed_value tensor([[453.0000, 453.0000, 453.0000],
        [ 73.0000,  73.0000,  73.0000],
        [403.0000, 403.0000, 403.0000],
        [445.5000, 445.5000, 445.5000],
        [816.0000, 816.0000, 816.0000],
        [ 16.0000,  16.0000,  16.0000],
        [210.0000, 210.0000, 210.0000],
        [ 50.0000,  50.0000,  50.0000],
        [647.0000, 647.0000, 647.0000],
        [175.0000, 175.0000, 175.0000]], device='cuda:1',
       grad_fn=<KnownShardedCachedEmbeddingFnBackward>)
DEBUG [test_emb.py:200] 0:embed_value tensor([[505.0000, 505.0000, 505.0000],
        [946.0000, 946.0000, 946.0000],
        [156.0000, 156.0000, 156.0000],
        [315.0000, 315.0000, 315.0000],
        [284.0000, 284.0000, 284.0000],
        [749.5000, 749.5000, 749.5000],
        [108.0000, 108.0000, 108.0000],
        [ 71.0000,  71.0000,  71.0000],
        [847.0000, 847.0000, 847.0000],
        [108.0000, 108.0000, 108.0000]], device='cuda:0',
       grad_fn=<KnownShardedCachedEmbeddingFnBackward>)
 45%|████▌     | 9/20 [00:00<00:00, 16.56it/s]DEBUG [test_emb.py:176] emb._trace []
DEBUG [test_emb.py:177] emb._hand_grad []
DEBUG [test_emb.py:188] 1:step9, input_keys tensor([383, 655, 345, 999, 419, 372,  10, 190, 278, 680], device='cuda:1')
 45%|████▌     | 9/20 [00:00<00:00, 16.51it/s]DEBUG [test_emb.py:176] emb._trace []
DEBUG [test_emb.py:177] emb._hand_grad []
DEBUG [test_emb.py:188] 0:step9, input_keys tensor([ 75,  84, 884, 148, 144, 534, 519, 260, 907, 514], device='cuda:0')
DEBUG [test_emb.py:192] 1:std_embed_value tensor([[383., 383., 383.],
        [655., 655., 655.],
        [345., 345., 345.],
        [998., 998., 998.],
        [419., 419., 419.],
        [372., 372., 372.],
        [ 10.,  10.,  10.],
        [190., 190., 190.],
        [278., 278., 278.],
        [680., 680., 680.]], device='cuda:1', grad_fn=<EmbeddingBackward0>)
DEBUG [test_emb.py:194] emb._trace []
DEBUG [test_emb.py:195] emb._hand_grad []
DEBUG [test_emb.py:192] 0:std_embed_value tensor([[ 75.0000,  75.0000,  75.0000],
        [ 84.0000,  84.0000,  84.0000],
        [884.0000, 884.0000, 884.0000],
        [148.0000, 148.0000, 148.0000],
        [144.0000, 144.0000, 144.0000],
        [534.0000, 534.0000, 534.0000],
        [518.5000, 518.5000, 518.5000],
        [260.0000, 260.0000, 260.0000],
        [907.0000, 907.0000, 907.0000],
        [514.0000, 514.0000, 514.0000]], device='cuda:0',
       grad_fn=<EmbeddingBackward0>)
DEBUG [test_emb.py:194] emb._trace []
DEBUG [test_emb.py:195] emb._hand_grad []
DEBUG [sharded_cache.py:235] 1: a2a cache_query_values
DEBUG [sharded_cache.py:235] 0: a2a cache_query_values
DEBUG [sharded_cache.py:238] 1: a2a cache_query_values done
DEBUG [sharded_cache.py:238] 0: a2a cache_query_values done
DEBUG [utils.py:209] rank0: before sum sparse tensors
INFO [utils.py:213] rank0: after sum sparse tensors
DEBUG [sharded_cache.py:286] 1 missing_keys = tensor([383, 655, 345, 999, 419, 372, 190, 278, 680], device='cuda:1')
DEBUG [sharded_cache.py:286] 0 missing_keys = tensor([884, 148, 144, 534, 519, 260, 907, 514], device='cuda:0')
DEBUG [sharded_cache.py:287] 1 missing_grads = tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], device='cuda:1')
DEBUG [sharded_cache.py:287] 0 missing_grads = tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], device='cuda:0')
DEBUG [sharded_cache.py:292] reduced_missing_grads = tensor(indices=tensor([[144, 148, 190, 260, 278, 345, 372, 383, 419, 514, 519,
                        534, 655, 680, 884, 907, 999]]),
       values=tensor([[1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.]]),
       size=(1000, 3), nnz=17, layout=torch.sparse_coo)
DEBUG [DistEmb.py:116] record_grad tensor([144, 148, 190, 260, 278, 345, 372, 383, 419, 514, 519, 534, 655, 680,
        884, 907, 999]) tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]])
DEBUG [test_emb.py:200] 1:embed_value tensor([[383., 383., 383.],
        [655., 655., 655.],
        [345., 345., 345.],
        [998., 998., 998.],
        [419., 419., 419.],
        [372., 372., 372.],
        [ 10.,  10.,  10.],
        [190., 190., 190.],
        [278., 278., 278.],
        [680., 680., 680.]], device='cuda:1',
       grad_fn=<KnownShardedCachedEmbeddingFnBackward>)
DEBUG [test_emb.py:200] 0:embed_value tensor([[ 75.0000,  75.0000,  75.0000],
        [ 84.0000,  84.0000,  84.0000],
        [884.0000, 884.0000, 884.0000],
        [148.0000, 148.0000, 148.0000],
        [144.0000, 144.0000, 144.0000],
        [534.0000, 534.0000, 534.0000],
        [518.5000, 518.5000, 518.5000],
        [260.0000, 260.0000, 260.0000],
        [907.0000, 907.0000, 907.0000],
        [514.0000, 514.0000, 514.0000]], device='cuda:0',
       grad_fn=<KnownShardedCachedEmbeddingFnBackward>)
DEBUG [test_emb.py:176] emb._trace []
DEBUG [test_emb.py:177] emb._hand_grad []
DEBUG [test_emb.py:188] 1:step10, input_keys tensor([969, 829, 403, 504, 472, 501, 549, 406, 308, 909], device='cuda:1')
DEBUG [test_emb.py:176] emb._trace []
DEBUG [test_emb.py:177] emb._hand_grad []
DEBUG [test_emb.py:188] 0:step10, input_keys tensor([133, 120, 249, 389, 793, 611, 528, 231,  77, 758], device='cuda:0')
DEBUG [test_emb.py:192] 1:std_embed_value tensor([[969.0000, 969.0000, 969.0000],
        [828.5000, 828.5000, 828.5000],
        [402.5000, 402.5000, 402.5000],
        [504.0000, 504.0000, 504.0000],
        [472.0000, 472.0000, 472.0000],
        [501.0000, 501.0000, 501.0000],
        [548.5000, 548.5000, 548.5000],
        [406.0000, 406.0000, 406.0000],
        [308.0000, 308.0000, 308.0000],
        [909.0000, 909.0000, 909.0000]], device='cuda:1',
       grad_fn=<EmbeddingBackward0>)
DEBUG [test_emb.py:194] emb._trace []
DEBUG [test_emb.py:195] emb._hand_grad []
DEBUG [test_emb.py:192] 0:std_embed_value tensor([[133., 133., 133.],
        [120., 120., 120.],
        [248., 248., 248.],
        [389., 389., 389.],
        [793., 793., 793.],
        [611., 611., 611.],
        [528., 528., 528.],
        [231., 231., 231.],
        [ 77.,  77.,  77.],
        [758., 758., 758.]], device='cuda:0', grad_fn=<EmbeddingBackward0>)
DEBUG [test_emb.py:194] emb._trace []
DEBUG [test_emb.py:195] emb._hand_grad []
DEBUG [sharded_cache.py:235] 0: a2a cache_query_values
DEBUG [sharded_cache.py:235] 1: a2a cache_query_values
DEBUG [sharded_cache.py:238] 1: a2a cache_query_values done
DEBUG [sharded_cache.py:238] 0: a2a cache_query_values done
DEBUG [utils.py:209] rank0: before sum sparse tensors
INFO [utils.py:213] rank0: after sum sparse tensors
DEBUG [sharded_cache.py:286] 1 missing_keys = tensor([969, 829, 403, 504, 472, 501, 549, 406, 308, 909], device='cuda:1')
DEBUG [sharded_cache.py:286] 0 missing_keys = tensor([133, 120, 249, 389, 793, 611, 528, 231, 758], device='cuda:0')
DEBUG [sharded_cache.py:287] 1 missing_grads = tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], device='cuda:1')
DEBUG [sharded_cache.py:287] 0 missing_grads = tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], device='cuda:0')
DEBUG [sharded_cache.py:292] reduced_missing_grads = tensor(indices=tensor([[120, 133, 231, 249, 308, 389, 403, 406, 472, 501, 504,
                        528, 549, 611, 758, 793, 829, 909, 969]]),
       values=tensor([[1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.]]),
       size=(1000, 3), nnz=19, layout=torch.sparse_coo)
DEBUG [DistEmb.py:116] record_grad tensor([120, 133, 231, 249, 308, 389, 403, 406, 472, 501, 504, 528, 549, 611,
        758, 793, 829, 909, 969]) tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]])
DEBUG [test_emb.py:200] 1:embed_value tensor([[969.0000, 969.0000, 969.0000],
        [828.5000, 828.5000, 828.5000],
        [402.5000, 402.5000, 402.5000],
        [504.0000, 504.0000, 504.0000],
        [472.0000, 472.0000, 472.0000],
        [501.0000, 501.0000, 501.0000],
        [548.5000, 548.5000, 548.5000],
        [406.0000, 406.0000, 406.0000],
        [308.0000, 308.0000, 308.0000],
        [909.0000, 909.0000, 909.0000]], device='cuda:1',
       grad_fn=<KnownShardedCachedEmbeddingFnBackward>)
DEBUG [test_emb.py:176] emb._trace []
DEBUG [test_emb.py:177] emb._hand_grad []
DEBUG [test_emb.py:200] 0:embed_value tensor([[133., 133., 133.],
        [120., 120., 120.],
        [248., 248., 248.],
        [389., 389., 389.],
        [793., 793., 793.],
        [611., 611., 611.],
        [528., 528., 528.],
        [231., 231., 231.],
        [ 77.,  77.,  77.],
        [758., 758., 758.]], device='cuda:0',
       grad_fn=<KnownShardedCachedEmbeddingFnBackward>)
DEBUG [test_emb.py:188] 1:step11, input_keys tensor([878, 458, 596, 867, 709,  19, 671, 597, 352, 235], device='cuda:1')
 55%|█████▌    | 11/20 [00:00<00:00, 17.36it/s]DEBUG [test_emb.py:176] emb._trace []
DEBUG [test_emb.py:177] emb._hand_grad []
DEBUG [test_emb.py:188] 0:step11, input_keys tensor([784, 601, 970,  84, 490, 984, 469, 227, 570,  10], device='cuda:0')
DEBUG [test_emb.py:192] 1:std_embed_value tensor([[877.5000, 877.5000, 877.5000],
        [458.0000, 458.0000, 458.0000],
        [596.0000, 596.0000, 596.0000],
        [867.0000, 867.0000, 867.0000],
        [709.0000, 709.0000, 709.0000],
        [ 19.0000,  19.0000,  19.0000],
        [670.5000, 670.5000, 670.5000],
        [597.0000, 597.0000, 597.0000],
        [352.0000, 352.0000, 352.0000],
        [234.5000, 234.5000, 234.5000]], device='cuda:1',
       grad_fn=<EmbeddingBackward0>)
DEBUG [test_emb.py:194] emb._trace []
DEBUG [test_emb.py:195] emb._hand_grad []
DEBUG [test_emb.py:192] 0:std_embed_value tensor([[784.0000, 784.0000, 784.0000],
        [601.0000, 601.0000, 601.0000],
        [970.0000, 970.0000, 970.0000],
        [ 83.5000,  83.5000,  83.5000],
        [489.5000, 489.5000, 489.5000],
        [984.0000, 984.0000, 984.0000],
        [469.0000, 469.0000, 469.0000],
        [227.0000, 227.0000, 227.0000],
        [570.0000, 570.0000, 570.0000],
        [  9.5000,   9.5000,   9.5000]], device='cuda:0',
       grad_fn=<EmbeddingBackward0>)
DEBUG [test_emb.py:194] emb._trace []
DEBUG [test_emb.py:195] emb._hand_grad []
DEBUG [sharded_cache.py:235] 1: a2a cache_query_values
DEBUG [sharded_cache.py:235] 0: a2a cache_query_values
DEBUG [sharded_cache.py:238] 1: a2a cache_query_values done
DEBUG [sharded_cache.py:238] 0: a2a cache_query_values done
DEBUG [utils.py:209] rank0: before sum sparse tensors
INFO [utils.py:213] rank0: after sum sparse tensors
DEBUG [sharded_cache.py:286] 1 missing_keys = tensor([878, 458, 596, 867, 709, 671, 597, 352, 235], device='cuda:1')
DEBUG [sharded_cache.py:286] 0 missing_keys = tensor([784, 601, 970, 490, 984, 469, 227, 570], device='cuda:0')
DEBUG [sharded_cache.py:287] 1 missing_grads = tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], device='cuda:1')
DEBUG [sharded_cache.py:287] 0 missing_grads = tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], device='cuda:0')
DEBUG [sharded_cache.py:292] reduced_missing_grads = tensor(indices=tensor([[227, 235, 352, 458, 469, 490, 570, 596, 597, 601, 671,
                        709, 784, 867, 878, 970, 984]]),
       values=tensor([[1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.]]),
       size=(1000, 3), nnz=17, layout=torch.sparse_coo)
DEBUG [DistEmb.py:116] record_grad tensor([227, 235, 352, 458, 469, 490, 570, 596, 597, 601, 671, 709, 784, 867,
        878, 970, 984]) tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]])
DEBUG [test_emb.py:200] 1:embed_value tensor([[877.5000, 877.5000, 877.5000],
        [458.0000, 458.0000, 458.0000],
        [596.0000, 596.0000, 596.0000],
        [867.0000, 867.0000, 867.0000],
        [709.0000, 709.0000, 709.0000],
        [ 19.0000,  19.0000,  19.0000],
        [670.5000, 670.5000, 670.5000],
        [597.0000, 597.0000, 597.0000],
        [352.0000, 352.0000, 352.0000],
        [234.5000, 234.5000, 234.5000]], device='cuda:1',
       grad_fn=<KnownShardedCachedEmbeddingFnBackward>)
DEBUG [test_emb.py:200] 0:embed_value tensor([[784.0000, 784.0000, 784.0000],
        [601.0000, 601.0000, 601.0000],
        [970.0000, 970.0000, 970.0000],
        [ 83.5000,  83.5000,  83.5000],
        [489.5000, 489.5000, 489.5000],
        [984.0000, 984.0000, 984.0000],
        [469.0000, 469.0000, 469.0000],
        [227.0000, 227.0000, 227.0000],
        [570.0000, 570.0000, 570.0000],
        [  9.5000,   9.5000,   9.5000]], device='cuda:0',
       grad_fn=<KnownShardedCachedEmbeddingFnBackward>)
 60%|██████    | 12/20 [00:00<00:00, 17.95it/s]DEBUG [test_emb.py:176] emb._trace []
DEBUG [test_emb.py:177] emb._hand_grad []
DEBUG [test_emb.py:188] 1:step12, input_keys tensor([996, 455, 855, 349,  12, 281, 992, 100, 426, 816], device='cuda:1')
DEBUG [test_emb.py:176] emb._trace []
DEBUG [test_emb.py:177] emb._hand_grad []
DEBUG [test_emb.py:188] 0:step12, input_keys tensor([905, 399, 272, 962, 642, 971, 314, 239, 471,  11], device='cuda:0')
DEBUG [test_emb.py:192] 1:std_embed_value tensor([[995.5000, 995.5000, 995.5000],
        [455.0000, 455.0000, 455.0000],
        [855.0000, 855.0000, 855.0000],
        [349.0000, 349.0000, 349.0000],
        [ 10.5000,  10.5000,  10.5000],
        [281.0000, 281.0000, 281.0000],
        [992.0000, 992.0000, 992.0000],
        [100.0000, 100.0000, 100.0000],
        [426.0000, 426.0000, 426.0000],
        [815.5000, 815.5000, 815.5000]], device='cuda:1',
       grad_fn=<EmbeddingBackward0>)
DEBUG [test_emb.py:194] emb._trace []
DEBUG [test_emb.py:195] emb._hand_grad []
DEBUG [test_emb.py:192] 0:std_embed_value tensor([[905.0000, 905.0000, 905.0000],
        [398.5000, 398.5000, 398.5000],
        [272.0000, 272.0000, 272.0000],
        [962.0000, 962.0000, 962.0000],
        [642.0000, 642.0000, 642.0000],
        [971.0000, 971.0000, 971.0000],
        [314.0000, 314.0000, 314.0000],
        [239.0000, 239.0000, 239.0000],
        [471.0000, 471.0000, 471.0000],
        [ 11.0000,  11.0000,  11.0000]], device='cuda:0',
       grad_fn=<EmbeddingBackward0>)
DEBUG [test_emb.py:194] emb._trace []
DEBUG [test_emb.py:195] emb._hand_grad []
DEBUG [sharded_cache.py:235] 1: a2a cache_query_values
DEBUG [sharded_cache.py:235] 0: a2a cache_query_values
DEBUG [sharded_cache.py:238] 0: a2a cache_query_values done
DEBUG [sharded_cache.py:238] 1: a2a cache_query_values done
DEBUG [utils.py:209] rank0: before sum sparse tensors
INFO [utils.py:213] rank0: after sum sparse tensors
DEBUG [sharded_cache.py:286] 1 missing_keys = tensor([996, 455, 855, 349, 281, 992, 100, 426, 816], device='cuda:1')
DEBUG [sharded_cache.py:286] 0 missing_keys = tensor([905, 399, 272, 962, 642, 971, 314, 239, 471], device='cuda:0')
DEBUG [sharded_cache.py:287] 1 missing_grads = tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], device='cuda:1')
DEBUG [sharded_cache.py:287] 0 missing_grads = tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], device='cuda:0')
DEBUG [sharded_cache.py:292] reduced_missing_grads = tensor(indices=tensor([[100, 239, 272, 281, 314, 349, 399, 426, 455, 471, 642,
                        816, 855, 905, 962, 971, 992, 996]]),
       values=tensor([[1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.]]),
       size=(1000, 3), nnz=18, layout=torch.sparse_coo)
DEBUG [DistEmb.py:116] record_grad tensor([100, 239, 272, 281, 314, 349, 399, 426, 455, 471, 642, 816, 855, 905,
        962, 971, 992, 996]) tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]])
DEBUG [test_emb.py:200] 1:embed_value tensor([[995.5000, 995.5000, 995.5000],
        [455.0000, 455.0000, 455.0000],
        [855.0000, 855.0000, 855.0000],
        [349.0000, 349.0000, 349.0000],
        [ 10.5000,  10.5000,  10.5000],
        [281.0000, 281.0000, 281.0000],
        [992.0000, 992.0000, 992.0000],
        [100.0000, 100.0000, 100.0000],
        [426.0000, 426.0000, 426.0000],
        [815.5000, 815.5000, 815.5000]], device='cuda:1',
       grad_fn=<KnownShardedCachedEmbeddingFnBackward>)
DEBUG [test_emb.py:200] 0:embed_value tensor([[905.0000, 905.0000, 905.0000],
        [398.5000, 398.5000, 398.5000],
        [272.0000, 272.0000, 272.0000],
        [962.0000, 962.0000, 962.0000],
        [642.0000, 642.0000, 642.0000],
        [971.0000, 971.0000, 971.0000],
        [314.0000, 314.0000, 314.0000],
        [239.0000, 239.0000, 239.0000],
        [471.0000, 471.0000, 471.0000],
        [ 11.0000,  11.0000,  11.0000]], device='cuda:0',
       grad_fn=<KnownShardedCachedEmbeddingFnBackward>)
DEBUG [test_emb.py:176] emb._trace []
DEBUG [test_emb.py:177] emb._hand_grad []
DEBUG [test_emb.py:188] 1:step13, input_keys tensor([613, 282, 775, 140, 729, 471, 209, 824, 954,  20], device='cuda:1')
DEBUG [test_emb.py:176] emb._trace []
DEBUG [test_emb.py:177] emb._hand_grad []
DEBUG [test_emb.py:188] 0:step13, input_keys tensor([650, 273, 330, 166, 987, 326, 419, 971, 106, 694], device='cuda:0')
DEBUG [test_emb.py:192] 1:std_embed_value tensor([[613.0000, 613.0000, 613.0000],
        [282.0000, 282.0000, 282.0000],
        [774.0000, 774.0000, 774.0000],
        [140.0000, 140.0000, 140.0000],
        [728.5000, 728.5000, 728.5000],
        [470.5000, 470.5000, 470.5000],
        [209.0000, 209.0000, 209.0000],
        [824.0000, 824.0000, 824.0000],
        [954.0000, 954.0000, 954.0000],
        [ 20.0000,  20.0000,  20.0000]], device='cuda:1',
       grad_fn=<EmbeddingBackward0>)
DEBUG [test_emb.py:194] emb._trace []
DEBUG [test_emb.py:195] emb._hand_grad []
DEBUG [test_emb.py:192] 0:std_embed_value tensor([[650.0000, 650.0000, 650.0000],
        [272.5000, 272.5000, 272.5000],
        [330.0000, 330.0000, 330.0000],
        [165.5000, 165.5000, 165.5000],
        [987.0000, 987.0000, 987.0000],
        [325.5000, 325.5000, 325.5000],
        [418.5000, 418.5000, 418.5000],
        [970.5000, 970.5000, 970.5000],
        [106.0000, 106.0000, 106.0000],
        [694.0000, 694.0000, 694.0000]], device='cuda:0',
       grad_fn=<EmbeddingBackward0>)
DEBUG [test_emb.py:194] emb._trace []
DEBUG [test_emb.py:195] emb._hand_grad []
DEBUG [sharded_cache.py:235] 1: a2a cache_query_values
DEBUG [sharded_cache.py:235] 0: a2a cache_query_values
DEBUG [sharded_cache.py:238] 0: a2a cache_query_values done
DEBUG [sharded_cache.py:238] 1: a2a cache_query_values done
DEBUG [utils.py:209] rank0: before sum sparse tensors
INFO [utils.py:213] rank0: after sum sparse tensors
DEBUG [sharded_cache.py:286] 1 missing_keys = tensor([613, 282, 775, 140, 729, 471, 209, 824, 954], device='cuda:1')
DEBUG [sharded_cache.py:286] 0 missing_keys = tensor([650, 273, 330, 166, 987, 326, 419, 971, 106, 694], device='cuda:0')
DEBUG [sharded_cache.py:287] 1 missing_grads = tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], device='cuda:1')
DEBUG [sharded_cache.py:287] 0 missing_grads = tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], device='cuda:0')
DEBUG [sharded_cache.py:292] reduced_missing_grads = tensor(indices=tensor([[106, 140, 166, 209, 273, 282, 326, 330, 419, 471, 613,
                        650, 694, 729, 775, 824, 954, 971, 987]]),
       values=tensor([[1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.]]),
       size=(1000, 3), nnz=19, layout=torch.sparse_coo)
DEBUG [DistEmb.py:116] record_grad tensor([106, 140, 166, 209, 273, 282, 326, 330, 419, 471, 613, 650, 694, 729,
        775, 824, 954, 971, 987]) tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]])
DEBUG [test_emb.py:200] 1:embed_value tensor([[613.0000, 613.0000, 613.0000],
        [282.0000, 282.0000, 282.0000],
        [774.0000, 774.0000, 774.0000],
        [140.0000, 140.0000, 140.0000],
        [728.5000, 728.5000, 728.5000],
        [470.5000, 470.5000, 470.5000],
        [209.0000, 209.0000, 209.0000],
        [824.0000, 824.0000, 824.0000],
        [954.0000, 954.0000, 954.0000],
        [ 20.0000,  20.0000,  20.0000]], device='cuda:1',
       grad_fn=<KnownShardedCachedEmbeddingFnBackward>)
DEBUG [test_emb.py:200] 0:embed_value tensor([[650.0000, 650.0000, 650.0000],
        [272.5000, 272.5000, 272.5000],
        [330.0000, 330.0000, 330.0000],
        [165.5000, 165.5000, 165.5000],
        [987.0000, 987.0000, 987.0000],
        [325.5000, 325.5000, 325.5000],
        [418.5000, 418.5000, 418.5000],
        [970.5000, 970.5000, 970.5000],
        [106.0000, 106.0000, 106.0000],
        [694.0000, 694.0000, 694.0000]], device='cuda:0',
       grad_fn=<KnownShardedCachedEmbeddingFnBackward>)
 70%|███████   | 14/20 [00:00<00:00, 18.41it/s]DEBUG [test_emb.py:176] emb._trace []
DEBUG [test_emb.py:177] emb._hand_grad []
DEBUG [test_emb.py:188] 1:step14, input_keys tensor([662, 332, 457, 808, 538, 163, 785, 274, 188, 844], device='cuda:1')
 70%|███████   | 14/20 [00:00<00:00, 18.53it/s]DEBUG [test_emb.py:176] emb._trace []
DEBUG [test_emb.py:177] emb._hand_grad []
DEBUG [test_emb.py:188] 0:step14, input_keys tensor([275, 118, 401, 818, 361, 412, 778, 150, 243, 728], device='cuda:0')
DEBUG [test_emb.py:192] 1:std_embed_value tensor([[662.0000, 662.0000, 662.0000],
        [332.0000, 332.0000, 332.0000],
        [457.0000, 457.0000, 457.0000],
        [808.0000, 808.0000, 808.0000],
        [538.0000, 538.0000, 538.0000],
        [163.0000, 163.0000, 163.0000],
        [784.5000, 784.5000, 784.5000],
        [274.0000, 274.0000, 274.0000],
        [188.0000, 188.0000, 188.0000],
        [844.0000, 844.0000, 844.0000]], device='cuda:1',
       grad_fn=<EmbeddingBackward0>)
DEBUG [test_emb.py:194] emb._trace []
DEBUG [test_emb.py:195] emb._hand_grad []
DEBUG [test_emb.py:192] 0:std_embed_value tensor([[275.0000, 275.0000, 275.0000],
        [117.5000, 117.5000, 117.5000],
        [401.0000, 401.0000, 401.0000],
        [818.0000, 818.0000, 818.0000],
        [361.0000, 361.0000, 361.0000],
        [412.0000, 412.0000, 412.0000],
        [777.5000, 777.5000, 777.5000],
        [150.0000, 150.0000, 150.0000],
        [243.0000, 243.0000, 243.0000],
        [728.0000, 728.0000, 728.0000]], device='cuda:0',
       grad_fn=<EmbeddingBackward0>)
DEBUG [test_emb.py:194] emb._trace []
DEBUG [test_emb.py:195] emb._hand_grad []
DEBUG [sharded_cache.py:235] 1: a2a cache_query_values
DEBUG [sharded_cache.py:235] 0: a2a cache_query_values
DEBUG [sharded_cache.py:238] 1: a2a cache_query_values done
DEBUG [sharded_cache.py:238] 0: a2a cache_query_values done
DEBUG [utils.py:209] rank0: before sum sparse tensors
INFO [utils.py:213] rank0: after sum sparse tensors
DEBUG [sharded_cache.py:286] 1 missing_keys = tensor([662, 332, 457, 808, 538, 163, 785, 274, 188, 844], device='cuda:1')
DEBUG [sharded_cache.py:286] 0 missing_keys = tensor([275, 118, 401, 818, 361, 412, 778, 150, 243, 728], device='cuda:0')
DEBUG [sharded_cache.py:287] 1 missing_grads = tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], device='cuda:1')
DEBUG [sharded_cache.py:287] 0 missing_grads = tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], device='cuda:0')
DEBUG [sharded_cache.py:292] reduced_missing_grads = tensor(indices=tensor([[118, 150, 163, 188, 243, 274, 275, 332, 361, 401, 412,
                        457, 538, 662, 728, 778, 785, 808, 818, 844]]),
       values=tensor([[1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.]]),
       size=(1000, 3), nnz=20, layout=torch.sparse_coo)
DEBUG [DistEmb.py:116] record_grad tensor([118, 150, 163, 188, 243, 274, 275, 332, 361, 401, 412, 457, 538, 662,
        728, 778, 785, 808, 818, 844]) tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]])
DEBUG [test_emb.py:200] 0:embed_value tensor([[275.0000, 275.0000, 275.0000],
        [117.5000, 117.5000, 117.5000],
        [401.0000, 401.0000, 401.0000],
        [818.0000, 818.0000, 818.0000],
        [361.0000, 361.0000, 361.0000],
        [412.0000, 412.0000, 412.0000],
        [777.5000, 777.5000, 777.5000],
        [150.0000, 150.0000, 150.0000],
        [243.0000, 243.0000, 243.0000],
        [728.0000, 728.0000, 728.0000]], device='cuda:0',
       grad_fn=<KnownShardedCachedEmbeddingFnBackward>)
DEBUG [test_emb.py:200] 1:embed_value tensor([[662.0000, 662.0000, 662.0000],
        [332.0000, 332.0000, 332.0000],
        [457.0000, 457.0000, 457.0000],
        [808.0000, 808.0000, 808.0000],
        [538.0000, 538.0000, 538.0000],
        [163.0000, 163.0000, 163.0000],
        [784.5000, 784.5000, 784.5000],
        [274.0000, 274.0000, 274.0000],
        [188.0000, 188.0000, 188.0000],
        [844.0000, 844.0000, 844.0000]], device='cuda:1',
       grad_fn=<KnownShardedCachedEmbeddingFnBackward>)
DEBUG [test_emb.py:176] emb._trace []
DEBUG [test_emb.py:177] emb._hand_grad []
DEBUG [test_emb.py:176] emb._trace []
DEBUG [test_emb.py:177] emb._hand_grad []
DEBUG [test_emb.py:188] 1:step15, input_keys tensor([319,  74, 898, 390, 448, 889, 226, 710, 110, 738], device='cuda:1')
DEBUG [test_emb.py:188] 0:step15, input_keys tensor([739, 682, 507, 760, 888, 878, 696, 398, 915, 118], device='cuda:0')
DEBUG [test_emb.py:192] 0:std_embed_value tensor([[739.0000, 739.0000, 739.0000],
        [681.5000, 681.5000, 681.5000],
        [507.0000, 507.0000, 507.0000],
        [760.0000, 760.0000, 760.0000],
        [888.0000, 888.0000, 888.0000],
        [877.0000, 877.0000, 877.0000],
        [696.0000, 696.0000, 696.0000],
        [398.0000, 398.0000, 398.0000],
        [915.0000, 915.0000, 915.0000],
        [117.0000, 117.0000, 117.0000]], device='cuda:0',
       grad_fn=<EmbeddingBackward0>)
DEBUG [test_emb.py:194] emb._trace []
DEBUG [test_emb.py:195] emb._hand_grad []
DEBUG [test_emb.py:192] 1:std_embed_value tensor([[319., 319., 319.],
        [ 74.,  74.,  74.],
        [898., 898., 898.],
        [390., 390., 390.],
        [448., 448., 448.],
        [889., 889., 889.],
        [226., 226., 226.],
        [710., 710., 710.],
        [110., 110., 110.],
        [738., 738., 738.]], device='cuda:1', grad_fn=<EmbeddingBackward0>)
DEBUG [test_emb.py:194] emb._trace []
DEBUG [test_emb.py:195] emb._hand_grad []
DEBUG [sharded_cache.py:235] 0: a2a cache_query_values
DEBUG [sharded_cache.py:235] 1: a2a cache_query_values
DEBUG [sharded_cache.py:238] 1: a2a cache_query_values done
DEBUG [sharded_cache.py:238] 0: a2a cache_query_values done
DEBUG [utils.py:209] rank0: before sum sparse tensors
INFO [utils.py:213] rank0: after sum sparse tensors
DEBUG [sharded_cache.py:286] 1 missing_keys = tensor([319, 898, 390, 448, 889, 226, 710, 110, 738], device='cuda:1')
DEBUG [sharded_cache.py:286] 0 missing_keys = tensor([739, 682, 507, 760, 888, 878, 696, 398, 915, 118], device='cuda:0')
DEBUG [sharded_cache.py:287] 1 missing_grads = tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], device='cuda:1')
DEBUG [sharded_cache.py:287] 0 missing_grads = tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], device='cuda:0')
DEBUG [sharded_cache.py:292] reduced_missing_grads = tensor(indices=tensor([[110, 118, 226, 319, 390, 398, 448, 507, 682, 696, 710,
                        738, 739, 760, 878, 888, 889, 898, 915]]),
       values=tensor([[1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.]]),
       size=(1000, 3), nnz=19, layout=torch.sparse_coo)
DEBUG [DistEmb.py:116] record_grad tensor([110, 118, 226, 319, 390, 398, 448, 507, 682, 696, 710, 738, 739, 760,
        878, 888, 889, 898, 915]) tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]])
DEBUG [test_emb.py:200] 0:embed_value tensor([[739.0000, 739.0000, 739.0000],
        [681.5000, 681.5000, 681.5000],
        [507.0000, 507.0000, 507.0000],
        [760.0000, 760.0000, 760.0000],
        [888.0000, 888.0000, 888.0000],
        [877.0000, 877.0000, 877.0000],
        [696.0000, 696.0000, 696.0000],
        [398.0000, 398.0000, 398.0000],
        [915.0000, 915.0000, 915.0000],
        [117.0000, 117.0000, 117.0000]], device='cuda:0',
       grad_fn=<KnownShardedCachedEmbeddingFnBackward>)
DEBUG [test_emb.py:200] 1:embed_value tensor([[319., 319., 319.],
        [ 74.,  74.,  74.],
        [898., 898., 898.],
        [390., 390., 390.],
        [448., 448., 448.],
        [889., 889., 889.],
        [226., 226., 226.],
        [710., 710., 710.],
        [110., 110., 110.],
        [738., 738., 738.]], device='cuda:1',
       grad_fn=<KnownShardedCachedEmbeddingFnBackward>)
DEBUG [test_emb.py:176] emb._trace []
DEBUG [test_emb.py:177] emb._hand_grad []
DEBUG [test_emb.py:176] emb._trace []
DEBUG [test_emb.py:177] emb._hand_grad []
DEBUG [test_emb.py:188] 0:step16, input_keys tensor([312, 677, 942, 584, 869, 418, 395, 606, 274, 843], device='cuda:0')
DEBUG [test_emb.py:188] 1:step16, input_keys tensor([951, 349, 233, 586, 172, 814, 328,  46, 325, 445], device='cuda:1')
DEBUG [test_emb.py:192] 1:std_embed_value tensor([[951.0000, 951.0000, 951.0000],
        [348.5000, 348.5000, 348.5000],
        [232.5000, 232.5000, 232.5000],
        [586.0000, 586.0000, 586.0000],
        [172.0000, 172.0000, 172.0000],
        [814.0000, 814.0000, 814.0000],
        [328.0000, 328.0000, 328.0000],
        [ 46.0000,  46.0000,  46.0000],
        [325.0000, 325.0000, 325.0000],
        [444.5000, 444.5000, 444.5000]], device='cuda:1',
       grad_fn=<EmbeddingBackward0>)
DEBUG [test_emb.py:194] emb._trace []
DEBUG [test_emb.py:195] emb._hand_grad []
DEBUG [test_emb.py:192] 0:std_embed_value tensor([[312.0000, 312.0000, 312.0000],
        [677.0000, 677.0000, 677.0000],
        [942.0000, 942.0000, 942.0000],
        [584.0000, 584.0000, 584.0000],
        [869.0000, 869.0000, 869.0000],
        [418.0000, 418.0000, 418.0000],
        [394.5000, 394.5000, 394.5000],
        [606.0000, 606.0000, 606.0000],
        [273.5000, 273.5000, 273.5000],
        [843.0000, 843.0000, 843.0000]], device='cuda:0',
       grad_fn=<EmbeddingBackward0>)
DEBUG [test_emb.py:194] emb._trace []
DEBUG [test_emb.py:195] emb._hand_grad []
DEBUG [sharded_cache.py:235] 1: a2a cache_query_values
DEBUG [sharded_cache.py:235] 0: a2a cache_query_values
DEBUG [sharded_cache.py:238] 0: a2a cache_query_values done
DEBUG [sharded_cache.py:238] 1: a2a cache_query_values done
DEBUG [utils.py:209] rank0: before sum sparse tensors
INFO [utils.py:213] rank0: after sum sparse tensors
DEBUG [sharded_cache.py:286] 1 missing_keys = tensor([951, 349, 233, 586, 172, 814, 328, 325, 445], device='cuda:1')
DEBUG [sharded_cache.py:286] 0 missing_keys = tensor([312, 677, 942, 584, 869, 418, 395, 606, 274, 843], device='cuda:0')
DEBUG [sharded_cache.py:287] 1 missing_grads = tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], device='cuda:1')
DEBUG [sharded_cache.py:287] 0 missing_grads = tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], device='cuda:0')
DEBUG [sharded_cache.py:292] reduced_missing_grads = tensor(indices=tensor([[172, 233, 274, 312, 325, 328, 349, 395, 418, 445, 584,
                        586, 606, 677, 814, 843, 869, 942, 951]]),
       values=tensor([[1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.]]),
       size=(1000, 3), nnz=19, layout=torch.sparse_coo)
DEBUG [DistEmb.py:116] record_grad tensor([172, 233, 274, 312, 325, 328, 349, 395, 418, 445, 584, 586, 606, 677,
        814, 843, 869, 942, 951]) tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]])
DEBUG [test_emb.py:200] 1:embed_value tensor([[951.0000, 951.0000, 951.0000],
        [348.5000, 348.5000, 348.5000],
        [232.5000, 232.5000, 232.5000],
        [586.0000, 586.0000, 586.0000],
        [172.0000, 172.0000, 172.0000],
        [814.0000, 814.0000, 814.0000],
        [328.0000, 328.0000, 328.0000],
        [ 46.0000,  46.0000,  46.0000],
        [325.0000, 325.0000, 325.0000],
        [444.5000, 444.5000, 444.5000]], device='cuda:1',
       grad_fn=<KnownShardedCachedEmbeddingFnBackward>)
DEBUG [test_emb.py:200] 0:embed_value tensor([[312.0000, 312.0000, 312.0000],
        [677.0000, 677.0000, 677.0000],
        [942.0000, 942.0000, 942.0000],
        [584.0000, 584.0000, 584.0000],
        [869.0000, 869.0000, 869.0000],
        [418.0000, 418.0000, 418.0000],
        [394.5000, 394.5000, 394.5000],
        [606.0000, 606.0000, 606.0000],
        [273.5000, 273.5000, 273.5000],
        [843.0000, 843.0000, 843.0000]], device='cuda:0',
       grad_fn=<KnownShardedCachedEmbeddingFnBackward>)
 85%|████████▌ | 17/20 [00:01<00:00, 19.60it/s]DEBUG [test_emb.py:176] emb._trace []
DEBUG [test_emb.py:177] emb._hand_grad []
DEBUG [test_emb.py:188] 1:step17, input_keys tensor([937, 866, 737,  85, 316, 298, 712, 711, 525, 472], device='cuda:1')
 85%|████████▌ | 17/20 [00:01<00:00, 19.56it/s]DEBUG [test_emb.py:176] emb._trace []
DEBUG [test_emb.py:177] emb._hand_grad []
DEBUG [test_emb.py:188] 0:step17, input_keys tensor([333,  64, 600, 512, 915, 151, 712, 578, 202, 144], device='cuda:0')
DEBUG [test_emb.py:192] 1:std_embed_value tensor([[937.0000, 937.0000, 937.0000],
        [865.5000, 865.5000, 865.5000],
        [737.0000, 737.0000, 737.0000],
        [ 84.5000,  84.5000,  84.5000],
        [316.0000, 316.0000, 316.0000],
        [298.0000, 298.0000, 298.0000],
        [712.0000, 712.0000, 712.0000],
        [710.5000, 710.5000, 710.5000],
        [525.0000, 525.0000, 525.0000],
        [471.5000, 471.5000, 471.5000]], device='cuda:1',
       grad_fn=<EmbeddingBackward0>)
DEBUG [test_emb.py:194] emb._trace []
DEBUG [test_emb.py:195] emb._hand_grad []
DEBUG [test_emb.py:192] 0:std_embed_value tensor([[333.0000, 333.0000, 333.0000],
        [ 64.0000,  64.0000,  64.0000],
        [600.0000, 600.0000, 600.0000],
        [512.0000, 512.0000, 512.0000],
        [914.5000, 914.5000, 914.5000],
        [151.0000, 151.0000, 151.0000],
        [712.0000, 712.0000, 712.0000],
        [578.0000, 578.0000, 578.0000],
        [202.0000, 202.0000, 202.0000],
        [143.5000, 143.5000, 143.5000]], device='cuda:0',
       grad_fn=<EmbeddingBackward0>)
DEBUG [test_emb.py:194] emb._trace []
DEBUG [test_emb.py:195] emb._hand_grad []
DEBUG [sharded_cache.py:235] 0: a2a cache_query_values
DEBUG [sharded_cache.py:235] 1: a2a cache_query_values
DEBUG [sharded_cache.py:238] 1: a2a cache_query_values done
DEBUG [sharded_cache.py:238] 0: a2a cache_query_values done
DEBUG [utils.py:209] rank0: before sum sparse tensors
INFO [utils.py:213] rank0: after sum sparse tensors
DEBUG [sharded_cache.py:286] 1 missing_keys = tensor([937, 866, 737, 316, 298, 712, 711, 525, 472], device='cuda:1')
DEBUG [sharded_cache.py:286] 0 missing_keys = tensor([333, 600, 512, 915, 151, 712, 578, 202, 144], device='cuda:0')
DEBUG [sharded_cache.py:287] 1 missing_grads = tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], device='cuda:1')
DEBUG [sharded_cache.py:287] 0 missing_grads = tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], device='cuda:0')
DEBUG [sharded_cache.py:292] reduced_missing_grads = tensor(indices=tensor([[144, 151, 202, 298, 316, 333, 472, 512, 525, 578, 600,
                        711, 712, 737, 866, 915, 937]]),
       values=tensor([[1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [2., 2., 2.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.]]),
       size=(1000, 3), nnz=17, layout=torch.sparse_coo)
DEBUG [DistEmb.py:116] record_grad tensor([144, 151, 202, 298, 316, 333, 472, 512, 525, 578, 600, 711, 712, 737,
        866, 915, 937]) tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [2., 2., 2.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]])
DEBUG [test_emb.py:200] 1:embed_value tensor([[937.0000, 937.0000, 937.0000],
        [865.5000, 865.5000, 865.5000],
        [737.0000, 737.0000, 737.0000],
        [ 84.5000,  84.5000,  84.5000],
        [316.0000, 316.0000, 316.0000],
        [298.0000, 298.0000, 298.0000],
        [712.0000, 712.0000, 712.0000],
        [710.5000, 710.5000, 710.5000],
        [525.0000, 525.0000, 525.0000],
        [471.5000, 471.5000, 471.5000]], device='cuda:1',
       grad_fn=<KnownShardedCachedEmbeddingFnBackward>)
DEBUG [test_emb.py:200] 0:embed_value tensor([[333.0000, 333.0000, 333.0000],
        [ 64.0000,  64.0000,  64.0000],
        [600.0000, 600.0000, 600.0000],
        [512.0000, 512.0000, 512.0000],
        [914.5000, 914.5000, 914.5000],
        [151.0000, 151.0000, 151.0000],
        [712.0000, 712.0000, 712.0000],
        [578.0000, 578.0000, 578.0000],
        [202.0000, 202.0000, 202.0000],
        [143.5000, 143.5000, 143.5000]], device='cuda:0',
       grad_fn=<KnownShardedCachedEmbeddingFnBackward>)
DEBUG [test_emb.py:176] emb._trace []
DEBUG [test_emb.py:177] emb._hand_grad []
DEBUG [test_emb.py:188] 0:step18, input_keys tensor([878, 568, 540, 115, 621, 403, 980, 718,  21,  41], device='cuda:0')
DEBUG [test_emb.py:176] emb._trace []
DEBUG [test_emb.py:177] emb._hand_grad []
DEBUG [test_emb.py:188] 1:step18, input_keys tensor([857, 851, 561, 641,  22,  69, 220, 740, 587, 710], device='cuda:1')
DEBUG [test_emb.py:192] 0:std_embed_value tensor([[876.5000, 876.5000, 876.5000],
        [567.5000, 567.5000, 567.5000],
        [540.0000, 540.0000, 540.0000],
        [115.0000, 115.0000, 115.0000],
        [621.0000, 621.0000, 621.0000],
        [402.0000, 402.0000, 402.0000],
        [980.0000, 980.0000, 980.0000],
        [718.0000, 718.0000, 718.0000],
        [ 20.5000,  20.5000,  20.5000],
        [ 41.0000,  41.0000,  41.0000]], device='cuda:0',
       grad_fn=<EmbeddingBackward0>)
DEBUG [test_emb.py:194] emb._trace []
DEBUG [test_emb.py:195] emb._hand_grad []
DEBUG [test_emb.py:192] 1:std_embed_value tensor([[857.0000, 857.0000, 857.0000],
        [851.0000, 851.0000, 851.0000],
        [561.0000, 561.0000, 561.0000],
        [641.0000, 641.0000, 641.0000],
        [ 22.0000,  22.0000,  22.0000],
        [ 69.0000,  69.0000,  69.0000],
        [220.0000, 220.0000, 220.0000],
        [740.0000, 740.0000, 740.0000],
        [587.0000, 587.0000, 587.0000],
        [709.5000, 709.5000, 709.5000]], device='cuda:1',
       grad_fn=<EmbeddingBackward0>)
DEBUG [test_emb.py:194] emb._trace []
DEBUG [test_emb.py:195] emb._hand_grad []
DEBUG [sharded_cache.py:235] 1: a2a cache_query_values
DEBUG [sharded_cache.py:235] 0: a2a cache_query_values
DEBUG [sharded_cache.py:238] 0: a2a cache_query_values done
DEBUG [sharded_cache.py:238] 1: a2a cache_query_values done
DEBUG [utils.py:209] rank0: before sum sparse tensors
INFO [utils.py:213] rank0: after sum sparse tensors
DEBUG [sharded_cache.py:286] 1 missing_keys = tensor([857, 851, 561, 641, 220, 740, 587, 710], device='cuda:1')
DEBUG [sharded_cache.py:286] 0 missing_keys = tensor([878, 568, 540, 115, 621, 403, 980, 718], device='cuda:0')
DEBUG [sharded_cache.py:287] 1 missing_grads = tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], device='cuda:1')
DEBUG [sharded_cache.py:287] 0 missing_grads = tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], device='cuda:0')
DEBUG [sharded_cache.py:292] reduced_missing_grads = tensor(indices=tensor([[115, 220, 403, 540, 561, 568, 587, 621, 641, 710, 718,
                        740, 851, 857, 878, 980]]),
       values=tensor([[1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.]]),
       size=(1000, 3), nnz=16, layout=torch.sparse_coo)
DEBUG [DistEmb.py:116] record_grad tensor([115, 220, 403, 540, 561, 568, 587, 621, 641, 710, 718, 740, 851, 857,
        878, 980]) tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]])
DEBUG [test_emb.py:200] 0:embed_value tensor([[876.5000, 876.5000, 876.5000],
        [567.5000, 567.5000, 567.5000],
        [540.0000, 540.0000, 540.0000],
        [115.0000, 115.0000, 115.0000],
        [621.0000, 621.0000, 621.0000],
        [402.0000, 402.0000, 402.0000],
        [980.0000, 980.0000, 980.0000],
        [718.0000, 718.0000, 718.0000],
        [ 20.5000,  20.5000,  20.5000],
        [ 41.0000,  41.0000,  41.0000]], device='cuda:0',
       grad_fn=<KnownShardedCachedEmbeddingFnBackward>)
DEBUG [test_emb.py:200] 1:embed_value tensor([[857.0000, 857.0000, 857.0000],
        [851.0000, 851.0000, 851.0000],
        [561.0000, 561.0000, 561.0000],
        [641.0000, 641.0000, 641.0000],
        [ 22.0000,  22.0000,  22.0000],
        [ 69.0000,  69.0000,  69.0000],
        [220.0000, 220.0000, 220.0000],
        [740.0000, 740.0000, 740.0000],
        [587.0000, 587.0000, 587.0000],
        [709.5000, 709.5000, 709.5000]], device='cuda:1',
       grad_fn=<KnownShardedCachedEmbeddingFnBackward>)
DEBUG [test_emb.py:176] emb._trace []
DEBUG [test_emb.py:177] emb._hand_grad []
DEBUG [test_emb.py:176] emb._trace []
DEBUG [test_emb.py:177] emb._hand_grad []
DEBUG [test_emb.py:188] 0:step19, input_keys tensor([181, 747, 432, 832, 992, 493, 533,  86, 537, 288], device='cuda:0')
DEBUG [test_emb.py:188] 1:step19, input_keys tensor([101, 552, 595,  63, 216, 123, 561, 786, 719, 909], device='cuda:1')
DEBUG [test_emb.py:192] 1:std_embed_value tensor([[100.5000, 100.5000, 100.5000],
        [552.0000, 552.0000, 552.0000],
        [595.0000, 595.0000, 595.0000],
        [ 63.0000,  63.0000,  63.0000],
        [216.0000, 216.0000, 216.0000],
        [123.0000, 123.0000, 123.0000],
        [560.5000, 560.5000, 560.5000],
        [786.0000, 786.0000, 786.0000],
        [719.0000, 719.0000, 719.0000],
        [908.5000, 908.5000, 908.5000]], device='cuda:1',
       grad_fn=<EmbeddingBackward0>)
DEBUG [test_emb.py:194] emb._trace []
DEBUG [test_emb.py:195] emb._hand_grad []
DEBUG [test_emb.py:192] 0:std_embed_value tensor([[181.0000, 181.0000, 181.0000],
        [747.0000, 747.0000, 747.0000],
        [431.5000, 431.5000, 431.5000],
        [831.5000, 831.5000, 831.5000],
        [991.5000, 991.5000, 991.5000],
        [492.5000, 492.5000, 492.5000],
        [533.0000, 533.0000, 533.0000],
        [ 86.0000,  86.0000,  86.0000],
        [536.5000, 536.5000, 536.5000],
        [287.5000, 287.5000, 287.5000]], device='cuda:0',
       grad_fn=<EmbeddingBackward0>)
DEBUG [test_emb.py:194] emb._trace []
DEBUG [test_emb.py:195] emb._hand_grad []
DEBUG [sharded_cache.py:235] 0: a2a cache_query_values
DEBUG [sharded_cache.py:235] 1: a2a cache_query_values
DEBUG [sharded_cache.py:238] 1: a2a cache_query_values done
DEBUG [sharded_cache.py:238] 0: a2a cache_query_values done
DEBUG [utils.py:209] rank0: before sum sparse tensors
INFO [utils.py:213] rank0: after sum sparse tensors
DEBUG [sharded_cache.py:286] 1 missing_keys = tensor([101, 552, 595, 216, 123, 561, 786, 719, 909], device='cuda:1')
DEBUG [sharded_cache.py:286] 0 missing_keys = tensor([181, 747, 432, 832, 992, 493, 533, 537, 288], device='cuda:0')
DEBUG [sharded_cache.py:287] 1 missing_grads = tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], device='cuda:1')
DEBUG [sharded_cache.py:287] 0 missing_grads = tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], device='cuda:0')
DEBUG [sharded_cache.py:292] reduced_missing_grads = tensor(indices=tensor([[101, 123, 181, 216, 288, 432, 493, 533, 537, 552, 561,
                        595, 719, 747, 786, 832, 909, 992]]),
       values=tensor([[1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.],
                      [1., 1., 1.]]),
       size=(1000, 3), nnz=18, layout=torch.sparse_coo)
DEBUG [DistEmb.py:116] record_grad tensor([101, 123, 181, 216, 288, 432, 493, 533, 537, 552, 561, 595, 719, 747,
        786, 832, 909, 992]) tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]])
DEBUG [test_emb.py:200] 1:embed_value tensor([[100.5000, 100.5000, 100.5000],
        [552.0000, 552.0000, 552.0000],
        [595.0000, 595.0000, 595.0000],
        [ 63.0000,  63.0000,  63.0000],
        [216.0000, 216.0000, 216.0000],
        [123.0000, 123.0000, 123.0000],
        [560.5000, 560.5000, 560.5000],
        [786.0000, 786.0000, 786.0000],
        [719.0000, 719.0000, 719.0000],
        [908.5000, 908.5000, 908.5000]], device='cuda:1',
       grad_fn=<KnownShardedCachedEmbeddingFnBackward>)
DEBUG [test_emb.py:200] 0:embed_value tensor([[181.0000, 181.0000, 181.0000],
        [747.0000, 747.0000, 747.0000],
        [431.5000, 431.5000, 431.5000],
        [831.5000, 831.5000, 831.5000],
        [991.5000, 991.5000, 991.5000],
        [492.5000, 492.5000, 492.5000],
        [533.0000, 533.0000, 533.0000],
        [ 86.0000,  86.0000,  86.0000],
        [536.5000, 536.5000, 536.5000],
        [287.5000, 287.5000, 287.5000]], device='cuda:0',
       grad_fn=<KnownShardedCachedEmbeddingFnBackward>)
100%|██████████| 20/20 [00:01<00:00, 19.89it/s]100%|██████████| 20/20 [00:01<00:00, 16.91it/s]========== Step 0 ========== 
========== Step 1 ========== 
========== Step 2 ========== 
========== Step 3 ========== 
========== Step 4 ========== 
========== Step 5 ========== 
========== Step 6 ========== 
========== Step 7 ========== 
========== Step 8 ========== 
========== Step 9 ========== 
========== Step 10 ========== 
========== Step 11 ========== 
========== Step 12 ========== 
========== Step 13 ========== 
========== Step 14 ========== 
========== Step 15 ========== 
========== Step 16 ========== 
========== Step 17 ========== 
========== Step 18 ========== 
========== Step 19 ========== 

100%|██████████| 20/20 [00:01<00:00, 19.93it/s]100%|██████████| 20/20 [00:01<00:00, 16.89it/s]========== Step 0 ========== 
========== Step 1 ========== 
========== Step 2 ========== 
========== Step 3 ========== 
========== Step 4 ========== 
========== Step 5 ========== 
========== Step 6 ========== 
========== Step 7 ========== 
========== Step 8 ========== 
========== Step 9 ========== 
========== Step 10 ========== 
========== Step 11 ========== 
========== Step 12 ========== 
========== Step 13 ========== 
========== Step 14 ========== 
========== Step 15 ========== 
========== Step 16 ========== 
========== Step 17 ========== 
========== Step 18 ========== 
========== Step 19 ========== 

join all processes done
