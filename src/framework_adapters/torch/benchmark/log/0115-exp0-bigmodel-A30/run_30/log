/home/xieminhui/RecStore/src/framework_adapters/torch/kg/dgl-0.9.1/python/dgl/_deprecate/graph.py:1023: DGLWarning: multigraph will be deprecated.DGL will treat all graphs as multigraph in the future.
  dgl_warning("multigraph will be deprecated." \
/home/xieminhui/RecStore/src/framework_adapters/torch/kg/dgl-0.9.1/python/dgl/heterograph.py:72: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.
  dgl_warning('Recommend creating graphs by `dgl.graph(data)`'
Reading train triples....
Finished. Read 483142 train triples.
Reading valid triples....
Finished. Read 50000 valid triples.
Reading test triples....
Finished. Read 59071 test triples.
ARGS:  Namespace(model_name='TransE_l1', data_path='/home/xieminhui/dgl-data', dataset='FB15k', format='built_in', data_files=None, delimiter='\t', save_path='/tmp/ckpts/TransE_l1_FB15k_215', no_save_emb=True, max_step=500, batch_size=3000, batch_size_eval=16, neg_sample_size=200, neg_deg_sample=False, neg_deg_sample_eval=False, neg_sample_size_eval=14951, eval_percent=1, no_eval_filter=False, log_interval=200, eval_interval=10000, test=False, num_proc=4, num_thread=1, force_sync_interval=1, hidden_dim=400, lr=0.01, gamma=16.0, double_ent=False, double_rel=False, neg_adversarial_sampling=False, adversarial_temperature=1.0, regularization_coef=1e-07, regularization_norm=3, pairwise=False, loss_genre='Logsigmoid', margin=1.0, nr_gpus=4, gpu=[0, 1, 2, 3], mix_cpu_gpu=True, valid=False, rel_part=False, async_update=None, has_edge_importance=False, cached_emb_type='None', use_my_emb=False, cache_ratio=0.2, shuffle=False, backwardMode='CppSync', L=10, kForwardItersPerStep=2, backgrad_init='both', eval_filter=True, soft_rel_part=False)
|Train|: 483142
original graph:  DGLGraph(num_nodes=14951, num_edges=592213,
         ndata_schemes={}
         edata_schemes={'tid': Scheme(shape=(), dtype=torch.int64)})
Loading cached metis
WARNING [2392294 sampler.py:454] Start PreSampling
WARNING [2392294 sampler.py:532] Before construct renumbering_dict
WARNING [2392294 sampler.py:555] PreSampling done
WARNING: Logging before InitGoogleLogging() is written to STDERR
W20240115 01:14:13.397677 2392294 IPCTensor.h:367] NewIPCTensor: cached_sampler_r0_0 [100000]0x100000002000
W20240115 01:14:13.397855 2392294 IPCTensor.h:367] NewIPCTensor: cached_sampler_r0_1 [100000]0x1000000c7000
W20240115 01:14:13.397891 2392294 IPCTensor.h:367] NewIPCTensor: cached_sampler_r0_2 [100000]0x10000018c000
W20240115 01:14:13.397922 2392294 IPCTensor.h:367] NewIPCTensor: cached_sampler_r0_3 [100000]0x100000251000
W20240115 01:14:13.397951 2392294 IPCTensor.h:367] NewIPCTensor: cached_sampler_r0_4 [100000]0x100000316000
W20240115 01:14:13.397981 2392294 IPCTensor.h:367] NewIPCTensor: cached_sampler_r0_5 [100000]0x1000003db000
W20240115 01:14:13.398010 2392294 IPCTensor.h:367] NewIPCTensor: cached_sampler_r0_6 [100000]0x1000004a0000
W20240115 01:14:13.398041 2392294 IPCTensor.h:367] NewIPCTensor: cached_sampler_r0_7 [100000]0x100000565000
W20240115 01:14:13.398070 2392294 IPCTensor.h:367] NewIPCTensor: cached_sampler_r0_8 [100000]0x10000062a000
W20240115 01:14:13.398099 2392294 IPCTensor.h:367] NewIPCTensor: cached_sampler_r0_9 [100000]0x1000006ef000
W20240115 01:14:13.398130 2392294 IPCTensor.h:367] NewIPCTensor: step_r0 [10]0x1000007b4000
W20240115 01:14:13.398162 2392294 IPCTensor.h:367] NewIPCTensor: circle_buffer_end_r0 [1]0x1000007b6000
W20240115 01:14:13.398187 2392294 IPCTensor.h:367] NewIPCTensor: circle_buffer_end_cppseen_r0 [1]0x1000007b8000
W20240115 01:14:13.398281 2392294 IPCTensor.h:367] NewIPCTensor: cached_sampler_r1_0 [100000]0x1000007ba000
W20240115 01:14:13.398320 2392294 IPCTensor.h:367] NewIPCTensor: cached_sampler_r1_1 [100000]0x10000087f000
W20240115 01:14:13.398350 2392294 IPCTensor.h:367] NewIPCTensor: cached_sampler_r1_2 [100000]0x100000944000
W20240115 01:14:13.398378 2392294 IPCTensor.h:367] NewIPCTensor: cached_sampler_r1_3 [100000]0x100000a09000
W20240115 01:14:13.398406 2392294 IPCTensor.h:367] NewIPCTensor: cached_sampler_r1_4 [100000]0x100000ace000
W20240115 01:14:13.398433 2392294 IPCTensor.h:367] NewIPCTensor: cached_sampler_r1_5 [100000]0x100000b93000
W20240115 01:14:13.398476 2392294 IPCTensor.h:367] NewIPCTensor: cached_sampler_r1_6 [100000]0x100000c58000
W20240115 01:14:13.398505 2392294 IPCTensor.h:367] NewIPCTensor: cached_sampler_r1_7 [100000]0x100000d1d000
W20240115 01:14:13.398531 2392294 IPCTensor.h:367] NewIPCTensor: cached_sampler_r1_8 [100000]0x100000de2000
W20240115 01:14:13.398558 2392294 IPCTensor.h:367] NewIPCTensor: cached_sampler_r1_9 [100000]0x100000ea7000
W20240115 01:14:13.398586 2392294 IPCTensor.h:367] NewIPCTensor: step_r1 [10]0x100000f6c000
W20240115 01:14:13.398612 2392294 IPCTensor.h:367] NewIPCTensor: circle_buffer_end_r1 [1]0x100000f6e000
W20240115 01:14:13.398636 2392294 IPCTensor.h:367] NewIPCTensor: circle_buffer_end_cppseen_r1 [1]0x100000f70000
W20240115 01:14:13.398685 2392294 IPCTensor.h:367] NewIPCTensor: cached_sampler_r2_0 [100000]0x100000f72000
W20240115 01:14:13.398717 2392294 IPCTensor.h:367] NewIPCTensor: cached_sampler_r2_1 [100000]0x100001037000
W20240115 01:14:13.398757 2392294 IPCTensor.h:367] NewIPCTensor: cached_sampler_r2_2 [100000]0x1000010fc000
W20240115 01:14:13.398785 2392294 IPCTensor.h:367] NewIPCTensor: cached_sampler_r2_3 [100000]0x1000011c1000
W20240115 01:14:13.398813 2392294 IPCTensor.h:367] NewIPCTensor: cached_sampler_r2_4 [100000]0x100001286000
W20240115 01:14:13.398841 2392294 IPCTensor.h:367] NewIPCTensor: cached_sampler_r2_5 [100000]0x10000134b000
W20240115 01:14:13.398869 2392294 IPCTensor.h:367] NewIPCTensor: cached_sampler_r2_6 [100000]0x100001410000
W20240115 01:14:13.398895 2392294 IPCTensor.h:367] NewIPCTensor: cached_sampler_r2_7 [100000]0x1000014d5000
W20240115 01:14:13.398922 2392294 IPCTensor.h:367] NewIPCTensor: cached_sampler_r2_8 [100000]0x10000159a000
W20240115 01:14:13.398950 2392294 IPCTensor.h:367] NewIPCTensor: cached_sampler_r2_9 [100000]0x10000165f000
W20240115 01:14:13.398978 2392294 IPCTensor.h:367] NewIPCTensor: step_r2 [10]0x100001724000
W20240115 01:14:13.399003 2392294 IPCTensor.h:367] NewIPCTensor: circle_buffer_end_r2 [1]0x100001726000
W20240115 01:14:13.399027 2392294 IPCTensor.h:367] NewIPCTensor: circle_buffer_end_cppseen_r2 [1]0x100001728000
W20240115 01:14:13.399075 2392294 IPCTensor.h:367] NewIPCTensor: cached_sampler_r3_0 [100000]0x10000172a000
W20240115 01:14:13.399108 2392294 IPCTensor.h:367] NewIPCTensor: cached_sampler_r3_1 [100000]0x1000017ef000
W20240115 01:14:13.399137 2392294 IPCTensor.h:367] NewIPCTensor: cached_sampler_r3_2 [100000]0x1000018b4000
W20240115 01:14:13.399164 2392294 IPCTensor.h:367] NewIPCTensor: cached_sampler_r3_3 [100000]0x100001979000
W20240115 01:14:13.399192 2392294 IPCTensor.h:367] NewIPCTensor: cached_sampler_r3_4 [100000]0x100001a3e000
W20240115 01:14:13.399219 2392294 IPCTensor.h:367] NewIPCTensor: cached_sampler_r3_5 [100000]0x100001b03000
W20240115 01:14:13.399246 2392294 IPCTensor.h:367] NewIPCTensor: cached_sampler_r3_6 [100000]0x100001bc8000
W20240115 01:14:13.399276 2392294 IPCTensor.h:367] NewIPCTensor: cached_sampler_r3_7 [100000]0x100001c8d000
W20240115 01:14:13.399303 2392294 IPCTensor.h:367] NewIPCTensor: cached_sampler_r3_8 [100000]0x100001d52000
W20240115 01:14:13.399330 2392294 IPCTensor.h:367] NewIPCTensor: cached_sampler_r3_9 [100000]0x100001e17000
W20240115 01:14:13.399358 2392294 IPCTensor.h:367] NewIPCTensor: step_r3 [10]0x100001edc000
W20240115 01:14:13.399382 2392294 IPCTensor.h:367] NewIPCTensor: circle_buffer_end_r3 [1]0x100001ede000
W20240115 01:14:13.399406 2392294 IPCTensor.h:367] NewIPCTensor: circle_buffer_end_cppseen_r3 [1]0x100001ee0000
{0: Graph(num_nodes=9480, num_edges=154759,
      ndata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64), 'inner_node': Scheme(shape=(), dtype=torch.int32), 'part_id': Scheme(shape=(), dtype=torch.int64)}
      edata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64), 'inner_edge': Scheme(shape=(), dtype=torch.int8)}), 1: Graph(num_nodes=11158, num_edges=169559,
      ndata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64), 'inner_node': Scheme(shape=(), dtype=torch.int32), 'part_id': Scheme(shape=(), dtype=torch.int64)}
      edata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64), 'inner_edge': Scheme(shape=(), dtype=torch.int8)}), 2: Graph(num_nodes=11355, num_edges=241153,
      ndata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64), 'inner_node': Scheme(shape=(), dtype=torch.int32), 'part_id': Scheme(shape=(), dtype=torch.int64)}
      edata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64), 'inner_edge': Scheme(shape=(), dtype=torch.int8)}), 3: Graph(num_nodes=10394, num_edges=202691,
      ndata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64), 'inner_node': Scheme(shape=(), dtype=torch.int32), 'part_id': Scheme(shape=(), dtype=torch.int64)}
      edata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64), 'inner_edge': Scheme(shape=(), dtype=torch.int8)})}
MertisPartition: part 0 has 9480 N 154759 E
MertisPartition: part 1 has 11158 N 169559 E
MertisPartition: part 2 has 11355 N 241153 E
MertisPartition: part 3 has 10394 N 202691 E
Rank0: cached key size 747
Rank1: cached key size 747
Rank2: cached key size 747
Rank3: cached key size 747
Before renumbering graph:  {'_ID': tensor([    6,    10,    11,  ...,  2559,  7188, 13282]), 'inner_node': tensor([1, 1, 1,  ..., 0, 0, 0], dtype=torch.int32), 'part_id': tensor([0, 0, 0,  ..., 2, 1, 3])}
After renumbering graph:  {'_ID': tensor([3167, 3268, 3295,  ..., 8657, 7763, 8175]), 'inner_node': tensor([1, 1, 1,  ..., 0, 0, 0], dtype=torch.int32), 'part_id': tensor([0, 0, 0,  ..., 2, 1, 3])}
part_g: DGLGraph(num_nodes=9480, num_edges=154759,
         ndata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64)}
         edata_schemes={'tid': Scheme(shape=(), dtype=torch.int64)})
part_g: DGLGraph(num_nodes=9480, num_edges=154759,
         ndata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64)}
         edata_schemes={'tid': Scheme(shape=(), dtype=torch.int64)})
Total initialize time 2.019 seconds
[Rank1] pid = 2392511
[Rank2] pid = 2392576
INFO [2392294 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 0
INFO [2392511 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 1
INFO [2392576 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 2
INFO [2392576 distributed_c10d.py:476] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
INFO [2392640 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 3
INFO [2392640 distributed_c10d.py:476] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
INFO [2392511 distributed_c10d.py:476] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
INFO [2392294 distributed_c10d.py:476] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
[proc 2][Train](200/500) average pos_loss: 0.5895419216156006
[proc 2][Train](200/500) average neg_loss: 0.415072672618553
[proc 2][Train](200/500) average loss: 0.5023072960972786
[proc 2][Train](200/500) average regularization: 1.903742522245011e-05
[proc 2] 200 steps, total: 16.479, sample: 1.270, forward: 3.802, backward: 3.788, update: 2.527
[Rank3] pid = 2392640
-------Step 0-------
tensor([  287, 14869, 14483, 11944,  6452,   432,   161,  3453,  4956, 12234]) tensor([ 3295,   562,  7786, 12984, 12250,   243, 10876,  5345, 12913,  9639])
-------Step 1-------
tensor([  287, 14869, 14483, 11944,  6452,   432,   161,  3453,  4956, 12234]) tensor([10855,  1669, 14603,  3544,  5537,  4060,  1257,  8106,  1685, 14444])
-------Step 2-------
tensor([ 4247, 10118,   257,   381, 10276,  3154,    37, 14829,  7271,  6170]) tensor([ 5011,  8680,  3783, 14434,  9218,  5590,  4169, 11256, 13992, 10955])
-------Step 3-------
tensor([ 4247, 10118,   257,   381, 10276,  3154,    37, 14829,  7271,  6170]) tensor([8544, 2396, 1717, 2253, 7520, 1895, 3051, 3422, 2373, 9262])
-------Step 4-------
tensor([ 1899, 14196, 14869,   680, 14723, 12152,  2145,   288,  1778,  8253]) tensor([11658,  9043,  4086, 13248,  3521,   558,  8176,  4304,  1891,  2141])
-------Step 5-------
tensor([ 1899, 14196, 14869,   680, 14723, 12152,  2145,   288,  1778,  8253]) tensor([ 7732,  2679, 11637,  4051,  4305,  4956,  2391,  9846, 10401,  2713])
-------Step 6-------
tensor([  379,  3563, 13029, 14928,   680,   483,  7992,  6013,   365, 13260]) tensor([ 1585,  5848,  7859,   256, 13732, 12686,  5637,  3269,  1272,  6352])
-------Step 7-------
tensor([  379,  3563, 13029, 14928,   680,   483,  7992,  6013,   365, 13260]) tensor([ 8436, 12529,  6421,  7714,  1683,  7364,   211, 10602, 10299,  2878])
-------Step 8-------
tensor([ 5515,    58, 12499, 13418,  7357,   579,  5860,  9721, 12120,   690]) tensor([ 5211,  3166,  1033, 10101,   245, 12461,  7931,    47, 14754,  1002])
-------Step 9-------
tensor([ 5515,    58, 12499, 13418,  7357,   579,  5860,  9721, 12120,   690]) tensor([ 4593,  8005, 14341,  3902,   692,  3242,   311,  3005,  1789,  4072])
[proc 0][Train](200/500) average pos_loss: 0.4876351007819176
[proc 0][Train](200/500) average neg_loss: 0.3391063398960978
[proc 0][Train](200/500) average loss: 0.41337072119116786
[proc 0][Train](200/500) average regularization: 1.8840715438273036e-05
[proc 0] 200 steps, total: 16.423, sample: 0.705, forward: 5.992, backward: 7.612, update: 1.698
[proc 1][Train](200/500) average pos_loss: 0.5260689273476601
[proc 1][Train](200/500) average neg_loss: 0.3874694542447105
[proc 1][Train](200/500) average loss: 0.45676919057965276
[proc 1][Train](200/500) average regularization: 1.866169838649512e-05
[proc 1] 200 steps, total: 16.456, sample: 1.249, forward: 3.817, backward: 3.720, update: 2.920
[proc 3][Train](200/500) average pos_loss: 0.5530706316232681
[proc 3][Train](200/500) average neg_loss: 0.3939139989344403
[proc 3][Train](200/500) average loss: 0.473492314517498
[proc 3][Train](200/500) average regularization: 1.9025722426704306e-05
[proc 3] 200 steps, total: 16.421, sample: 1.252, forward: 3.811, backward: 3.945, update: 2.452
[proc 0][Train](400/500) average pos_loss: 0.14611375596374274
[proc 0][Train](400/500) average neg_loss: 0.1727096876502037
[proc 0][Train](400/500) average loss: 0.15941172264516354
[proc 0][Train](400/500) average regularization: 2.0856439705312367e-05
[proc 0] 400 steps, total: 14.366, sample: 0.719, forward: 5.474, backward: 5.802, update: 1.778
[proc 1][Train](400/500) average pos_loss: 0.18191992208361626
[proc 1][Train](400/500) average neg_loss: 0.20724303126335145
[proc 1][Train](400/500) average loss: 0.19458147652447225
[proc 1][Train](400/500) average regularization: 2.047097813374421e-05
[proc 1] 400 steps, total: 14.366, sample: 1.242, forward: 3.044, backward: 2.045, update: 2.862
[proc 3][Train](400/500) average pos_loss: 0.21016585037112237
[proc 3][Train](400/500) average neg_loss: 0.22051819510757922
[proc 3][Train](400/500) average loss: 0.21534202314913273
[proc 3][Train](400/500) average regularization: 2.0358893443699344e-05
[proc 3] 400 steps, total: 14.367, sample: 1.308, forward: 3.229, backward: 2.214, update: 2.442
[proc 2][Train](400/500) average pos_loss: 0.2460726610571146
[proc 2][Train](400/500) average neg_loss: 0.24438142344355585
[proc 2][Train](400/500) average loss: 0.24522704228758813
[proc 2][Train](400/500) average regularization: 2.0307730765125597e-05
[proc 2] 400 steps, total: 14.367, sample: 1.238, forward: 3.224, backward: 2.223, update: 2.635
Successfully xmh. training takes 38.07665491104126 seconds
