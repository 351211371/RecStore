WARNING: Logging before InitGoogleLogging() is written to STDERR
I20240221 13:23:56.745908 1020235 IPC_barrier.h:128] MultiProcessBarrierFactory->ClearIPCMemory()
/home/xieminhui/RecStore/src/framework_adapters/torch/kg/dgl-0.9.1/python/dgl/_deprecate/graph.py:1023: DGLWarning: multigraph will be deprecated.DGL will treat all graphs as multigraph in the future.
  dgl_warning("multigraph will be deprecated." \
/home/xieminhui/RecStore/src/framework_adapters/torch/kg/dgl-0.9.1/python/dgl/heterograph.py:72: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.
  dgl_warning('Recommend creating graphs by `dgl.graph(data)`'
Reading train triples....
Finished. Read 483142 train triples.
Reading valid triples....
Finished. Read 50000 valid triples.
Reading test triples....
Finished. Read 59071 test triples.
ARGS:  Namespace(model_name='TransE', data_path='/home/xieminhui/dgl-data', dataset='FB15k', format='built_in', data_files=None, delimiter='\t', save_path='/tmp/ckpts/TransE_FB15k_62', no_save_emb=True, max_step=500, batch_size=1200, batch_size_eval=16, neg_sample_size=200, neg_deg_sample=False, neg_deg_sample_eval=False, neg_sample_size_eval=14951, eval_percent=1, no_eval_filter=False, log_interval=100, eval_interval=10000, test=False, num_proc=2, num_thread=1, force_sync_interval=1, hidden_dim=400, lr=0.01, gamma=16.0, double_ent=False, double_rel=False, neg_adversarial_sampling=False, adversarial_temperature=1.0, regularization_coef=0.0, regularization_norm=3, pairwise=False, loss_genre='Logsigmoid', margin=1.0, nr_gpus=2, gpu=[0, 1], mix_cpu_gpu=True, valid=False, rel_part=False, async_update=None, has_edge_importance=False, cached_emb_type='None', use_my_emb=False, cache_ratio=0.05, shuffle=False, backwardMode='CppSync', L=10, nr_background_threads=32, update_cache_use_omp=1, update_pq_use_omp=2, kForwardItersPerStep=2, backgrad_init='both', eval_filter=True, soft_rel_part=False)
|Train|: 483142
original graph:  DGLGraph(num_nodes=14951, num_edges=592213,
         ndata_schemes={}
         edata_schemes={'tid': Scheme(shape=(), dtype=torch.int64)})
Loading cached metis
WARNING [1020235 sampler.py:454] Start PreSampling
WARNING [1020235 sampler.py:532] Before construct renumbering_dict
WARNING [1020235 sampler.py:555] PreSampling done
W20240221 13:23:58.845084 1020235 IPCTensor.h:369] NewIPCTensor: cached_sampler_r0_0 [1000000]0x100000002000 7.629 MB
W20240221 13:23:58.845203 1020235 IPCTensor.h:369] NewIPCTensor: cached_sampler_r0_1 [1000000]0x1000007a5000 7.629 MB
W20240221 13:23:58.845225 1020235 IPCTensor.h:369] NewIPCTensor: cached_sampler_r0_2 [1000000]0x100000f48000 7.629 MB
W20240221 13:23:58.845243 1020235 IPCTensor.h:369] NewIPCTensor: cached_sampler_r0_3 [1000000]0x1000016eb000 7.629 MB
W20240221 13:23:58.845265 1020235 IPCTensor.h:369] NewIPCTensor: cached_sampler_r0_4 [1000000]0x100001e8e000 7.629 MB
W20240221 13:23:58.845280 1020235 IPCTensor.h:369] NewIPCTensor: cached_sampler_r0_5 [1000000]0x100002631000 7.629 MB
W20240221 13:23:58.845297 1020235 IPCTensor.h:369] NewIPCTensor: cached_sampler_r0_6 [1000000]0x100002dd4000 7.629 MB
W20240221 13:23:58.845316 1020235 IPCTensor.h:369] NewIPCTensor: cached_sampler_r0_7 [1000000]0x100003577000 7.629 MB
W20240221 13:23:58.845332 1020235 IPCTensor.h:369] NewIPCTensor: cached_sampler_r0_8 [1000000]0x100003d1a000 7.629 MB
W20240221 13:23:58.845351 1020235 IPCTensor.h:369] NewIPCTensor: cached_sampler_r0_9 [1000000]0x1000044bd000 7.629 MB
W20240221 13:23:58.845371 1020235 IPCTensor.h:369] NewIPCTensor: step_r0 [10]0x100004c60000 80 B 
W20240221 13:23:58.845387 1020235 IPCTensor.h:369] NewIPCTensor: circle_buffer_end_r0 [1]0x100004c62000 8 B 
W20240221 13:23:58.845403 1020235 IPCTensor.h:369] NewIPCTensor: circle_buffer_end_cppseen_r0 [1]0x100004c64000 8 B 
W20240221 13:23:58.845484 1020235 IPCTensor.h:369] NewIPCTensor: cached_sampler_r1_0 [1000000]0x100004c66000 7.629 MB
W20240221 13:23:58.845503 1020235 IPCTensor.h:369] NewIPCTensor: cached_sampler_r1_1 [1000000]0x100005409000 7.629 MB
W20240221 13:23:58.845522 1020235 IPCTensor.h:369] NewIPCTensor: cached_sampler_r1_2 [1000000]0x100005bac000 7.629 MB
W20240221 13:23:58.845551 1020235 IPCTensor.h:369] NewIPCTensor: cached_sampler_r1_3 [1000000]0x10000634f000 7.629 MB
W20240221 13:23:58.845568 1020235 IPCTensor.h:369] NewIPCTensor: cached_sampler_r1_4 [1000000]0x100006af2000 7.629 MB
W20240221 13:23:58.845590 1020235 IPCTensor.h:369] NewIPCTensor: cached_sampler_r1_5 [1000000]0x100007295000 7.629 MB
W20240221 13:23:58.845604 1020235 IPCTensor.h:369] NewIPCTensor: cached_sampler_r1_6 [1000000]0x100007a38000 7.629 MB
W20240221 13:23:58.845620 1020235 IPCTensor.h:369] NewIPCTensor: cached_sampler_r1_7 [1000000]0x1000081db000 7.629 MB
W20240221 13:23:58.845634 1020235 IPCTensor.h:369] NewIPCTensor: cached_sampler_r1_8 [1000000]0x10000897e000 7.629 MB
W20240221 13:23:58.845652 1020235 IPCTensor.h:369] NewIPCTensor: cached_sampler_r1_9 [1000000]0x100009121000 7.629 MB
W20240221 13:23:58.845671 1020235 IPCTensor.h:369] NewIPCTensor: step_r1 [10]0x1000098c4000 80 B 
W20240221 13:23:58.845685 1020235 IPCTensor.h:369] NewIPCTensor: circle_buffer_end_r1 [1]0x1000098c6000 8 B 
W20240221 13:23:58.845700 1020235 IPCTensor.h:369] NewIPCTensor: circle_buffer_end_cppseen_r1 [1]0x1000098c8000 8 B 
{0: Graph(num_nodes=13804, num_edges=289155,
      ndata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64), 'inner_node': Scheme(shape=(), dtype=torch.int32), 'part_id': Scheme(shape=(), dtype=torch.int64)}
      edata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64), 'inner_edge': Scheme(shape=(), dtype=torch.int8)}), 1: Graph(num_nodes=12626, num_edges=379866,
      ndata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64), 'inner_node': Scheme(shape=(), dtype=torch.int32), 'part_id': Scheme(shape=(), dtype=torch.int64)}
      edata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64), 'inner_edge': Scheme(shape=(), dtype=torch.int8)})}
MertisPartition: part 0 has 13804 N 289155 E
MertisPartition: part 1 has 12626 N 379866 E
Rank0: cached key size 373
Rank1: cached key size 373
Before renumbering graph:  {'_ID': tensor([    0,     2,     6,  ..., 11079,  2237, 14365]), 'inner_node': tensor([1, 1, 1,  ..., 0, 0, 0], dtype=torch.int32), 'part_id': tensor([0, 0, 0,  ..., 1, 1, 1])}
After renumbering graph:  {'_ID': tensor([  764,   817,   933,  ..., 10010, 12622, 12701]), 'inner_node': tensor([1, 1, 1,  ..., 0, 0, 0], dtype=torch.int32), 'part_id': tensor([0, 0, 0,  ..., 1, 1, 1])}
part_g: DGLGraph(num_nodes=13804, num_edges=289155,
         ndata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64)}
         edata_schemes={'tid': Scheme(shape=(), dtype=torch.int64)})
part_g: DGLGraph(num_nodes=13804, num_edges=289155,
         ndata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64)}
         edata_schemes={'tid': Scheme(shape=(), dtype=torch.int64)})
Total initialize time 2.262 seconds
INFO [1020462 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 1
INFO [1020462 distributed_c10d.py:476] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.
INFO [1020235 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 0
INFO [1020235 distributed_c10d.py:476] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.
E20240221 13:24:08.887745 1020498 recstore.cc:67] init folly done
E20240221 13:24:08.888224 1020463 recstore.cc:67] init folly done
I20240221 13:24:08.922397 1020498 IPC_barrier.h:118] CreateBarrier: new barrier, name: start_barrier, count: 2
I20240221 13:24:08.959997 1020463 IPC_barrier.h:112] CreateBarrier: find existing barrier, name: start_barrier, count: 2
[Rank1] pid = 1020462
train_sampler.Prefill()
-------Step 0-------
tensor([  223,  5762, 10336,  7120,  2775, 13513,  4022,  8314,  7570,  8068]) tensor([  979,  9936, 12434, 11955, 14908,  7020,  6662,   394,  6648,  8196])
-------Step 1-------
tensor([  223,  5762, 10336,  7120,  2775, 13513,  4022,  8314,  7570,  8068]) tensor([14382,  1671,  2018,  4563,  8809, 11487, 10573,  9577, 12569,  4261])
-------Step 2-------
tensor([13513,     4,    95,   243,  6242,  6826,  1545, 10618,  8211, 12323]) tensor([ 2359,  2120, 13122, 12148,  4879,  8133, 13566,  2118,  5750,  5554])
-------Step 3-------
tensor([13513,     4,    95,   243,  6242,  6826,  1545, 10618,  8211, 12323]) tensor([14703,  2305,   269,  5664,  5911,  9663,  3376,   822, 13690, 13541])
-------Step 4-------
tensor([ 2589,  1974,   151,  1696,  9194,  6615, 11860,  8577,    26,  6446]) tensor([ 6038, 13180,  9589,  8347, 13098,  6501,  7071,  9620,  4686,  7940])
-------Step 5-------
tensor([ 2589,  1974,   151,  1696,  9194,  6615, 11860,  8577,    26,  6446]) tensor([ 7732,  2916, 12070,  5088,  3535,  9873,    59,   795,  6502,  7934])
-------Step 6-------
tensor([ 6314,  3430,  6857,   233,  1224,   161, 13513,  7433,   221,   327]) tensor([13961, 14227,  8237,  8898,   878,  6165,  3962, 14039, 13277,  9455])
-------Step 7-------
tensor([ 6314,  3430,  6857,   233,  1224,   161, 13513,  7433,   221,   327]) tensor([ 7345,  1765, 13307, 13875, 13712,  8266,  1990,   580,    15,  7124])
-------Step 8-------
tensor([ 4802, 10414,  6435,  2009,  5762,  4964,  6242,  2589, 12406, 13124]) tensor([13673,  6417, 13113,  5544,  6543,  9453,  2476,  7861, 11558, 11925])
-------Step 9-------
tensor([ 4802, 10414,  6435,  2009,  5762,  4964,  6242,  2589, 12406, 13124]) tensor([ 4155, 10585, 13230, 14482, 12056,  4267,  2702,  9577, 11557,  3238])
before start barrier
start train
[proc 0] 100 steps, total: 3.235, sample: 0.205, forward: 1.536, backward: 1.009, update: 0.384
train_sampler.Prefill()
before start barrier
start train
[proc 1] 100 steps, total: 3.272, sample: 0.207, forward: 1.245, backward: 0.672, update: 0.392
[proc 0] 200 steps, total: 1.658, sample: 0.210, forward: 0.502, backward: 0.440, update: 0.390
[proc 1] 200 steps, total: 1.658, sample: 0.215, forward: 0.471, backward: 0.442, update: 0.383
+---------------------------+---------------------------+---------------------------+
| Name                      | P50                       | P99                       |
+---------------------------+---------------------------+---------------------------+
| GenInput                  | 828.260 us                | 10.430 ms                 |
| Forward                   | 4.730 ms                  | 12.581 ms                 |
| Backward                  | 4.438 ms                  | 4.866 ms                  |
| Optimize                  | 3.647 ms                  | 6.740 ms                  |
| OneStep                   | 14.649 ms                 | 38.217 ms                 |
+---------------------------+---------------------------+---------------------------+
[proc 0] 300 steps, total: 1.866, sample: 0.198, forward: 0.608, backward: 0.458, update: 0.474
[proc 1] 300 steps, total: 1.866, sample: 0.202, forward: 0.559, backward: 0.463, update: 0.458
[proc 0] 400 steps, total: 1.466, sample: 0.174, forward: 0.426, backward: 0.432, update: 0.334
[proc 1] 400 steps, total: 1.466, sample: 0.189, forward: 0.417, backward: 0.434, update: 0.345
[proc 0] 500 steps, total: 1.396, sample: 0.182, forward: 0.407, backward: 0.419, update: 0.310
[proc 1] 500 steps, total: 1.396, sample: 0.184, forward: 0.399, backward: 0.368, update: 0.322
Successfully xmh. training takes 9.621007919311523 seconds
before call kg_cache_controller.StopThreads()
+---------------------------+---------------------------+---------------------------+
| Name                      | P50                       | P99                       |
+---------------------------+---------------------------+---------------------------+
| GenInput                  | 795.751 us                | 10.911 ms                 |
| Forward                   | 4.500 ms                  | 9.058 ms                  |
| Backward                  | 4.453 ms                  | 4.857 ms                  |
| Optimize                  | 3.515 ms                  | 6.288 ms                  |
| OneStep                   | 14.375 ms                 | 28.271 ms                 |
+---------------------------+---------------------------+---------------------------+
KGCacheControllerWrapperDummy.StopThreads
