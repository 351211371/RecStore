WARNING: Logging before InitGoogleLogging() is written to STDERR
I20240118 18:41:43.151901 869638 IPC_barrier.h:128] MultiProcessBarrierFactory->ClearIPCMemory()
/home/xieminhui/RecStore/src/framework_adapters/torch/kg/dgl-0.9.1/python/dgl/_deprecate/graph.py:1023: DGLWarning: multigraph will be deprecated.DGL will treat all graphs as multigraph in the future.
  dgl_warning("multigraph will be deprecated." \
/home/xieminhui/RecStore/src/framework_adapters/torch/kg/dgl-0.9.1/python/dgl/heterograph.py:72: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.
  dgl_warning('Recommend creating graphs by `dgl.graph(data)`'
Reading train triples....
Finished. Read 483142 train triples.
Reading valid triples....
Finished. Read 50000 valid triples.
Reading test triples....
Finished. Read 59071 test triples.
ARGS:  Namespace(model_name='TransR', data_path='/home/xieminhui/dgl-data', dataset='FB15k', format='built_in', data_files=None, delimiter='\t', save_path='/tmp/ckpts/TransR_FB15k_2', no_save_emb=True, max_step=500, batch_size=4800, batch_size_eval=16, neg_sample_size=200, neg_deg_sample=False, neg_deg_sample_eval=False, neg_sample_size_eval=14951, eval_percent=1, no_eval_filter=False, log_interval=100, eval_interval=10000, test=False, num_proc=4, num_thread=1, force_sync_interval=1, hidden_dim=400, lr=0.01, gamma=16.0, double_ent=False, double_rel=False, neg_adversarial_sampling=False, adversarial_temperature=1.0, regularization_coef=0.0, regularization_norm=3, pairwise=False, loss_genre='Logsigmoid', margin=1.0, nr_gpus=4, gpu=[0, 1, 2, 3], mix_cpu_gpu=True, valid=False, rel_part=False, async_update=None, has_edge_importance=False, cached_emb_type='KnownLocalCachedEmbedding', use_my_emb=True, cache_ratio=0.1, shuffle=False, backwardMode='CppAsyncV2', L=10, kForwardItersPerStep=2, backgrad_init='both', eval_filter=True, soft_rel_part=False)
|Train|: 483142
original graph:  DGLGraph(num_nodes=14951, num_edges=592213,
         ndata_schemes={}
         edata_schemes={'tid': Scheme(shape=(), dtype=torch.int64)})
Loading cached metis
WARNING [869638 sampler.py:454] Start PreSampling
WARNING [869638 sampler.py:532] Before construct renumbering_dict
WARNING [869638 sampler.py:555] PreSampling done
W20240118 18:41:44.827998 869638 IPCTensor.h:367] NewIPCTensor: cached_sampler_r0_0 [100000]0x100000002000
W20240118 18:41:44.828095 869638 IPCTensor.h:367] NewIPCTensor: cached_sampler_r0_1 [100000]0x1000000c7000
W20240118 18:41:44.828115 869638 IPCTensor.h:367] NewIPCTensor: cached_sampler_r0_2 [100000]0x10000018c000
W20240118 18:41:44.828131 869638 IPCTensor.h:367] NewIPCTensor: cached_sampler_r0_3 [100000]0x100000251000
W20240118 18:41:44.828145 869638 IPCTensor.h:367] NewIPCTensor: cached_sampler_r0_4 [100000]0x100000316000
W20240118 18:41:44.828159 869638 IPCTensor.h:367] NewIPCTensor: cached_sampler_r0_5 [100000]0x1000003db000
W20240118 18:41:44.828173 869638 IPCTensor.h:367] NewIPCTensor: cached_sampler_r0_6 [100000]0x1000004a0000
W20240118 18:41:44.828192 869638 IPCTensor.h:367] NewIPCTensor: cached_sampler_r0_7 [100000]0x100000565000
W20240118 18:41:44.828207 869638 IPCTensor.h:367] NewIPCTensor: cached_sampler_r0_8 [100000]0x10000062a000
W20240118 18:41:44.828222 869638 IPCTensor.h:367] NewIPCTensor: cached_sampler_r0_9 [100000]0x1000006ef000
W20240118 18:41:44.828238 869638 IPCTensor.h:367] NewIPCTensor: step_r0 [10]0x1000007b4000
W20240118 18:41:44.828253 869638 IPCTensor.h:367] NewIPCTensor: circle_buffer_end_r0 [1]0x1000007b6000
W20240118 18:41:44.828265 869638 IPCTensor.h:367] NewIPCTensor: circle_buffer_end_cppseen_r0 [1]0x1000007b8000
W20240118 18:41:44.828342 869638 IPCTensor.h:367] NewIPCTensor: cached_sampler_r1_0 [100000]0x1000007ba000
W20240118 18:41:44.828359 869638 IPCTensor.h:367] NewIPCTensor: cached_sampler_r1_1 [100000]0x10000087f000
W20240118 18:41:44.828373 869638 IPCTensor.h:367] NewIPCTensor: cached_sampler_r1_2 [100000]0x100000944000
W20240118 18:41:44.828387 869638 IPCTensor.h:367] NewIPCTensor: cached_sampler_r1_3 [100000]0x100000a09000
W20240118 18:41:44.828399 869638 IPCTensor.h:367] NewIPCTensor: cached_sampler_r1_4 [100000]0x100000ace000
W20240118 18:41:44.828429 869638 IPCTensor.h:367] NewIPCTensor: cached_sampler_r1_5 [100000]0x100000b93000
W20240118 18:41:44.828444 869638 IPCTensor.h:367] NewIPCTensor: cached_sampler_r1_6 [100000]0x100000c58000
W20240118 18:41:44.828456 869638 IPCTensor.h:367] NewIPCTensor: cached_sampler_r1_7 [100000]0x100000d1d000
W20240118 18:41:44.828469 869638 IPCTensor.h:367] NewIPCTensor: cached_sampler_r1_8 [100000]0x100000de2000
W20240118 18:41:44.828482 869638 IPCTensor.h:367] NewIPCTensor: cached_sampler_r1_9 [100000]0x100000ea7000
W20240118 18:41:44.828495 869638 IPCTensor.h:367] NewIPCTensor: step_r1 [10]0x100000f6c000
W20240118 18:41:44.828507 869638 IPCTensor.h:367] NewIPCTensor: circle_buffer_end_r1 [1]0x100000f6e000
W20240118 18:41:44.828519 869638 IPCTensor.h:367] NewIPCTensor: circle_buffer_end_cppseen_r1 [1]0x100000f70000
W20240118 18:41:44.828548 869638 IPCTensor.h:367] NewIPCTensor: cached_sampler_r2_0 [100000]0x100000f72000
W20240118 18:41:44.828564 869638 IPCTensor.h:367] NewIPCTensor: cached_sampler_r2_1 [100000]0x100001037000
W20240118 18:41:44.828577 869638 IPCTensor.h:367] NewIPCTensor: cached_sampler_r2_2 [100000]0x1000010fc000
W20240118 18:41:44.828591 869638 IPCTensor.h:367] NewIPCTensor: cached_sampler_r2_3 [100000]0x1000011c1000
W20240118 18:41:44.828604 869638 IPCTensor.h:367] NewIPCTensor: cached_sampler_r2_4 [100000]0x100001286000
W20240118 18:41:44.828617 869638 IPCTensor.h:367] NewIPCTensor: cached_sampler_r2_5 [100000]0x10000134b000
W20240118 18:41:44.828630 869638 IPCTensor.h:367] NewIPCTensor: cached_sampler_r2_6 [100000]0x100001410000
W20240118 18:41:44.828646 869638 IPCTensor.h:367] NewIPCTensor: cached_sampler_r2_7 [100000]0x1000014d5000
W20240118 18:41:44.828660 869638 IPCTensor.h:367] NewIPCTensor: cached_sampler_r2_8 [100000]0x10000159a000
W20240118 18:41:44.828673 869638 IPCTensor.h:367] NewIPCTensor: cached_sampler_r2_9 [100000]0x10000165f000
W20240118 18:41:44.828689 869638 IPCTensor.h:367] NewIPCTensor: step_r2 [10]0x100001724000
W20240118 18:41:44.828701 869638 IPCTensor.h:367] NewIPCTensor: circle_buffer_end_r2 [1]0x100001726000
W20240118 18:41:44.828711 869638 IPCTensor.h:367] NewIPCTensor: circle_buffer_end_cppseen_r2 [1]0x100001728000
W20240118 18:41:44.828737 869638 IPCTensor.h:367] NewIPCTensor: cached_sampler_r3_0 [100000]0x10000172a000
W20240118 18:41:44.828753 869638 IPCTensor.h:367] NewIPCTensor: cached_sampler_r3_1 [100000]0x1000017ef000
W20240118 18:41:44.828766 869638 IPCTensor.h:367] NewIPCTensor: cached_sampler_r3_2 [100000]0x1000018b4000
W20240118 18:41:44.828781 869638 IPCTensor.h:367] NewIPCTensor: cached_sampler_r3_3 [100000]0x100001979000
W20240118 18:41:44.828794 869638 IPCTensor.h:367] NewIPCTensor: cached_sampler_r3_4 [100000]0x100001a3e000
W20240118 18:41:44.828807 869638 IPCTensor.h:367] NewIPCTensor: cached_sampler_r3_5 [100000]0x100001b03000
W20240118 18:41:44.828820 869638 IPCTensor.h:367] NewIPCTensor: cached_sampler_r3_6 [100000]0x100001bc8000
W20240118 18:41:44.828835 869638 IPCTensor.h:367] NewIPCTensor: cached_sampler_r3_7 [100000]0x100001c8d000
W20240118 18:41:44.828847 869638 IPCTensor.h:367] NewIPCTensor: cached_sampler_r3_8 [100000]0x100001d52000
W20240118 18:41:44.828860 869638 IPCTensor.h:367] NewIPCTensor: cached_sampler_r3_9 [100000]0x100001e17000
W20240118 18:41:44.828873 869638 IPCTensor.h:367] NewIPCTensor: step_r3 [10]0x100001edc000
W20240118 18:41:44.828886 869638 IPCTensor.h:367] NewIPCTensor: circle_buffer_end_r3 [1]0x100001ede000
W20240118 18:41:44.828895 869638 IPCTensor.h:367] NewIPCTensor: circle_buffer_end_cppseen_r3 [1]0x100001ee0000
W20240118 18:41:44.933295 869638 IPCTensor.h:367] NewIPCTensor: full_emb [14951, 400]0x100001ee2000
W20240118 18:41:44.933400 869638 IPCTensor.h:367] NewIPCTensor: full_emb_SparseRowWiseAdaGrad_sum [14951]0x1000035b4000
{0: Graph(num_nodes=9480, num_edges=154759,
      ndata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64), 'inner_node': Scheme(shape=(), dtype=torch.int32), 'part_id': Scheme(shape=(), dtype=torch.int64)}
      edata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64), 'inner_edge': Scheme(shape=(), dtype=torch.int8)}), 1: Graph(num_nodes=11158, num_edges=169559,
      ndata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64), 'inner_node': Scheme(shape=(), dtype=torch.int32), 'part_id': Scheme(shape=(), dtype=torch.int64)}
      edata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64), 'inner_edge': Scheme(shape=(), dtype=torch.int8)}), 2: Graph(num_nodes=11355, num_edges=241153,
      ndata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64), 'inner_node': Scheme(shape=(), dtype=torch.int32), 'part_id': Scheme(shape=(), dtype=torch.int64)}
      edata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64), 'inner_edge': Scheme(shape=(), dtype=torch.int8)}), 3: Graph(num_nodes=10394, num_edges=202691,
      ndata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64), 'inner_node': Scheme(shape=(), dtype=torch.int32), 'part_id': Scheme(shape=(), dtype=torch.int64)}
      edata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64), 'inner_edge': Scheme(shape=(), dtype=torch.int8)})}
MertisPartition: part 0 has 9480 N 154759 E
MertisPartition: part 1 has 11158 N 169559 E
MertisPartition: part 2 has 11355 N 241153 E
MertisPartition: part 3 has 10394 N 202691 E
Rank0: cached key size 373
Rank1: cached key size 373
Rank2: cached key size 373
Rank3: cached key size 373
Before renumbering graph:  {'_ID': tensor([    6,    10,    11,  ...,  2559,  7188, 13282]), 'inner_node': tensor([1, 1, 1,  ..., 0, 0, 0], dtype=torch.int32), 'part_id': tensor([0, 0, 0,  ..., 2, 1, 3])}
After renumbering graph:  {'_ID': tensor([1669, 1785, 1823,  ..., 7817, 6549, 7540]), 'inner_node': tensor([1, 1, 1,  ..., 0, 0, 0], dtype=torch.int32), 'part_id': tensor([0, 0, 0,  ..., 2, 1, 3])}
part_g: DGLGraph(num_nodes=9480, num_edges=154759,
         ndata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64)}
         edata_schemes={'tid': Scheme(shape=(), dtype=torch.int64)})
part_g: DGLGraph(num_nodes=9480, num_edges=154759,
         ndata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64)}
         edata_schemes={'tid': Scheme(shape=(), dtype=torch.int64)})
Total initialize time 3.208 seconds
[Rank1] pid = 869990
[Rank2] pid = 870055
INFO [869638 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 0
INFO [869638 distributed_c10d.py:476] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
INFO [869990 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 1
INFO [870055 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 2
INFO [870055 distributed_c10d.py:476] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
INFO [869990 distributed_c10d.py:476] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
W20240118 18:41:48.852926 870120 IPCTensor.h:406] NewIPCGPUTensor: embedding_cache_0 [1495, 400]; dev=0; size=2.281 MB
W20240118 18:41:48.853653 870120 IPCTensor.h:367] NewIPCTensor: input_keys_0 [100000]0x1000035c6000
W20240118 18:41:48.853859 870120 IPCTensor.h:367] NewIPCTensor: input_keys_neg_0 [100000]0x10000368b000
W20240118 18:41:48.853986 870120 IPCTensor.h:367] NewIPCTensor: backward_grads_0 [100000, 400]0x100003750000
W20240118 18:41:48.854061 870120 IPCTensor.h:367] NewIPCTensor: backward_grads_neg_0 [100000, 400]0x10000cfe8000
W20240118 18:41:48.854164 870120 IPCTensor.h:406] NewIPCGPUTensor: backward_grads_0_gpu [100000, 400]; dev=0; size=152.6 MB
W20240118 18:41:48.854426 870056 IPCTensor.h:406] NewIPCGPUTensor: embedding_cache_2 [1495, 400]; dev=2; size=2.281 MB
W20240118 18:41:48.854425 869991 IPCTensor.h:406] NewIPCGPUTensor: embedding_cache_1 [1495, 400]; dev=1; size=2.281 MB
W20240118 18:41:48.855234 870120 IPCTensor.h:406] NewIPCGPUTensor: backward_grads_neg_0_gpu [100000, 400]; dev=0; size=152.6 MB
W20240118 18:41:48.855329 870056 IPCTensor.h:367] NewIPCTensor: input_keys_2 [100000]0x100016886000
W20240118 18:41:48.855332 869991 IPCTensor.h:367] NewIPCTensor: input_keys_1 [100000]0x10001694b000
W20240118 18:41:48.855998 870056 IPCTensor.h:367] NewIPCTensor: input_keys_neg_2 [100000]0x100016a10000
W20240118 18:41:48.856009 869991 IPCTensor.h:367] NewIPCTensor: input_keys_neg_1 [100000]0x100016ad5000
W20240118 18:41:48.856047 869991 IPCTensor.h:367] NewIPCTensor: backward_grads_1 [100000, 400]0x100016b9a000
W20240118 18:41:48.856077 870056 IPCTensor.h:367] NewIPCTensor: backward_grads_2 [100000, 400]0x100020432000
W20240118 18:41:48.856108 870056 IPCTensor.h:367] NewIPCTensor: backward_grads_neg_2 [100000, 400]0x100029cca000
W20240118 18:41:48.856112 869991 IPCTensor.h:367] NewIPCTensor: backward_grads_neg_1 [100000, 400]0x100033562000
W20240118 18:41:48.856194 869991 IPCTensor.h:406] NewIPCGPUTensor: backward_grads_1_gpu [100000, 400]; dev=1; size=152.6 MB
W20240118 18:41:48.856209 870056 IPCTensor.h:406] NewIPCGPUTensor: backward_grads_2_gpu [100000, 400]; dev=2; size=152.6 MB
W20240118 18:41:48.856762 870056 IPCTensor.h:406] NewIPCGPUTensor: backward_grads_neg_2_gpu [100000, 400]; dev=2; size=152.6 MB
W20240118 18:41:48.856770 869991 IPCTensor.h:406] NewIPCGPUTensor: backward_grads_neg_1_gpu [100000, 400]; dev=1; size=152.6 MB
INFO [870119 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 3
INFO [870119 distributed_c10d.py:476] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
W20240118 18:41:48.858038 870121 IPCTensor.h:406] NewIPCGPUTensor: embedding_cache_3 [1495, 400]; dev=3; size=2.281 MB
W20240118 18:41:48.859706 870121 IPCTensor.h:367] NewIPCTensor: input_keys_3 [100000]0x10003ce06000
W20240118 18:41:48.859802 870121 IPCTensor.h:367] NewIPCTensor: input_keys_neg_3 [100000]0x10003cecb000
W20240118 18:41:48.859841 870121 IPCTensor.h:367] NewIPCTensor: backward_grads_3 [100000, 400]0x10003cf90000
W20240118 18:41:48.859870 870121 IPCTensor.h:367] NewIPCTensor: backward_grads_neg_3 [100000, 400]0x100046828000
W20240118 18:41:48.859902 870121 IPCTensor.h:406] NewIPCGPUTensor: backward_grads_3_gpu [100000, 400]; dev=3; size=152.6 MB
W20240118 18:41:48.861938 870121 IPCTensor.h:406] NewIPCGPUTensor: backward_grads_neg_3_gpu [100000, 400]; dev=3; size=152.6 MB
cudaHostRegister 0x100001ee2000
2: KnownLocalCachedEmbedding init done
WARNING [870055 DistTensor.py:56] The tensor name already exists in the kvstore
cudaHostRegister 0x100001ee2000
1: KnownLocalCachedEmbedding init done
WARNING [869990 DistTensor.py:56] The tensor name already exists in the kvstore
cudaHostRegister 0x100001ee2000
3: KnownLocalCachedEmbedding init done
WARNING [870119 DistTensor.py:56] The tensor name already exists in the kvstore
[Rank3] pid = 870119
New CachedEmbedding, name=full_emb, shape=(14951, 400), cache_type=KnownLocalCachedEmbedding
fixed cache_range is [(0, 1495), (1495, 2990), (2990, 4485), (4485, 5980)]
cudaHostRegister 0x100001ee2000
0: KnownLocalCachedEmbedding init done
WARNING [869638 DistTensor.py:56] The tensor name already exists in the kvstore
I20240118 18:41:49.000263 870120 IPC_barrier.h:118] CreateBarrier: new barrier, name: kgcachecontroller, count: 4
I20240118 18:41:49.000313 870121 IPC_barrier.h:112] CreateBarrier: find existing barrier, name: kgcachecontroller, count: 4
I20240118 18:41:49.000485 870056 IPC_barrier.h:112] CreateBarrier: find existing barrier, name: kgcachecontroller, count: 4
I20240118 18:41:49.000542 869991 IPC_barrier.h:112] CreateBarrier: find existing barrier, name: kgcachecontroller, count: 4
W20240118 18:41:49.001510 870120 grad_base.h:55] KGCacheController, config={
        "num_gpus": 4,
        "L": 10,
        "backgrad_init": "both", 
        "kForwardItersPerStep": 2,
        "clr": 1,
        "nr_background_threads": 32,
        "backwardMode": "CppAsyncV2",
        "cache_ratio": 0.1
        }
W20240118 18:41:49.001595 870120 grad_base.h:172] Init GradProcessingBase done
I20240118 18:41:49.004948 870120 grad_async_v2.h:32] Use background thread to update emb. nr_background_threads=32
W20240118 18:41:49.005026 870120 kg_controller.h:78] after init GradAsyncProcessingV2
I20240118 18:41:49.005033 870120 kg_controller.h:84] Construct KGCacheController done
I20240118 18:41:49.358670 870120 grad_base.h:60] GraphEnv RegTensorsPerProcess
W20240118 18:41:50.071719 870539 grad_async_v2.h:120] Detect new sample comes, old_end0, new_end1
E20240118 18:41:50.072019 870539 parallel_pq_v2.h:76] insert failed, size(hashtable)=1
W20240118 18:41:50.078527 870538 HazptrDomain.h:770] 10 request backlog for hazptr asynchronous reclamation executor
I20240118 18:41:50.107736 870121 IPC_barrier.h:118] CreateBarrier: new barrier, name: start_barrier, count: 4
I20240118 18:41:50.116729 870120 IPC_barrier.h:112] CreateBarrier: find existing barrier, name: start_barrier, count: 4
I20240118 18:41:50.133097 869991 IPC_barrier.h:112] CreateBarrier: find existing barrier, name: start_barrier, count: 4
I20240118 18:41:50.139763 870056 IPC_barrier.h:112] CreateBarrier: find existing barrier, name: start_barrier, count: 4
+---------------------------+---------------------------+---------------------------+
| Name                      | P50                       | P99                       |
+---------------------------+---------------------------+---------------------------+
| GenInput                  | 1.301 ms                  | 1.301 ms                  |
+---------------------------+---------------------------+---------------------------+
+---------------------------+---------------------------+---------------------------+
| Name                      | P50                       | P99                       |
+---------------------------+---------------------------+---------------------------+
| GenInput                  | 1.301 ms                  | 1.301 ms                  |
+---------------------------+---------------------------+---------------------------+
+---------------------------+---------------------------+---------------------------+
| Name                      | P50                       | P99                       |
+---------------------------+---------------------------+---------------------------+
| GenInput                  | 1.301 ms                  | 1.301 ms                  |
| Forward                   | 9.272 s                   | 9.272 s                   |
| Backward                  | 1.722 s                   | 1.722 s                   |
+---------------------------+---------------------------+---------------------------+
E20240118 18:42:07.430946 870120 parallel_pq_v2.h:76] insert failed, size(hashtable)=0
W20240118 18:42:07.508824 870120 grad_async_v2.h:249] Sleep in <BlockToStepN>, step_no=1, pq.top=0
W20240118 18:42:07.513829 870539 grad_async_v2.h:120] Detect new sample comes, old_end1, new_end2
+---------------------------+---------------------------+---------------------------+
| Name                      | P50                       | P99                       |
+---------------------------+---------------------------+---------------------------+
| GenInput                  | 1.301 ms                  | 1.964 ms                  |
| Forward                   | 9.272 s                   | 9.272 s                   |
| Backward                  | 1.722 s                   | 1.722 s                   |
| Optimize                  | 6.018 s                   | 6.018 s                   |
| BarrierTimeBeforeRank0    | 147.318 us                | 147.318 us                |
| AfterBackward             | 353.982 ms                | 353.982 ms                |
| BlockToStepN              | 2.607 ms                  | 2.607 ms                  |
| OneStep                   | 17.372 s                  | 17.372 s                  |
+---------------------------+---------------------------+---------------------------+
+---------------------------+---------------------------+---------------------------+
| Name                      | P50                       | P99                       |
+---------------------------+---------------------------+---------------------------+
| ProcessBack:Shuffle       | 73.512 ms                 | 76.788 ms                 |
| ProcessBack:UpdateCache   | 190.314 ms                | 190.876 ms                |
| ProcessBack:UpsertPq      | 45.062 ms                 | 76.727 ms                 |
| ProcessOneStep            | 352.968 ms                | 352.968 ms                |
| BlockToStepN              | 2.322 ms                  | 2.322 ms                  |
+---------------------------+---------------------------+---------------------------+
Process Process-2:
Traceback (most recent call last):
  File "/home/xieminhui/miniconda3/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/xieminhui/miniconda3/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/xieminhui/RecStore/src/framework_adapters/torch/kg/dgl-ke/python/dglke/models/pytorch/tensor_models.py", line 140, in decorated_function
    raise exception.__class__(trace)
torch.cuda.OutOfMemoryError: Traceback (most recent call last):
  File "/home/xieminhui/RecStore/src/framework_adapters/torch/kg/dgl-ke/python/dglke/models/pytorch/tensor_models.py", line 128, in _queue_result
    res = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xieminhui/RecStore/src/framework_adapters/torch/kg/dgl-ke/python/dglke/train_pytorch.py", line 409, in train_mp
    train(json_str, args, model, train_sampler, valid_samplers,
  File "/home/xieminhui/RecStore/src/framework_adapters/torch/kg/dgl-ke/python/dglke/train_pytorch.py", line 239, in train
    loss, log = model.forward(pos_g, neg_g, gpu_id)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xieminhui/RecStore/src/framework_adapters/torch/kg/dgl-ke/python/dglke/models/general_models.py", line 599, in forward
    neg_score = self.predict_neg_score(pos_g, neg_g, to_device=cuda,
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xieminhui/RecStore/src/framework_adapters/torch/kg/dgl-ke/python/dglke/models/general_models.py", line 432, in predict_neg_score
    neg_head, tail = self.head_neg_prepare(pos_g.edata['id'], num_chunks, neg_head, tail, gpu_id, trace)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xieminhui/RecStore/src/framework_adapters/torch/kg/dgl-ke/python/dglke/models/pytorch/score_fun.py", line 142, in fn
    projection = self.projection_emb(rel_id, gpu_id, trace)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xieminhui/RecStore/src/framework_adapters/torch/kg/dgl-ke/python/dglke/models/pytorch/tensor_models.py", line 310, in __call__
    data = s.clone().detach().requires_grad_(True)
           ^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.86 GiB (GPU 2; 23.50 GiB total capacity; 17.40 GiB already allocated; 1.96 GiB free; 20.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

[W tensorpipe_agent.cpp:725] RPC agent for worker0 encountered error when reading incoming request from worker2: eof (this error originated at tensorpipe/transport/shm/connection_impl.cc:259)
Process Process-1:
Traceback (most recent call last):
  File "/home/xieminhui/miniconda3/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/xieminhui/miniconda3/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/xieminhui/RecStore/src/framework_adapters/torch/kg/dgl-ke/python/dglke/models/pytorch/tensor_models.py", line 140, in decorated_function
    raise exception.__class__(trace)
torch.cuda.OutOfMemoryError: Traceback (most recent call last):
  File "/home/xieminhui/RecStore/src/framework_adapters/torch/kg/dgl-ke/python/dglke/models/pytorch/tensor_models.py", line 128, in _queue_result
    res = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xieminhui/RecStore/src/framework_adapters/torch/kg/dgl-ke/python/dglke/train_pytorch.py", line 409, in train_mp
    train(json_str, args, model, train_sampler, valid_samplers,
  File "/home/xieminhui/RecStore/src/framework_adapters/torch/kg/dgl-ke/python/dglke/train_pytorch.py", line 239, in train
    loss, log = model.forward(pos_g, neg_g, gpu_id)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xieminhui/RecStore/src/framework_adapters/torch/kg/dgl-ke/python/dglke/models/general_models.py", line 599, in forward
    neg_score = self.predict_neg_score(pos_g, neg_g, to_device=cuda,
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xieminhui/RecStore/src/framework_adapters/torch/kg/dgl-ke/python/dglke/models/general_models.py", line 432, in predict_neg_score
    neg_head, tail = self.head_neg_prepare(pos_g.edata['id'], num_chunks, neg_head, tail, gpu_id, trace)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xieminhui/RecStore/src/framework_adapters/torch/kg/dgl-ke/python/dglke/models/pytorch/score_fun.py", line 142, in fn
    projection = self.projection_emb(rel_id, gpu_id, trace)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xieminhui/RecStore/src/framework_adapters/torch/kg/dgl-ke/python/dglke/models/pytorch/tensor_models.py", line 310, in __call__
    data = s.clone().detach().requires_grad_(True)
           ^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.86 GiB (GPU 1; 23.50 GiB total capacity; 17.40 GiB already allocated; 1.98 GiB free; 20.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

[W tensorpipe_agent.cpp:725] RPC agent for worker0 encountered error when reading incoming request from worker1: eof (this error originated at tensorpipe/transport/shm/connection_impl.cc:259)
Process Process-3:
Traceback (most recent call last):
  File "/home/xieminhui/miniconda3/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/xieminhui/miniconda3/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/xieminhui/RecStore/src/framework_adapters/torch/kg/dgl-ke/python/dglke/models/pytorch/tensor_models.py", line 140, in decorated_function
    raise exception.__class__(trace)
torch.cuda.OutOfMemoryError: Traceback (most recent call last):
  File "/home/xieminhui/RecStore/src/framework_adapters/torch/kg/dgl-ke/python/dglke/models/pytorch/tensor_models.py", line 128, in _queue_result
    res = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xieminhui/RecStore/src/framework_adapters/torch/kg/dgl-ke/python/dglke/train_pytorch.py", line 409, in train_mp
    train(json_str, args, model, train_sampler, valid_samplers,
  File "/home/xieminhui/RecStore/src/framework_adapters/torch/kg/dgl-ke/python/dglke/train_pytorch.py", line 239, in train
    loss, log = model.forward(pos_g, neg_g, gpu_id)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xieminhui/RecStore/src/framework_adapters/torch/kg/dgl-ke/python/dglke/models/general_models.py", line 599, in forward
    neg_score = self.predict_neg_score(pos_g, neg_g, to_device=cuda,
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xieminhui/RecStore/src/framework_adapters/torch/kg/dgl-ke/python/dglke/models/general_models.py", line 432, in predict_neg_score
    neg_head, tail = self.head_neg_prepare(pos_g.edata['id'], num_chunks, neg_head, tail, gpu_id, trace)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xieminhui/RecStore/src/framework_adapters/torch/kg/dgl-ke/python/dglke/models/pytorch/score_fun.py", line 142, in fn
    projection = self.projection_emb(rel_id, gpu_id, trace)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xieminhui/RecStore/src/framework_adapters/torch/kg/dgl-ke/python/dglke/models/pytorch/tensor_models.py", line 310, in __call__
    data = s.clone().detach().requires_grad_(True)
           ^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.86 GiB (GPU 3; 23.50 GiB total capacity; 17.40 GiB already allocated; 2.19 GiB free; 20.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

[W tensorpipe_agent.cpp:725] RPC agent for worker0 encountered error when reading incoming request from worker3: eof (this error originated at tensorpipe/transport/shm/connection_impl.cc:259)
-------Step 0-------
tensor([ 9125, 13416, 14259, 11704,  5590,  6210,   226,  1982,  3482, 11435]) tensor([ 1823,   242,  7292, 12284, 12301,   146, 10570,  4338, 11769,  8608])
-------Step 1-------
tensor([ 9125, 13416, 14259, 11704,  5590,  6210,   226,  1982,  3482, 11435]) tensor([ 8437, 12983,  3730, 13158,  2682,  3459,  8449,  2257, 14864,  9534])
-------Step 2-------
tensor([    4, 12465,  7484,   378,   869,  9636,  3302,    81, 10658,   353]) tensor([ 9257, 10588,  7422,  7243,  3150,  6562,  7134,  5483, 11855, 12397])
-------Step 3-------
tensor([    4, 12465,  7484,   378,   869,  9636,  3302,    81, 10658,   353]) tensor([10896,  5578, 14923, 13730, 11593, 11977, 13977,  3632,  9679,  5065])
-------Step 4-------
tensor([14070,  5767,  6607,  5360,  1770,  5401,  1092, 11888, 11490,   184]) tensor([ 5985,  3743,  9560, 11896,  8191,  6163,     7, 14937,  7246,  5757])
-------Step 5-------
tensor([14070,  5767,  6607,  5360,  1770,  5401,  1092, 11888, 11490,   184]) tensor([ 5705, 11268,  4854,  9209, 11455,  5912, 11428,  4865,  9857,   242])
-------Step 6-------
tensor([ 2316,    30,  3443,   183, 13361,   152, 10186,  4675, 11117,  3749]) tensor([  525, 12256,  6584,   217,  1754,  2452, 13279, 14937,  9529,  6357])
-------Step 7-------
tensor([ 2316,    30,  3443,   183, 13361,   152, 10186,  4675, 11117,  3749]) tensor([ 6838,  6582, 14049,  5200,  6445,  2194,  1842, 12533,  6881,  7155])
-------Step 8-------
tensor([   75,  2057,  1488,  6922,  3904,  6585,  2495,   139, 11424, 13059]) tensor([ 5540,  2231,   107, 12924,  4086, 10102,  3486,  5030,   328,  8603])
-------Step 9-------
tensor([   75,  2057,  1488,  6922,  3904,  6585,  2495,   139, 11424, 13059]) tensor([  336,  1301,  6349,  8533,     6,  1416,  8948, 11604, 10552,  1134])
Traceback (most recent call last):
  File "/home/xieminhui/RecStore/src/framework_adapters/torch/python/dgl-ke-main.py", line 518, in <module>
    main()
  File "/home/xieminhui/RecStore/src/framework_adapters/torch/python/dgl-ke-main.py", line 433, in main
    train_mp(json_str, args, model,
  File "/home/xieminhui/RecStore/src/framework_adapters/torch/kg/dgl-ke/python/dglke/models/pytorch/tensor_models.py", line 140, in decorated_function
    raise exception.__class__(trace)
torch.cuda.OutOfMemoryError: Traceback (most recent call last):
  File "/home/xieminhui/RecStore/src/framework_adapters/torch/kg/dgl-ke/python/dglke/models/pytorch/tensor_models.py", line 128, in _queue_result
    res = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xieminhui/RecStore/src/framework_adapters/torch/kg/dgl-ke/python/dglke/train_pytorch.py", line 409, in train_mp
    train(json_str, args, model, train_sampler, valid_samplers,
  File "/home/xieminhui/RecStore/src/framework_adapters/torch/kg/dgl-ke/python/dglke/train_pytorch.py", line 239, in train
    loss, log = model.forward(pos_g, neg_g, gpu_id)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xieminhui/RecStore/src/framework_adapters/torch/kg/dgl-ke/python/dglke/models/general_models.py", line 599, in forward
    neg_score = self.predict_neg_score(pos_g, neg_g, to_device=cuda,
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xieminhui/RecStore/src/framework_adapters/torch/kg/dgl-ke/python/dglke/models/general_models.py", line 432, in predict_neg_score
    neg_head, tail = self.head_neg_prepare(pos_g.edata['id'], num_chunks, neg_head, tail, gpu_id, trace)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xieminhui/RecStore/src/framework_adapters/torch/kg/dgl-ke/python/dglke/models/pytorch/score_fun.py", line 142, in fn
    projection = self.projection_emb(rel_id, gpu_id, trace)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xieminhui/RecStore/src/framework_adapters/torch/kg/dgl-ke/python/dglke/models/pytorch/tensor_models.py", line 310, in __call__
    data = s.clone().detach().requires_grad_(True)
           ^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.86 GiB (GPU 0; 23.50 GiB total capacity; 17.40 GiB already allocated; 2.70 GiB free; 20.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

timer name is Forward
timer name is OneStep
*** Aborted at 1705574534 (Unix time, try 'date -d @1705574534') ***
*** Signal 11 (SIGSEGV) (0x1000007b8000) received by PID 869638 (pthread TID 0x7f1cb4ff9640) (linux TID 870539) (code: address not mapped to object), stack trace: ***
W20240118 18:42:16.055827 869638 HazptrDomain.h:148] Tagged objects remain. This may indicate a higher-level leak of object(s) that use hazptr_obj_cohort.
    @ 00000000006ddb22 folly::symbolizer::(anonymous namespace)::innerSignalHandler(int, siginfo_t*, void*)
                       /home/xieminhui/RecStore/third_party/folly/folly/experimental/symbolizer/SignalHandler.cpp:449
    @ 00000000006ddc08 folly::symbolizer::(anonymous namespace)::signalHandler(int, siginfo_t*, void*)
                       /home/xieminhui/RecStore/third_party/folly/folly/experimental/symbolizer/SignalHandler.cpp:470
    @ 000000000004251f (unknown)
    @ 0000000001702ce4 at::native::_local_scalar_dense_cpu(at::Tensor const&)::{lambda()#1}::operator()() const
                       /home/xieminhui/pytorch/aten/src/ATen/native/Scalar.cpp:33
    @ 00000000017033ac at::native::_local_scalar_dense_cpu(at::Tensor const&)
                       /home/xieminhui/pytorch/aten/src/ATen/native/Scalar.cpp:33
    @ 000000000232fdb4 c10::impl::wrap_kernel_functor_unboxed_<c10::impl::detail::WrapFunctionIntoFunctor_<c10::CompileTimeFunctionPointer<c10::Scalar (at::Tensor const&), &at::(anonymous namespace)::(anonymous namespace)::wrapper_CPU___local_scalar_dense>, c10::Scalar, c10::guts::typelist::typelist<at::Tensor const&> >, c10::Scalar (at::Tensor const&)>::call(c10::OperatorKernel*, c10::DispatchKeySet, at::Tensor const&)
                       /home/xieminhui/pytorch/build/aten/src/ATen/RegisterCPU.cpp:12766
    @ 0000000001f6cdc7 at::_ops::_local_scalar_dense::redispatch(c10::DispatchKeySet, at::Tensor const&)
                       /home/xieminhui/pytorch/aten/src/ATen/core/boxing/KernelFunction_impl.h:50
                       -> /home/xieminhui/pytorch/build/aten/src/ATen/Operators_2.cpp
    @ 00000000038c0612 torch::autograd::VariableType::(anonymous namespace)::_local_scalar_dense(c10::DispatchKeySet, at::Tensor const&)
                       /home/xieminhui/pytorch/build/aten/src/ATen/RedispatchFunctions.h:8567
                       -> /home/xieminhui/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp
    @ 00000000038c06f7 c10::impl::wrap_kernel_functor_unboxed_<c10::impl::detail::WrapFunctionIntoFunctor_<c10::CompileTimeFunctionPointer<c10::Scalar (c10::DispatchKeySet, at::Tensor const&), &torch::autograd::VariableType::(anonymous namespace)::_local_scalar_dense>, c10::Scalar, c10::guts::typelist::typelist<c10::DispatchKeySet, at::Tensor const&> >, c10::Scalar (c10::DispatchKeySet, at::Tensor const&)>::call(c10::OperatorKernel*, c10::DispatchKeySet, at::Tensor const&)
                       /home/xieminhui/pytorch/aten/src/ATen/core/boxing/impl/WrapFunctionIntoFunctor.h:13
                       -> /home/xieminhui/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp
    @ 00000000020051b7 at::_ops::_local_scalar_dense::call(at::Tensor const&)
                       /home/xieminhui/pytorch/aten/src/ATen/core/boxing/KernelFunction_impl.h:50
                       -> /home/xieminhui/pytorch/build/aten/src/ATen/Operators_2.cpp
    @ 0000000001703473 at::native::item(at::Tensor const&)
                       /home/xieminhui/pytorch/build/aten/src/ATen/ops/_local_scalar_dense.h:27
                       -> /home/xieminhui/pytorch/aten/src/ATen/native/Scalar.cpp
    @ 0000000002668ee4 c10::impl::wrap_kernel_functor_unboxed_<c10::impl::detail::WrapFunctionIntoFunctor_<c10::CompileTimeFunctionPointer<c10::Scalar (at::Tensor const&), &at::(anonymous namespace)::(anonymous namespace)::wrapper_CompositeImplicitAutograd__item>, c10::Scalar, c10::guts::typelist::typelist<at::Tensor const&> >, c10::Scalar (at::Tensor const&)>::call(c10::OperatorKernel*, c10::DispatchKeySet, at::Tensor const&)
                       /home/xieminhui/pytorch/build/aten/src/ATen/RegisterCompositeImplicitAutograd.cpp:4379
    @ 0000000001e71d57 at::_ops::item::call(at::Tensor const&)
                       /home/xieminhui/pytorch/aten/src/ATen/core/boxing/KernelFunction_impl.h:50
                       -> /home/xieminhui/pytorch/build/aten/src/ATen/Operators_1.cpp
    @ 0000000002c26d3b long at::Tensor::item<long>() const
                       /home/xieminhui/pytorch/build/aten/src/ATen/core/TensorBody.h:4161
                       -> /home/xieminhui/pytorch/build/aten/src/ATen/core/TensorMethods.cpp
    @ 000000000051b03d recstore::GradAsyncProcessingV2::DetectNewSamplesCome()
                       /home/xieminhui/RecStore/src/framework_adapters/torch/grad_async_v2.h:117
                       -> /home/xieminhui/RecStore/src/framework_adapters/torch/kg_controller.cc
    @ 000000000051bd37 recstore::GradAsyncProcessingV2::DetectThread()
                       /home/xieminhui/RecStore/src/framework_adapters/torch/grad_async_v2.h:189
                       -> /home/xieminhui/RecStore/src/framework_adapters/torch/kg_controller.cc
    @ 00000000000d3e94 execute_native_thread_routine
                       ../../../../../libstdc++-v3/src/c++11/thread.cc:104
                       -> /home/conda/feedstock_root/build_artifacts/gcc_compilers_1699751170916/work/build/x86_64-conda-linux-gnu/libstdc++-v3/src/c++11/../../../../../libstdc++-v3/src/c++11/thread.cc
    @ 0000000000094ac2 (unknown)
    @ 000000000012684f (unknown)
