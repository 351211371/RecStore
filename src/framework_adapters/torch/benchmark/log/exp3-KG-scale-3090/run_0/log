WARNING: Logging before InitGoogleLogging() is written to STDERR
I20240123 01:28:01.082348  7634 IPC_barrier.h:128] MultiProcessBarrierFactory->ClearIPCMemory()
/home/xieminhui/RecStore/src/framework_adapters/torch/kg/dgl-0.9.1/python/dgl/_deprecate/graph.py:1023: DGLWarning: multigraph will be deprecated.DGL will treat all graphs as multigraph in the future.
  dgl_warning("multigraph will be deprecated." \
/home/xieminhui/RecStore/src/framework_adapters/torch/kg/dgl-0.9.1/python/dgl/heterograph.py:72: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.
  dgl_warning('Recommend creating graphs by `dgl.graph(data)`'
Reading train triples....
Finished. Read 483142 train triples.
Reading valid triples....
Finished. Read 50000 valid triples.
Reading test triples....
Finished. Read 59071 test triples.
ARGS:  Namespace(model_name='TransE', data_path='/home/xieminhui/dgl-data', dataset='FB15k', format='built_in', data_files=None, delimiter='\t', save_path='/tmp/ckpts/TransE_FB15k_15', no_save_emb=True, max_step=500, batch_size=400, batch_size_eval=16, neg_sample_size=200, neg_deg_sample=False, neg_deg_sample_eval=False, neg_sample_size_eval=14951, eval_percent=1, no_eval_filter=False, log_interval=100, eval_interval=10000, test=False, num_proc=2, num_thread=1, force_sync_interval=1, hidden_dim=400, lr=0.01, gamma=16.0, double_ent=False, double_rel=False, neg_adversarial_sampling=False, adversarial_temperature=1.0, regularization_coef=0.0, regularization_norm=3, pairwise=False, loss_genre='Logsigmoid', margin=1.0, nr_gpus=2, gpu=[0, 1], mix_cpu_gpu=True, valid=False, rel_part=False, async_update=None, has_edge_importance=False, cached_emb_type='KnownLocalCachedEmbedding', use_my_emb=True, cache_ratio=0.05, shuffle=False, backwardMode='CppAsyncV2', L=10, kForwardItersPerStep=2, backgrad_init='both', eval_filter=True, soft_rel_part=False)
|Train|: 483142
original graph:  DGLGraph(num_nodes=14951, num_edges=592213,
         ndata_schemes={}
         edata_schemes={'tid': Scheme(shape=(), dtype=torch.int64)})
Loading cached metis
WARNING [7634 sampler.py:454] Start PreSampling
WARNING [7634 sampler.py:532] Before construct renumbering_dict
WARNING [7634 sampler.py:555] PreSampling done
W20240123 01:28:03.984127  7634 IPCTensor.h:367] NewIPCTensor: cached_sampler_r0_0 [100000]0x100000002000
W20240123 01:28:03.984436  7634 IPCTensor.h:367] NewIPCTensor: cached_sampler_r0_1 [100000]0x1000000c7000
W20240123 01:28:03.984504  7634 IPCTensor.h:367] NewIPCTensor: cached_sampler_r0_2 [100000]0x10000018c000
W20240123 01:28:03.984534  7634 IPCTensor.h:367] NewIPCTensor: cached_sampler_r0_3 [100000]0x100000251000
W20240123 01:28:03.984572  7634 IPCTensor.h:367] NewIPCTensor: cached_sampler_r0_4 [100000]0x100000316000
W20240123 01:28:03.984613  7634 IPCTensor.h:367] NewIPCTensor: cached_sampler_r0_5 [100000]0x1000003db000
W20240123 01:28:03.984644  7634 IPCTensor.h:367] NewIPCTensor: cached_sampler_r0_6 [100000]0x1000004a0000
W20240123 01:28:03.984696  7634 IPCTensor.h:367] NewIPCTensor: cached_sampler_r0_7 [100000]0x100000565000
W20240123 01:28:03.984735  7634 IPCTensor.h:367] NewIPCTensor: cached_sampler_r0_8 [100000]0x10000062a000
W20240123 01:28:03.984762  7634 IPCTensor.h:367] NewIPCTensor: cached_sampler_r0_9 [100000]0x1000006ef000
W20240123 01:28:03.984833  7634 IPCTensor.h:367] NewIPCTensor: step_r0 [10]0x1000007b4000
W20240123 01:28:03.984869  7634 IPCTensor.h:367] NewIPCTensor: circle_buffer_end_r0 [1]0x1000007b6000
W20240123 01:28:03.984913  7634 IPCTensor.h:367] NewIPCTensor: circle_buffer_end_cppseen_r0 [1]0x1000007b8000
W20240123 01:28:03.985061  7634 IPCTensor.h:367] NewIPCTensor: cached_sampler_r1_0 [100000]0x1000007ba000
W20240123 01:28:03.985106  7634 IPCTensor.h:367] NewIPCTensor: cached_sampler_r1_1 [100000]0x10000087f000
W20240123 01:28:03.985143  7634 IPCTensor.h:367] NewIPCTensor: cached_sampler_r1_2 [100000]0x100000944000
W20240123 01:28:03.985169  7634 IPCTensor.h:367] NewIPCTensor: cached_sampler_r1_3 [100000]0x100000a09000
W20240123 01:28:03.985196  7634 IPCTensor.h:367] NewIPCTensor: cached_sampler_r1_4 [100000]0x100000ace000
W20240123 01:28:03.985249  7634 IPCTensor.h:367] NewIPCTensor: cached_sampler_r1_5 [100000]0x100000b93000
W20240123 01:28:03.985284  7634 IPCTensor.h:367] NewIPCTensor: cached_sampler_r1_6 [100000]0x100000c58000
W20240123 01:28:03.985333  7634 IPCTensor.h:367] NewIPCTensor: cached_sampler_r1_7 [100000]0x100000d1d000
W20240123 01:28:03.985388  7634 IPCTensor.h:367] NewIPCTensor: cached_sampler_r1_8 [100000]0x100000de2000
W20240123 01:28:03.985436  7634 IPCTensor.h:367] NewIPCTensor: cached_sampler_r1_9 [100000]0x100000ea7000
W20240123 01:28:03.985479  7634 IPCTensor.h:367] NewIPCTensor: step_r1 [10]0x100000f6c000
W20240123 01:28:03.985503  7634 IPCTensor.h:367] NewIPCTensor: circle_buffer_end_r1 [1]0x100000f6e000
W20240123 01:28:03.985527  7634 IPCTensor.h:367] NewIPCTensor: circle_buffer_end_cppseen_r1 [1]0x100000f70000
W20240123 01:28:04.194584  7634 IPCTensor.h:367] NewIPCTensor: full_emb [14951, 400]0x100000f72000
W20240123 01:28:04.194730  7634 IPCTensor.h:367] NewIPCTensor: full_emb_SparseRowWiseAdaGrad_sum [14951]0x100002644000
{0: Graph(num_nodes=13500, num_edges=273181,
      ndata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64), 'inner_node': Scheme(shape=(), dtype=torch.int32), 'part_id': Scheme(shape=(), dtype=torch.int64)}
      edata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64), 'inner_edge': Scheme(shape=(), dtype=torch.int8)}), 1: Graph(num_nodes=12803, num_edges=385691,
      ndata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64), 'inner_node': Scheme(shape=(), dtype=torch.int32), 'part_id': Scheme(shape=(), dtype=torch.int64)}
      edata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64), 'inner_edge': Scheme(shape=(), dtype=torch.int8)})}
MertisPartition: part 0 has 13500 N 273181 E
MertisPartition: part 1 has 12803 N 385691 E
Rank0: cached key size 373
Rank1: cached key size 373
Before renumbering graph:  {'_ID': tensor([    0,     2,     6,  ..., 11079,  2237, 14365]), 'inner_node': tensor([1, 1, 1,  ..., 0, 0, 0], dtype=torch.int32), 'part_id': tensor([0, 0, 0,  ..., 1, 1, 1])}
After renumbering graph:  {'_ID': tensor([  758,   832,   930,  ..., 10928, 11515, 11065]), 'inner_node': tensor([1, 1, 1,  ..., 0, 0, 0], dtype=torch.int32), 'part_id': tensor([0, 0, 0,  ..., 1, 1, 1])}
part_g: DGLGraph(num_nodes=13500, num_edges=273181,
         ndata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64)}
         edata_schemes={'tid': Scheme(shape=(), dtype=torch.int64)})
part_g: DGLGraph(num_nodes=13500, num_edges=273181,
         ndata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64)}
         edata_schemes={'tid': Scheme(shape=(), dtype=torch.int64)})
Total initialize time 3.147 seconds
INFO [7634 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 0
INFO [8367 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 1
INFO [7634 distributed_c10d.py:476] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.
INFO [8367 distributed_c10d.py:476] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.
W20240123 01:28:05.304515  8371 IPCTensor.h:406] NewIPCGPUTensor: embedding_cache_0 [747, 400]; dev=0; size=1.14 MB
W20240123 01:28:05.305016  8380 IPCTensor.h:406] NewIPCGPUTensor: embedding_cache_1 [747, 400]; dev=1; size=1.14 MB
W20240123 01:28:05.327579  8371 IPCTensor.h:367] NewIPCTensor: input_keys_0 [100000]0x100002656000
W20240123 01:28:05.327684  8371 IPCTensor.h:367] NewIPCTensor: input_keys_neg_0 [100000]0x10000271b000
W20240123 01:28:05.327731  8371 IPCTensor.h:367] NewIPCTensor: backward_grads_0 [100000, 400]0x1000027e2000
W20240123 01:28:05.327760  8371 IPCTensor.h:367] NewIPCTensor: backward_grads_neg_0 [100000, 400]0x10000c07a000
W20240123 01:28:05.327792  8371 IPCTensor.h:406] NewIPCGPUTensor: backward_grads_0_gpu [100000, 400]; dev=0; size=152.6 MB
W20240123 01:28:05.327859  8380 IPCTensor.h:367] NewIPCTensor: input_keys_1 [100000]0x100015912000
W20240123 01:28:05.327955  8380 IPCTensor.h:367] NewIPCTensor: input_keys_neg_1 [100000]0x1000159d7000
W20240123 01:28:05.328006  8380 IPCTensor.h:367] NewIPCTensor: backward_grads_1 [100000, 400]0x100015a9c000
W20240123 01:28:05.328050  8380 IPCTensor.h:367] NewIPCTensor: backward_grads_neg_1 [100000, 400]0x10001f334000
W20240123 01:28:05.328092  8380 IPCTensor.h:406] NewIPCGPUTensor: backward_grads_1_gpu [100000, 400]; dev=1; size=152.6 MB
W20240123 01:28:05.328430  8371 IPCTensor.h:406] NewIPCGPUTensor: backward_grads_neg_0_gpu [100000, 400]; dev=0; size=152.6 MB
W20240123 01:28:05.328737  8380 IPCTensor.h:406] NewIPCGPUTensor: backward_grads_neg_1_gpu [100000, 400]; dev=1; size=152.6 MB
[Rank1] pid = 8367
New CachedEmbedding, name=full_emb, shape=(14951, 400), cache_type=KnownLocalCachedEmbedding
fixed cache_range is [(0, 747), (747, 1494)]
cudaHostRegister 0x100000f72000
0: KnownLocalCachedEmbedding init done
cudaHostRegister 0x100000f72000
1: KnownLocalCachedEmbedding init done
WARNING [7634 DistTensor.py:56] The tensor name already exists in the kvstore
WARNING [8367 DistTensor.py:56] The tensor name already exists in the kvstore
I20240123 01:28:05.447994  8371 IPC_barrier.h:118] CreateBarrier: new barrier, name: kgcachecontroller, count: 2
I20240123 01:28:05.448032  8380 IPC_barrier.h:112] CreateBarrier: find existing barrier, name: kgcachecontroller, count: 2
W20240123 01:28:05.448889  8371 grad_base.h:55] KGCacheController, config={
        "num_gpus": 2,
        "L": 10,
        "backgrad_init": "both", 
        "kForwardItersPerStep": 2,
        "clr": 1,
        "nr_background_threads": 32,
        "backwardMode": "CppAsyncV2",
        "cache_ratio": 0.05
        }
W20240123 01:28:05.448985  8371 grad_base.h:172] Init GradProcessingBase done
I20240123 01:28:05.452138  8371 grad_async_v2.h:32] Use background thread to update emb. nr_background_threads=32
W20240123 01:28:05.452215  8371 kg_controller.h:78] after init GradAsyncProcessingV2
I20240123 01:28:05.452221  8371 kg_controller.h:84] Construct KGCacheController done
I20240123 01:28:05.657033  8371 grad_base.h:60] GraphEnv RegTensorsPerProcess
W20240123 01:28:05.863883  8902 grad_async_v2.h:120] Detect new sample comes, old_end0, new_end1
E20240123 01:28:05.864250  8902 parallel_pq_v2.h:76] insert failed, size(hashtable)=1
I20240123 01:28:05.905767  8371 IPC_barrier.h:118] CreateBarrier: new barrier, name: start_barrier, count: 2
I20240123 01:28:05.920342  8380 IPC_barrier.h:112] CreateBarrier: find existing barrier, name: start_barrier, count: 2
E20240123 01:28:07.040441  8371 parallel_pq_v2.h:76] insert failed, size(hashtable)=0
W20240123 01:28:07.048172  8371 grad_async_v2.h:249] Sleep in <BlockToStepN>, step_no=1, pq.top=1
W20240123 01:28:07.054580  8902 grad_async_v2.h:120] Detect new sample comes, old_end1, new_end2
E20240123 01:28:08.040148  8901 parallel_pq_v2.h:76] insert failed, size(hashtable)=1219
W20240123 01:28:08.066828  8371 grad_async_v2.h:249] Sleep in <BlockToStepN>, step_no=56, pq.top=56
W20240123 01:28:08.069478  8902 grad_async_v2.h:120] Detect new sample comes, old_end6, new_end7
train_sampler.Prefill()
before start barrier
start train
[proc 1] 100 steps, total: 3.004, sample: 0.195, forward: 0.945, backward: 0.530, update: 0.078
train_sampler.Prefill()
-------Step 0-------
tensor([  997, 12525,  5108,  9155,   110,  9515, 14171,    50,  3580, 14892]) tensor([  962,  4371,  6809,  3753, 13518,  6400, 10542,  7076,  8747, 13222])
-------Step 1-------
tensor([  997, 12525,  5108,  9155,   110,  9515, 14171,    50,  3580, 14892]) tensor([10803,  4056, 10808,  6699,  1341,  7053,  9247,  7922,  2702,  3893])
-------Step 2-------
tensor([ 5984,  7280, 11449, 11157,  8161,  9740,   124,  4719,  7266,  5978]) tensor([ 2818,  6643,  2657, 10767, 12783,  2639,  4278,  5014, 13620,  3695])
-------Step 3-------
tensor([ 5984,  7280, 11449, 11157,  8161,  9740,   124,  4719,  7266,  5978]) tensor([ 7814,  6264,  1053, 12544, 14275, 13061, 13709,  6138, 14702,  2934])
-------Step 4-------
tensor([ 9630,   187,  5561, 14531, 11944,  8993,  5923,   269,   103, 10837]) tensor([ 7399, 12562,  1091, 10952, 13780,  3697,  9455, 12593,  1475,    63])
-------Step 5-------
tensor([ 9630,   187,  5561, 14531, 11944,  8993,  5923,   269,   103, 10837]) tensor([ 3588,  9065, 12273, 12703,   189, 12098,  2186,  8011,  5140, 12298])
-------Step 6-------
tensor([ 3886,    44,  5268,  9592,  2158, 10103,  3700, 13839,  4042,  4602]) tensor([ 8085,  6421,  5261,  4455, 13930,  3049,   603,  5954, 14049,  3579])
-------Step 7-------
tensor([ 3886,    44,  5268,  9592,  2158, 10103,  3700, 13839,  4042,  4602]) tensor([10372, 10940,     9,  1526,  7452, 13453,   893,  9076,  8598,  7769])
-------Step 8-------
tensor([3941, 6398,  323,   48,   13, 5043, 4827, 6521, 8858,  212]) tensor([ 7842,  8132,  9214, 12823,  5604,  4746,  9550, 13285,  9976,  6065])
-------Step 9-------
tensor([3941, 6398,  323,   48,   13, 5043, 4827, 6521, 8858,  212]) tensor([ 3338,  7747, 12922,  6020,  4173,  8683, 14221,  7052, 10764,  2045])
before start barrier
start train
[proc 0] 100 steps, total: 3.018, sample: 0.221, forward: 1.013, backward: 0.480, update: 0.085
E20240123 01:28:09.040007  8901 parallel_pq_v2.h:76] insert failed, size(hashtable)=2389
W20240123 01:28:09.072377  8371 grad_async_v2.h:249] Sleep in <BlockToStepN>, step_no=107, pq.top=107
W20240123 01:28:09.079911  8902 grad_async_v2.h:120] Detect new sample comes, old_end7, new_end8
E20240123 01:28:10.040014  8901 parallel_pq_v2.h:76] insert failed, size(hashtable)=296
W20240123 01:28:10.072129  8371 grad_async_v2.h:249] Sleep in <BlockToStepN>, step_no=160, pq.top=160
W20240123 01:28:10.092403  8902 grad_async_v2.h:120] Detect new sample comes, old_end1, new_end2
+---------------------------+---------------------------+---------------------------+
| Name                      | P50                       | P99                       |
+---------------------------+---------------------------+---------------------------+
| GenInput                  | 1.032 ms                  | 11.506 ms                 |
| Forward                   | 3.319 ms                  | 9.481 ms                  |
| Backward                  | 3.192 ms                  | 4.039 ms                  |
| Optimize                  | 905.411 us                | 1.309 ms                  |
| BarrierTimeBeforeRank0    | 23.543 us                 | 2.495 ms                  |
| AfterBackward             | 4.903 ms                  | 10.044 ms                 |
| BlockToStepN              | 2.872 ms                  | 8.130 ms                  |
| OneStep                   | 18.308 ms                 | 35.503 ms                 |
+---------------------------+---------------------------+---------------------------+
+---------------------------+---------------------------+---------------------------+
| Name                      | P50                       | P99                       |
+---------------------------+---------------------------+---------------------------+
| ProcessBack:Shuffle       | 577.884 us                | 2.089 ms                  |
| ProcessBack:UpdateCache   | 615.572 us                | 910.933 us                |
| ProcessBack:UpsertPq      | 2.933 ms                  | 7.191 ms                  |
| ProcessOneStep            | 4.829 ms                  | 10.018 ms                 |
| BlockToStepN              | 2.861 ms                  | 8.075 ms                  |
+---------------------------+---------------------------+---------------------------+
[proc 1] 200 steps, total: 1.934, sample: 0.217, forward: 0.327, backward: 0.330, update: 0.086
[proc 0] 200 steps, total: 1.934, sample: 0.227, forward: 0.337, backward: 0.314, update: 0.083
E20240123 01:28:11.040047  8901 parallel_pq_v2.h:76] insert failed, size(hashtable)=6932
W20240123 01:28:11.083127  8371 grad_async_v2.h:249] Sleep in <BlockToStepN>, step_no=212, pq.top=212
W20240123 01:28:11.106036  8902 grad_async_v2.h:120] Detect new sample comes, old_end3, new_end4
E20240123 01:28:12.040006  8901 parallel_pq_v2.h:76] insert failed, size(hashtable)=2940
W20240123 01:28:12.110062  8371 grad_async_v2.h:249] Sleep in <BlockToStepN>, step_no=258, pq.top=258
W20240123 01:28:12.113484  8902 grad_async_v2.h:120] Detect new sample comes, old_end8, new_end9
[proc 0] 300 steps, total: 2.175, sample: 0.208, forward: 0.331, backward: 0.344, update: 0.083
[proc 1] 300 steps, total: 2.175, sample: 0.221, forward: 0.330, backward: 0.496, update: 0.089
E20240123 01:28:13.040047  8902 parallel_pq_v2.h:76] insert failed, size(hashtable)=399
W20240123 01:28:13.114442  8371 grad_async_v2.h:249] Sleep in <BlockToStepN>, step_no=305, pq.top=305
W20240123 01:28:13.119586  8902 grad_async_v2.h:120] Detect new sample comes, old_end5, new_end6
E20240123 01:28:14.040006  8901 parallel_pq_v2.h:76] insert failed, size(hashtable)=8020
W20240123 01:28:14.118242  8371 grad_async_v2.h:249] Sleep in <BlockToStepN>, step_no=358, pq.top=358
W20240123 01:28:14.130357  8902 grad_async_v2.h:120] Detect new sample comes, old_end8, new_end9
[proc 0] 400 steps, total: 1.936, sample: 0.212, forward: 0.342, backward: 0.330, update: 0.085
[proc 1] 400 steps, total: 1.936, sample: 0.211, forward: 0.341, backward: 0.330, update: 0.086
E20240123 01:28:15.040014  8901 parallel_pq_v2.h:76] insert failed, size(hashtable)=8056
W20240123 01:28:15.121642  8371 grad_async_v2.h:249] Sleep in <BlockToStepN>, step_no=407, pq.top=407
W20240123 01:28:15.136929  8902 grad_async_v2.h:120] Detect new sample comes, old_end7, new_end8
+---------------------------+---------------------------+---------------------------+
| Name                      | P50                       | P99                       |
+---------------------------+---------------------------+---------------------------+
| GenInput                  | 1.080 ms                  | 11.431 ms                 |
| Forward                   | 3.372 ms                  | 8.447 ms                  |
| Backward                  | 3.281 ms                  | 5.178 ms                  |
| Optimize                  | 924.560 us                | 1.282 ms                  |
| BarrierTimeBeforeRank0    | 45.208 us                 | 5.308 ms                  |
| AfterBackward             | 4.819 ms                  | 8.982 ms                  |
| BlockToStepN              | 3.333 ms                  | 13.232 ms                 |
| OneStep                   | 19.082 ms                 | 38.344 ms                 |
+---------------------------+---------------------------+---------------------------+
+---------------------------+---------------------------+---------------------------+
| Name                      | P50                       | P99                       |
+---------------------------+---------------------------+---------------------------+
| ProcessBack:Shuffle       | 595.024 us                | 1.119 ms                  |
| ProcessBack:UpdateCache   | 615.160 us                | 885.328 us                |
| ProcessBack:UpsertPq      | 2.912 ms                  | 6.524 ms                  |
| ProcessOneStep            | 4.789 ms                  | 8.948 ms                  |
| BlockToStepN              | 3.371 ms                  | 14.005 ms                 |
+---------------------------+---------------------------+---------------------------+
E20240123 01:28:16.040071  8901 parallel_pq_v2.h:76] insert failed, size(hashtable)=7676
W20240123 01:28:16.138525  8371 grad_async_v2.h:249] Sleep in <BlockToStepN>, step_no=455, pq.top=455
W20240123 01:28:16.152370  8902 grad_async_v2.h:120] Detect new sample comes, old_end5, new_end6
[proc 0] 500 steps, total: 2.060, sample: 0.219, forward: 0.342, backward: 0.334, update: 0.086
[proc 1] 500 steps, total: 2.060, sample: 0.216, forward: 0.360, backward: 0.340, update: 0.090
Successfully xmh. training takes 11.124093294143677 seconds
before call kg_cache_controller.StopThreads()
W20240123 01:28:17.029915  8371 grad_async_v2.h:71] call StopThreads. PID = 7634
W20240123 01:28:17.029949  8371 grad_base.h:210] before processOneStepNegThread_.join();
W20240123 01:28:17.030408  8371 grad_base.h:212] after processOneStepNegThread_.join();
W20240123 01:28:17.030423  8371 grad_async_v2.h:73] call GradProcessingBase::StopThreads.
W20240123 01:28:17.032300  8371 grad_async_v2.h:91] StopThreads done.
[W tensorpipe_agent.cpp:725] RPC agent for worker0 encountered error when reading incoming request from worker1: eof (this error originated at tensorpipe/transport/shm/connection_impl.cc:259)
On rank0, prepare to call self.controller.StopThreads(), self=<python.controller_process.KGCacheControllerWrapper object at 0x7f37d1f05390>
