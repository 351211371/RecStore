WARNING: Logging before InitGoogleLogging() is written to STDERR
I20240123 01:28:27.973832 11885 IPC_barrier.h:128] MultiProcessBarrierFactory->ClearIPCMemory()
/home/xieminhui/RecStore/src/framework_adapters/torch/kg/dgl-0.9.1/python/dgl/_deprecate/graph.py:1023: DGLWarning: multigraph will be deprecated.DGL will treat all graphs as multigraph in the future.
  dgl_warning("multigraph will be deprecated." \
/home/xieminhui/RecStore/src/framework_adapters/torch/kg/dgl-0.9.1/python/dgl/heterograph.py:72: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.
  dgl_warning('Recommend creating graphs by `dgl.graph(data)`'
Reading train triples....
Finished. Read 483142 train triples.
Reading valid triples....
Finished. Read 50000 valid triples.
Reading test triples....
Finished. Read 59071 test triples.
ARGS:  Namespace(model_name='TransE', data_path='/home/xieminhui/dgl-data', dataset='FB15k', format='built_in', data_files=None, delimiter='\t', save_path='/tmp/ckpts/TransE_FB15k_16', no_save_emb=True, max_step=500, batch_size=400, batch_size_eval=16, neg_sample_size=200, neg_deg_sample=False, neg_deg_sample_eval=False, neg_sample_size_eval=14951, eval_percent=1, no_eval_filter=False, log_interval=100, eval_interval=10000, test=False, num_proc=2, num_thread=1, force_sync_interval=1, hidden_dim=400, lr=0.01, gamma=16.0, double_ent=False, double_rel=False, neg_adversarial_sampling=False, adversarial_temperature=1.0, regularization_coef=0.0, regularization_norm=3, pairwise=False, loss_genre='Logsigmoid', margin=1.0, nr_gpus=2, gpu=[0, 1], mix_cpu_gpu=True, valid=False, rel_part=False, async_update=None, has_edge_importance=False, cached_emb_type='None', use_my_emb=False, cache_ratio=0.05, shuffle=False, backwardMode='CppSync', L=10, kForwardItersPerStep=2, backgrad_init='both', eval_filter=True, soft_rel_part=False)
|Train|: 483142
original graph:  DGLGraph(num_nodes=14951, num_edges=592213,
         ndata_schemes={}
         edata_schemes={'tid': Scheme(shape=(), dtype=torch.int64)})
Loading cached metis
WARNING [11885 sampler.py:454] Start PreSampling
WARNING [11885 sampler.py:532] Before construct renumbering_dict
WARNING [11885 sampler.py:555] PreSampling done
W20240123 01:28:30.888142 11885 IPCTensor.h:367] NewIPCTensor: cached_sampler_r0_0 [100000]0x100000002000
W20240123 01:28:30.888303 11885 IPCTensor.h:367] NewIPCTensor: cached_sampler_r0_1 [100000]0x1000000c7000
W20240123 01:28:30.888337 11885 IPCTensor.h:367] NewIPCTensor: cached_sampler_r0_2 [100000]0x10000018c000
W20240123 01:28:30.888370 11885 IPCTensor.h:367] NewIPCTensor: cached_sampler_r0_3 [100000]0x100000251000
W20240123 01:28:30.888393 11885 IPCTensor.h:367] NewIPCTensor: cached_sampler_r0_4 [100000]0x100000316000
W20240123 01:28:30.888423 11885 IPCTensor.h:367] NewIPCTensor: cached_sampler_r0_5 [100000]0x1000003db000
W20240123 01:28:30.888448 11885 IPCTensor.h:367] NewIPCTensor: cached_sampler_r0_6 [100000]0x1000004a0000
W20240123 01:28:30.888479 11885 IPCTensor.h:367] NewIPCTensor: cached_sampler_r0_7 [100000]0x100000565000
W20240123 01:28:30.888514 11885 IPCTensor.h:367] NewIPCTensor: cached_sampler_r0_8 [100000]0x10000062a000
W20240123 01:28:30.888540 11885 IPCTensor.h:367] NewIPCTensor: cached_sampler_r0_9 [100000]0x1000006ef000
W20240123 01:28:30.888573 11885 IPCTensor.h:367] NewIPCTensor: step_r0 [10]0x1000007b4000
W20240123 01:28:30.888597 11885 IPCTensor.h:367] NewIPCTensor: circle_buffer_end_r0 [1]0x1000007b6000
W20240123 01:28:30.888623 11885 IPCTensor.h:367] NewIPCTensor: circle_buffer_end_cppseen_r0 [1]0x1000007b8000
W20240123 01:28:30.888736 11885 IPCTensor.h:367] NewIPCTensor: cached_sampler_r1_0 [100000]0x1000007ba000
W20240123 01:28:30.888767 11885 IPCTensor.h:367] NewIPCTensor: cached_sampler_r1_1 [100000]0x10000087f000
W20240123 01:28:30.888798 11885 IPCTensor.h:367] NewIPCTensor: cached_sampler_r1_2 [100000]0x100000944000
W20240123 01:28:30.888826 11885 IPCTensor.h:367] NewIPCTensor: cached_sampler_r1_3 [100000]0x100000a09000
W20240123 01:28:30.888852 11885 IPCTensor.h:367] NewIPCTensor: cached_sampler_r1_4 [100000]0x100000ace000
W20240123 01:28:30.888895 11885 IPCTensor.h:367] NewIPCTensor: cached_sampler_r1_5 [100000]0x100000b93000
W20240123 01:28:30.888924 11885 IPCTensor.h:367] NewIPCTensor: cached_sampler_r1_6 [100000]0x100000c58000
W20240123 01:28:30.888947 11885 IPCTensor.h:367] NewIPCTensor: cached_sampler_r1_7 [100000]0x100000d1d000
W20240123 01:28:30.888975 11885 IPCTensor.h:367] NewIPCTensor: cached_sampler_r1_8 [100000]0x100000de2000
W20240123 01:28:30.889000 11885 IPCTensor.h:367] NewIPCTensor: cached_sampler_r1_9 [100000]0x100000ea7000
W20240123 01:28:30.889027 11885 IPCTensor.h:367] NewIPCTensor: step_r1 [10]0x100000f6c000
W20240123 01:28:30.889047 11885 IPCTensor.h:367] NewIPCTensor: circle_buffer_end_r1 [1]0x100000f6e000
W20240123 01:28:30.889067 11885 IPCTensor.h:367] NewIPCTensor: circle_buffer_end_cppseen_r1 [1]0x100000f70000
{0: Graph(num_nodes=13500, num_edges=273181,
      ndata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64), 'inner_node': Scheme(shape=(), dtype=torch.int32), 'part_id': Scheme(shape=(), dtype=torch.int64)}
      edata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64), 'inner_edge': Scheme(shape=(), dtype=torch.int8)}), 1: Graph(num_nodes=12803, num_edges=385691,
      ndata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64), 'inner_node': Scheme(shape=(), dtype=torch.int32), 'part_id': Scheme(shape=(), dtype=torch.int64)}
      edata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64), 'inner_edge': Scheme(shape=(), dtype=torch.int8)})}
MertisPartition: part 0 has 13500 N 273181 E
MertisPartition: part 1 has 12803 N 385691 E
Rank0: cached key size 373
Rank1: cached key size 373
Before renumbering graph:  {'_ID': tensor([    0,     2,     6,  ..., 11079,  2237, 14365]), 'inner_node': tensor([1, 1, 1,  ..., 0, 0, 0], dtype=torch.int32), 'part_id': tensor([0, 0, 0,  ..., 1, 1, 1])}
After renumbering graph:  {'_ID': tensor([  752,   808,   914,  ...,  9638, 12772, 12664]), 'inner_node': tensor([1, 1, 1,  ..., 0, 0, 0], dtype=torch.int32), 'part_id': tensor([0, 0, 0,  ..., 1, 1, 1])}
part_g: DGLGraph(num_nodes=13500, num_edges=273181,
         ndata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64)}
         edata_schemes={'tid': Scheme(shape=(), dtype=torch.int64)})
part_g: DGLGraph(num_nodes=13500, num_edges=273181,
         ndata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64)}
         edata_schemes={'tid': Scheme(shape=(), dtype=torch.int64)})
Total initialize time 3.157 seconds
INFO [12170 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 1
INFO [11885 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 0
INFO [11885 distributed_c10d.py:476] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.
INFO [12170 distributed_c10d.py:476] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.
I20240123 01:28:33.220248 12171 IPC_barrier.h:118] CreateBarrier: new barrier, name: start_barrier, count: 2
I20240123 01:28:33.229327 12176 IPC_barrier.h:112] CreateBarrier: find existing barrier, name: start_barrier, count: 2
[Rank1] pid = 12170
train_sampler.Prefill()
-------Step 0-------
tensor([  928, 12645,  4757,  8434,   106, 13939, 13223,     5,  3798, 13839]) tensor([  946,  4014,  6353,  3720, 13597,  6397, 11775,  6505,  7993, 14114])
-------Step 1-------
tensor([  928, 12645,  4757,  8434,   106, 13939, 13223,     5,  3798, 13839]) tensor([ 9736,  3658, 12054,  7754,  1303,  6279,  8404,  8613,  2847,  4241])
-------Step 2-------
tensor([ 5957,  8076, 14752, 12748,  9411,  8612,   297,  4285,  6693,  5564]) tensor([ 2721,  6221,  2545,  9348, 13711,  2902,  3819,  4381, 12336,  3923])
-------Step 3-------
tensor([ 5957,  8076, 14752, 12748,  9411,  8612,   297,  4285,  6693,  5564]) tensor([ 7032,  7225,  1128, 13590, 14636, 11336, 11677,  5296, 14594,  2812])
-------Step 4-------
tensor([ 9464,    55,  4943, 12702, 13125, 10124,  5141,    83,    80, 10718]) tensor([11617, 11008,  1223, 12554, 12279,  3917,  8763, 11036,  1484,    41])
-------Step 5-------
tensor([ 9464,    55,  4943, 12702, 13125, 10124,  5141,    83,    80, 10718]) tensor([ 3404,  8423, 14804, 11529,   186, 10920,  2318,  7407,  5670, 13308])
-------Step 6-------
tensor([ 4199,    57,  5723,  8696,  2015,  9418,  3367, 11618,  3493,  4860]) tensor([ 7539,  7436,  4740,  4178, 12458,  2854,   632,  6471, 12508,  3375])
-------Step 7-------
tensor([ 4199,    57,  5723,  8696,  2015,  9418,  3367, 11618,  3493,  4860]) tensor([11323,  9478,     9,  1542,  6599, 12819,   839, 10463,  9954,  7197])
-------Step 8-------
tensor([4496, 5857,  342,   45,   18, 4686, 4402, 6613, 7607,   48]) tensor([ 7056,  9024,  8459, 11134,  5237,  4216, 10996, 14147, 11596,  9553])
-------Step 9-------
tensor([4496, 5857,  342,   45,   18, 4686, 4402, 6613, 7607,   48]) tensor([ 3011,  8560, 11333,  5571,  3815,  9562, 14558,  6550,  9650,  1955])
before start barrier
start train
[proc 0] 100 steps, total: 2.084, sample: 0.179, forward: 0.889, backward: 0.538, update: 0.274
train_sampler.Prefill()
before start barrier
start train
[proc 1] 100 steps, total: 2.075, sample: 0.202, forward: 0.858, backward: 0.552, update: 0.307
[proc 0] 200 steps, total: 1.319, sample: 0.183, forward: 0.367, backward: 0.388, update: 0.252
[proc 1] 200 steps, total: 1.319, sample: 0.205, forward: 0.362, backward: 0.380, update: 0.259
[proc 1] 300 steps, total: 1.333, sample: 0.183, forward: 0.375, backward: 0.331, update: 0.263
[proc 0] 300 steps, total: 1.333, sample: 0.168, forward: 0.390, backward: 0.380, update: 0.255
+---------------------------+---------------------------+---------------------------+
| Name                      | P50                       | P99                       |
+---------------------------+---------------------------+---------------------------+
| GenInput                  | 883.183 us                | 9.860 ms                  |
| Forward                   | 3.860 ms                  | 7.829 ms                  |
| Backward                  | 3.863 ms                  | 6.188 ms                  |
| Optimize                  | 2.590 ms                  | 4.380 ms                  |
| OneStep                   | 12.511 ms                 | 27.591 ms                 |
+---------------------------+---------------------------+---------------------------+
[proc 0] 400 steps, total: 1.210, sample: 0.163, forward: 0.340, backward: 0.375, update: 0.232
[proc 1] 400 steps, total: 1.210, sample: 0.164, forward: 0.337, backward: 0.194, update: 0.243
[proc 1] 500 steps, total: 1.325, sample: 0.199, forward: 0.362, backward: 0.216, update: 0.255
[proc 0] 500 steps, total: 1.325, sample: 0.180, forward: 0.378, backward: 0.389, update: 0.252
Successfully xmh. training takes 7.2712297439575195 seconds
before call kg_cache_controller.StopThreads()
KGCacheControllerWrapperDummy.StopThreads
