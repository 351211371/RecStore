perf_emb.py:126: DeprecationWarning: The 'warn' function is deprecated, use 'warning' instead
  logging.warn("Before init DistEmbedding")
WARNING [perf_emb.py:126] Before init DistEmbedding
perf_emb.py:129: DeprecationWarning: The 'warn' function is deprecated, use 'warning' instead
  logging.warn("After init DistEmbedding")
WARNING [perf_emb.py:129] After init DistEmbedding
========== Running Perf with routine <function routine_local_cache_helper at 0x7f5a2c014ee0>==========
Worker 0 pid=21821
Worker 1 pid=21822
Worker 2 pid=21823
Worker 3 pid=21824
Worker 4 pid=21825
INFO [distributed_c10d.py:432] Added key: store_based_barrier_key:1 to store for rank: 0
INFO [distributed_c10d.py:432] Added key: store_based_barrier_key:1 to store for rank: 1
INFO [distributed_c10d.py:432] Added key: store_based_barrier_key:1 to store for rank: 2
INFO [distributed_c10d.py:432] Added key: store_based_barrier_key:1 to store for rank: 4
INFO [distributed_c10d.py:432] Added key: store_based_barrier_key:1 to store for rank: 3
INFO [distributed_c10d.py:466] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 6 nodes.
INFO [distributed_c10d.py:466] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 6 nodes.
INFO [distributed_c10d.py:466] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 6 nodes.
INFO [distributed_c10d.py:432] Added key: store_based_barrier_key:1 to store for rank: 5
INFO [distributed_c10d.py:466] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 6 nodes.
INFO [distributed_c10d.py:466] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 6 nodes.
INFO [distributed_c10d.py:466] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 6 nodes.
cudaHostRegister cudaError.success, type(r)=<class 'torch._C._cudart.cudaError'>
weight.shape torch.Size([100000000, 32])
cudaHostRegister cudaError.success, type(r)=<class 'torch._C._cudart.cudaError'>
weight.shape torch.Size([100000000, 32])
cudaHostRegister cudaError.success, type(r)=<class 'torch._C._cudart.cudaError'>
weight.shape torch.Size([100000000, 32])
cudaHostRegister cudaError.success, type(r)=<class 'torch._C._cudart.cudaError'>
weight.shape torch.Size([100000000, 32])
cudaHostRegister cudaError.success, type(r)=<class 'torch._C._cudart.cudaError'>
weight.shape torch.Size([100000000, 32])
cudaHostRegister cudaError.success, type(r)=<class 'torch._C._cudart.cudaError'>
weight.shape torch.Size([100000000, 32])
Step99:rank1, time: 27.671, per_step: 0.276707
Step99:rank5, time: 27.630, per_step: 0.276302
Step99:rank3, time: 28.111, per_step: 0.281106
Step99:rank4, time: 27.850, per_step: 0.278497
Step99:rank2, time: 27.854, per_step: 0.278535
Step99:rank0, time: 29.983, per_step: 0.299830
Step199:rank3, time: 26.197, per_step: 0.259372
Step199:rank4, time: 26.195, per_step: 0.259361
Step199:rank2, time: 26.196, per_step: 0.259368
Step199:rank1, time: 26.230, per_step: 0.259701
Step199:rank5, time: 26.245, per_step: 0.259854
Step199:rank0, time: 26.146, per_step: 0.258874
Step299:rank4, time: 27.245, per_step: 0.269756
Step299:rank2, time: 27.251, per_step: 0.269816
Step299:rank3, time: 27.256, per_step: 0.269859
Step299:rank5, time: 27.235, per_step: 0.269658
Step299:rank1, time: 27.282, per_step: 0.270118
Step299:rank0, time: 27.386, per_step: 0.271150
Step399:rank3, time: 26.766, per_step: 0.265012
Step399:rank5, time: 26.781, per_step: 0.265160
Step399:rank2, time: 26.794, per_step: 0.265285
Step399:rank4, time: 26.803, per_step: 0.265378
Step399:rank1, time: 26.772, per_step: 0.265069
Step399:rank0, time: 26.700, per_step: 0.264358
Step499:rank3, time: 26.718, per_step: 0.264532
Step499:rank5, time: 26.711, per_step: 0.264464
Step499:rank1, time: 26.698, per_step: 0.264335
Step499:rank2, time: 26.708, per_step: 0.264439
Step499:rank4, time: 26.716, per_step: 0.264514
Step499:rank0, time: 26.666, per_step: 0.264019
Worker 5 pid=21826
Successfully xmh
