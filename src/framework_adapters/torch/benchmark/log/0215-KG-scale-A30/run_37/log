WARNING: Logging before InitGoogleLogging() is written to STDERR
I20240218 05:36:54.521564 1620071 IPC_barrier.h:128] MultiProcessBarrierFactory->ClearIPCMemory()
/home/xieminhui/RecStore/src/framework_adapters/torch/kg/dgl-0.9.1/python/dgl/_deprecate/graph.py:1023: DGLWarning: multigraph will be deprecated.DGL will treat all graphs as multigraph in the future.
  dgl_warning("multigraph will be deprecated." \
/home/xieminhui/RecStore/src/framework_adapters/torch/kg/dgl-0.9.1/python/dgl/heterograph.py:72: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.
  dgl_warning('Recommend creating graphs by `dgl.graph(data)`'
Reading train triples....
Finished. Read 483142 train triples.
Reading valid triples....
Finished. Read 50000 valid triples.
Reading test triples....
Finished. Read 59071 test triples.
ARGS:  Namespace(model_name='SimplE', data_path='/home/xieminhui/dgl-data', dataset='FB15k', format='built_in', data_files=None, delimiter='\t', save_path='/tmp/ckpts/SimplE_FB15k_12', no_save_emb=True, max_step=500, batch_size=1200, batch_size_eval=16, neg_sample_size=200, neg_deg_sample=False, neg_deg_sample_eval=False, neg_sample_size_eval=14951, eval_percent=1, no_eval_filter=False, log_interval=100, eval_interval=10000, test=False, num_proc=2, num_thread=1, force_sync_interval=1, hidden_dim=400, lr=0.01, gamma=16.0, double_ent=False, double_rel=False, neg_adversarial_sampling=False, adversarial_temperature=1.0, regularization_coef=0.0, regularization_norm=3, pairwise=False, loss_genre='Logsigmoid', margin=1.0, nr_gpus=2, gpu=[0, 1], mix_cpu_gpu=True, valid=False, rel_part=False, async_update=None, has_edge_importance=False, cached_emb_type='None', use_my_emb=False, cache_ratio=0.05, shuffle=False, backwardMode='CppSync', L=10, nr_background_threads=32, update_cache_use_omp=1, update_pq_use_omp=2, kForwardItersPerStep=2, backgrad_init='both', eval_filter=True, soft_rel_part=False)
|Train|: 483142
original graph:  DGLGraph(num_nodes=14951, num_edges=592213,
         ndata_schemes={}
         edata_schemes={'tid': Scheme(shape=(), dtype=torch.int64)})
Loading cached metis
WARNING [1620071 sampler.py:454] Start PreSampling
WARNING [1620071 sampler.py:532] Before construct renumbering_dict
WARNING [1620071 sampler.py:555] PreSampling done
W20240218 05:36:56.614324 1620071 IPCTensor.h:369] NewIPCTensor: cached_sampler_r0_0 [1000000]0x100000002000 7.629 MB
W20240218 05:36:56.614462 1620071 IPCTensor.h:369] NewIPCTensor: cached_sampler_r0_1 [1000000]0x1000007a5000 7.629 MB
W20240218 05:36:56.614483 1620071 IPCTensor.h:369] NewIPCTensor: cached_sampler_r0_2 [1000000]0x100000f48000 7.629 MB
W20240218 05:36:56.614501 1620071 IPCTensor.h:369] NewIPCTensor: cached_sampler_r0_3 [1000000]0x1000016eb000 7.629 MB
W20240218 05:36:56.614522 1620071 IPCTensor.h:369] NewIPCTensor: cached_sampler_r0_4 [1000000]0x100001e8e000 7.629 MB
W20240218 05:36:56.614547 1620071 IPCTensor.h:369] NewIPCTensor: cached_sampler_r0_5 [1000000]0x100002631000 7.629 MB
W20240218 05:36:56.614563 1620071 IPCTensor.h:369] NewIPCTensor: cached_sampler_r0_6 [1000000]0x100002dd4000 7.629 MB
W20240218 05:36:56.614594 1620071 IPCTensor.h:369] NewIPCTensor: cached_sampler_r0_7 [1000000]0x100003577000 7.629 MB
W20240218 05:36:56.614617 1620071 IPCTensor.h:369] NewIPCTensor: cached_sampler_r0_8 [1000000]0x100003d1a000 7.629 MB
W20240218 05:36:56.614638 1620071 IPCTensor.h:369] NewIPCTensor: cached_sampler_r0_9 [1000000]0x1000044bd000 7.629 MB
W20240218 05:36:56.614662 1620071 IPCTensor.h:369] NewIPCTensor: step_r0 [10]0x100004c60000 80 B 
W20240218 05:36:56.614678 1620071 IPCTensor.h:369] NewIPCTensor: circle_buffer_end_r0 [1]0x100004c62000 8 B 
W20240218 05:36:56.614696 1620071 IPCTensor.h:369] NewIPCTensor: circle_buffer_end_cppseen_r0 [1]0x100004c64000 8 B 
W20240218 05:36:56.614773 1620071 IPCTensor.h:369] NewIPCTensor: cached_sampler_r1_0 [1000000]0x100004c66000 7.629 MB
W20240218 05:36:56.614799 1620071 IPCTensor.h:369] NewIPCTensor: cached_sampler_r1_1 [1000000]0x100005409000 7.629 MB
W20240218 05:36:56.614822 1620071 IPCTensor.h:369] NewIPCTensor: cached_sampler_r1_2 [1000000]0x100005bac000 7.629 MB
W20240218 05:36:56.614854 1620071 IPCTensor.h:369] NewIPCTensor: cached_sampler_r1_3 [1000000]0x10000634f000 7.629 MB
W20240218 05:36:56.614869 1620071 IPCTensor.h:369] NewIPCTensor: cached_sampler_r1_4 [1000000]0x100006af2000 7.629 MB
W20240218 05:36:56.614883 1620071 IPCTensor.h:369] NewIPCTensor: cached_sampler_r1_5 [1000000]0x100007295000 7.629 MB
W20240218 05:36:56.614902 1620071 IPCTensor.h:369] NewIPCTensor: cached_sampler_r1_6 [1000000]0x100007a38000 7.629 MB
W20240218 05:36:56.614920 1620071 IPCTensor.h:369] NewIPCTensor: cached_sampler_r1_7 [1000000]0x1000081db000 7.629 MB
W20240218 05:36:56.614934 1620071 IPCTensor.h:369] NewIPCTensor: cached_sampler_r1_8 [1000000]0x10000897e000 7.629 MB
W20240218 05:36:56.614953 1620071 IPCTensor.h:369] NewIPCTensor: cached_sampler_r1_9 [1000000]0x100009121000 7.629 MB
W20240218 05:36:56.614967 1620071 IPCTensor.h:369] NewIPCTensor: step_r1 [10]0x1000098c4000 80 B 
W20240218 05:36:56.614980 1620071 IPCTensor.h:369] NewIPCTensor: circle_buffer_end_r1 [1]0x1000098c6000 8 B 
W20240218 05:36:56.614992 1620071 IPCTensor.h:369] NewIPCTensor: circle_buffer_end_cppseen_r1 [1]0x1000098c8000 8 B 
{0: Graph(num_nodes=13804, num_edges=289155,
      ndata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64), 'inner_node': Scheme(shape=(), dtype=torch.int32), 'part_id': Scheme(shape=(), dtype=torch.int64)}
      edata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64), 'inner_edge': Scheme(shape=(), dtype=torch.int8)}), 1: Graph(num_nodes=12626, num_edges=379866,
      ndata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64), 'inner_node': Scheme(shape=(), dtype=torch.int32), 'part_id': Scheme(shape=(), dtype=torch.int64)}
      edata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64), 'inner_edge': Scheme(shape=(), dtype=torch.int8)})}
MertisPartition: part 0 has 13804 N 289155 E
MertisPartition: part 1 has 12626 N 379866 E
Rank0: cached key size 373
Rank1: cached key size 373
Before renumbering graph:  {'_ID': tensor([    0,     2,     6,  ..., 11079,  2237, 14365]), 'inner_node': tensor([1, 1, 1,  ..., 0, 0, 0], dtype=torch.int32), 'part_id': tensor([0, 0, 0,  ..., 1, 1, 1])}
After renumbering graph:  {'_ID': tensor([  755,   829,   950,  ..., 10450, 12400, 12618]), 'inner_node': tensor([1, 1, 1,  ..., 0, 0, 0], dtype=torch.int32), 'part_id': tensor([0, 0, 0,  ..., 1, 1, 1])}
part_g: DGLGraph(num_nodes=13804, num_edges=289155,
         ndata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64)}
         edata_schemes={'tid': Scheme(shape=(), dtype=torch.int64)})
part_g: DGLGraph(num_nodes=13804, num_edges=289155,
         ndata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64)}
         edata_schemes={'tid': Scheme(shape=(), dtype=torch.int64)})
Total initialize time 2.264 seconds
INFO [1620071 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 0
INFO [1620405 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 1
INFO [1620071 distributed_c10d.py:476] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.
INFO [1620405 distributed_c10d.py:476] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.
E20240218 05:37:04.130999 1620406 recstore.cc:67] init folly done
E20240218 05:37:04.131018 1620443 recstore.cc:67] init folly done
I20240218 05:37:04.169160 1620443 IPC_barrier.h:118] CreateBarrier: new barrier, name: start_barrier, count: 2
I20240218 05:37:04.169793 1620406 IPC_barrier.h:112] CreateBarrier: find existing barrier, name: start_barrier, count: 2
[Rank1] pid = 1620405
train_sampler.Prefill()
-------Step 0-------
tensor([   67,  5655, 10463,  7066,  2874, 13265,  4051,  7531,  7418,  8498]) tensor([  981, 10053, 12601, 11789, 14514,  7140,  6915,   380,  6841,  8776])
-------Step 1-------
tensor([   67,  5655, 10463,  7066,  2874, 13265,  4051,  7531,  7418,  8498]) tensor([14806,  1679,  2033,  4757,  8908, 11264, 11037, 10034, 12499,  4243])
-------Step 2-------
tensor([13265,     4,    28,   269,  6341,  6366,  1656, 10687,  8064, 12285]) tensor([ 2401,  2007, 13280, 12554,  4731,  8247, 13475,  2193,  5250,  6290])
-------Step 3-------
tensor([13265,     4,    28,   269,  6341,  6366,  1656, 10687,  8064, 12285]) tensor([14567,  2266,   349,  5832,  5970,  9504,  3495,   839, 13333, 13164])
-------Step 4-------
tensor([ 2446,  1962,    44,  1805,  9276,  6338, 10681,  8990,   136,  6461]) tensor([ 6111, 12590,  9902,  7840, 12963,  6696,  7034,  9385,  4711,  8076])
-------Step 5-------
tensor([ 2446,  1962,    44,  1805,  9276,  6338, 10681,  8990,   136,  6461]) tensor([ 7625,  2915, 11742,  5090,  3383,  9279,    33,   791,  5915,  7882])
-------Step 6-------
tensor([ 6412,  3197,  6894,   262,  1242,    57, 13265,  8280,   140,   135]) tensor([13977, 14142,  8237,  9015,   850,  6331,  3861, 14207, 13443,  9871])
-------Step 7-------
tensor([ 6412,  3197,  6894,   262,  1242,    57, 13265,  8280,   140,   135]) tensor([ 7605,  1696, 13168, 13724, 13485,  7824,  2072,   603,    87,  7373])
-------Step 8-------
tensor([ 4914, 10160,  6536,  1990,  5655,  4892,  6341,  2446, 11992, 12755]) tensor([13506,  6699, 13262,  5128,  6610,  9515,  2601,  8092, 11990, 12181])
-------Step 9-------
tensor([ 4914, 10160,  6536,  1990,  5655,  4892,  6341,  2446, 11992, 12755]) tensor([ 3973, 10575, 13259, 14509, 12176,  4314,  2555, 10034, 11364,  3343])
before start barrier
start train
[proc 0] 100 steps, total: 3.318, sample: 0.203, forward: 1.091, backward: 1.138, update: 0.456
train_sampler.Prefill()
before start barrier
start train
[proc 1] 100 steps, total: 3.319, sample: 0.215, forward: 1.369, backward: 1.135, update: 0.482
+---------------------------+---------------------------+---------------------------+
| Name                      | P50                       | P99                       |
+---------------------------+---------------------------+---------------------------+
| GenInput                  | 883.000 us                | 11.127 ms                 |
| Forward                   | 5.307 ms                  | 12.574 ms                 |
| Backward                  | 4.988 ms                  | 8.021 ms                  |
| Optimize                  | 4.295 ms                  | 7.485 ms                  |
| OneStep                   | 16.841 ms                 | 37.090 ms                 |
+---------------------------+---------------------------+---------------------------+
[proc 0] 200 steps, total: 1.811, sample: 0.210, forward: 0.543, backward: 0.495, update: 0.433
[proc 1] 200 steps, total: 1.811, sample: 0.219, forward: 0.530, backward: 0.489, update: 0.452
[proc 0] 300 steps, total: 1.584, sample: 0.183, forward: 0.456, backward: 0.475, update: 0.362
[proc 1] 300 steps, total: 1.584, sample: 0.196, forward: 0.448, backward: 0.472, update: 0.377
[proc 1] 400 steps, total: 1.544, sample: 0.195, forward: 0.434, backward: 0.469, update: 0.362
[proc 0] 400 steps, total: 1.544, sample: 0.182, forward: 0.442, backward: 0.469, update: 0.346
[proc 0] 500 steps, total: 1.620, sample: 0.201, forward: 0.469, backward: 0.473, update: 0.368
Successfully xmh. training takes 9.876482725143433 seconds
before call kg_cache_controller.StopThreads()
[proc 1] 500 steps, total: 1.620, sample: 0.198, forward: 0.456, backward: 0.472, update: 0.381
+---------------------------+---------------------------+---------------------------+
| Name                      | P50                       | P99                       |
+---------------------------+---------------------------+---------------------------+
| GenInput                  | 787.383 us                | 11.127 ms                 |
| Forward                   | 4.666 ms                  | 8.586 ms                  |
| Backward                  | 4.883 ms                  | 5.265 ms                  |
| Optimize                  | 3.725 ms                  | 6.271 ms                  |
| OneStep                   | 14.845 ms                 | 28.591 ms                 |
+---------------------------+---------------------------+---------------------------+
KGCacheControllerWrapperDummy.StopThreads
